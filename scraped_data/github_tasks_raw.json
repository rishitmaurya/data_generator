[
  {
    "name": "asgiref fails with gevent patching",
    "description": "When `gevent.monkey.patch_all()` is used, `async def` functions fail when multiple concurrent requests are sent. asgiref gives the error `RuntimeError: You cannot use AsyncToSync in the same thread as an async event loop - just await the async function directly.`. This suggests that somehow, gevent is causing a running loop to be visible across threads.\n\n`example.py`:\n\n```python\n# /// script\n# requires-python = \"~=3.14.0\"\n# dependencies = [\n#     \"flask[async]\",\n#     \"gevent\",\n# ]\n#\n# [tool.uv]\n# exclude-newer = \"2026-01-05T00:00:00Z\"\n#\n# [tool.uv.sources]\n# flask = { path = \".\", editable = true }\n# ///\n\nimport gevent.monkey\ngevent.monkey.patch_all()\n\nimport asyncio\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.get(\"/\")\nasync def hello_world():\n    await asyncio.sleep(0.5)\n    return f\"Hello, World!\"\n```\n\n```\nuv run --with-requirements example.py flask -A example.py run\n```\n\n`load.py`:\n\n```python\n# /// script\n# requires-python = \"~=3.14.0\"\n# dependencies = [\n#     \"httpx\",\n# ]\n#\n# [tool.uv]\n# exclude-newer = \"2026-01-05\"\n# ///\n\nimport asyncio\nimport httpx\n\nasync def get(i, client: httpx.AsyncClient):\n    r = await client.get(\"http://127.0.0.1:5000\")\n    print(i, await r.aread())\n\nasync def main():\n    async with httpx.AsyncClient() as client, asyncio.TaskGroup() as group:\n        for i in range(50):\n            group.create_task(get(i, client))\n\nasyncio.run(main())\n```\n\n```\nuv run load.py\n```\n\nContinues #5817 with more details."
  },
  {
    "name": "Teardown handler chain exception handling",
    "description": "Flask executes registered teardown handlers (used for closing DB connections, releasing locks, etc.) in a chain. The do_teardown_request method which is supposed to make these teardown requests does not wrap individual handlers in a try/except block. If one handler raises an exception, the loop terminates immediately, and all subsequent handlers are skipped, this can lead to the application to skip critical cleanup routines if a non critical one fails.\n\nSample reproduction code\n```python\nimport threading\nimport time\nimport requests\nfrom flask import Flask\n\napp = Flask(__name__)\n\n#a simulation of a limited resource pool\nmock_db_connections = 0\n\n@app.route('/')\ndef index():\n    global mock_db_connections\n    mock_db_connections = mock_db_connections + 1\n    return \"Request Processed\"\n\n#simulating a critical teardown handler which is supposed to free resources from the db\n@app.teardown_request\ndef critical_db_close(exc):\n    global mock_db_connections\n    mock_db_connections = mock_db_connections - 1\n\n#teardown handler which can crash\n@app.teardown_request\ndef buggy_extension(exc):\n    raise Exception(\"Crash in buggy extension!\")\n\ndef run_server():\n    #just for clearner output\n    import logging\n    log = logging.getLogger('werkzeug')\n    log.setLevel(logging.ERROR)\n    app.logger.disabled = True\n    \n    app.run(port=5000, threaded=True)\n\ndef verify_bug():\n    \n    print(f\"Initial DB Connections: {mock_db_connections}\")\n    \n    try:\n        requests.get('http://127.0.0.1:5000/')\n    except Exception:\n        pass\n\n    #wait time to ensure teardown has happened in the other thread\n    time.sleep(1)\n    \n    print(f\"Final DB Connections: {mock_db_connections}\")\n    \n    if mock_db_connections > 0:\n        print(\"Teardown Chain Broken\")\n    else:\n        print(\"Failed to reproduce\")\n\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\nverify_bug()\n```"
  },
  {
    "name": "Modern CSRF Protection Using `Sec-Fetch-Site` Header",
    "description": "Flask's documentation states that CSRF protection requires a form validation framework (which Flask doesn't provide), necessitating one-time tokens stored in cookies and transmitted with form data. However, modern browsers now support `Sec-Fetch-Site` headers, making token-based CSRF protection unnecessary. Rails just merged this approach in [rails/rails#56350](https://github.com/rails/rails/pull/56350). Flask should offer a similar modern solution\u2014but in a more Flask-like way: as a simple argument to `@app.route()`.\n\n## Current State\n\nFrom Flask's security documentation:\n\n> \"Why does Flask not do that for you? The ideal place for this to happen is the form validation framework, which does not exist in Flask.\"\n\nThis was written when CSRF tokens were the only viable protection mechanism. That's no longer true. And with header-based protection, no form validation framework is needed\u2014just a simple check before dispatching to the view.\n\n## The Modern Approach\n\nThe `Sec-Fetch-Site` header is a [Fetch Metadata Request Header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Site) that modern browsers send automatically. It indicates the relationship between the request origin and the target origin:\n\n- `same-origin`: Request from the same origin (same scheme, host, and port)\n- `same-site`: Request from the same site but different origin\n- `cross-site`: Request from a completely different site\n- `none`: User-initiated navigation (typing URL, bookmark, etc.)\n\nFor state-changing requests (POST, PUT, DELETE, PATCH), we can simply reject requests where `Sec-Fetch-Site` is `cross-site`. No tokens needed.\n\n**Browser Support:** All modern browsers (Chrome 76+, Firefox 90+, Safari 16.4+, Edge 79+) - [caniuse.com/mdn-http_headers_sec-fetch-site](https://caniuse.com/mdn-http_headers_sec-fetch-site)\n\n## Why This Matters\n\nToken-based CSRF has significant operational pain:\n\n1. **Caching conflicts**: Cached pages contain stale tokens \u2192 false positives\n2. **Session expiry edge cases**: Token/session mismatch after timeout\n3. **SPA complexity**: Managing token refresh in JavaScript applications\n4. **Multi-tab issues**: Tokens invalidated when user opens multiple tabs\n\nHeader-based protection eliminates all of these.\n\n## Proposed Flask Implementation\n\nRather than adding another extension, this should be a first-class citizen in Flask's routing. The implementation is simple enough that it belongs in core\u2014just an argument to `@app.route()`:\n\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\napp.config['CSRF_TRUSTED_ORIGINS'] = ['https://accounts.google.com']\n\n# Protected by default for state-changing methods\n@app.route('/api/data', methods=['POST'])\ndef create_data():\n    return {'status': 'ok'}\n\n# Explicitly disable for webhooks that use signature verification\n@app.route('/webhooks/stripe', methods=['POST'], csrf=False)\ndef stripe_webhook():\n    return {'received': True}\n\n# GET requests are never protected (no state change)\n@app.route('/api/data', methods=['GET'])\ndef get_data():\n    return {'data': []}\n```\n\n## Configuration\n\n```python\n# Default configuration in Flask\nCSRF_ENABLED = True                    # Global kill switch\nCSRF_TRUSTED_ORIGINS = []              # Allow cross-origin from these origins\nCSRF_PROTECTED_METHODS = {'POST', 'PUT', 'PATCH', 'DELETE'}\n```\n\nNote: Unlike my earlier draft, `same-site` requests are **rejected by default**. This is intentional\u2014different subdomains often have different trust levels (e.g., `marketing.example.com` vs `admin.example.com`). If you need same-site requests, add the specific origin to `CSRF_TRUSTED_ORIGINS`.\n\n\n## Questions for Maintainers\n\n1. **Default On vs Off**: Should CSRF protection be on by default for state-changing methods (proposed), or require explicit `csrf=True`?\n\n2. **Same-site Policy**: The algorithm rejects `same-site` requests by default (per Filippo Valsorda's guidance). Should there be a config option to relax this, or is explicit `CSRF_TRUSTED_ORIGINS` sufficient?\n\n3. **Werkzeug Level**: Should the core check logic live in Werkzeug so other frameworks (Bottle, etc.) can use it?\n\n## References\n\n- [MDN: Sec-Fetch-Site](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Site)\n- [web.dev: Fetch Metadata](https://web.dev/articles/fetch-metadata)\n- [Rails PR #56350](https://github.com/rails/rails/pull/56350) - Rails implementation (merged Dec 2025)\n- [Rack Issue #2367](https://github.com/rack/rack/issues/2367) - Rack-level discussion\n- [Go proposal: CrossOriginForgeryHandler](https://github.com/golang/go/issues/73626)\n- [Blog from go author](https://words.filippo.io/csrf/)\n- [OWASP Fetch Metadata positioning](https://github.com/OWASP/CheatSheetSeries/pull/1875)\n\n## Willingness to Implement\n\nI'm prepared to submit a PR implementing this. The change is small and self-contained."
  },
  {
    "name": "Proposal: CLI command to validate API endpoints by making test requests",
    "description": "### Motivation\n\nWhen working on Flask applications with many API endpoints, it is sometimes\nuseful to quickly check whether all registered routes can be accessed without\nraising unexpected errors (e.g. unhandled exceptions, 500 errors).\n\nAt the moment, developers typically need to write custom scripts or tests to\nperform this kind of basic validation. Providing an opt-in CLI command could\nhelp improve developer experience, especially during local development.\n\n### Proposal\n\nIntroduce an optional Flask CLI command (for example, `flask check-endpoints`)\nthat iterates over registered routes and makes test requests using Flask\u2019s\ntest client.\n\nThe goal would not be full correctness testing, but a lightweight sanity check\nto detect obvious runtime errors.\n\nPossible initial scope:\n- Only routes with simple methods (e.g. GET by default)\n- Skip routes with required path parameters\n- Report endpoints that raise exceptions or return 5xx responses\n- Development-only usage\n\n### Design considerations\n\n- The command should be fully opt-in and not affect existing behavior.\n- No network requests; use Flask\u2019s built-in test client.\n- Keep output simple and readable (similar to `flask routes`).\n- Advanced features (custom payloads, auth, etc.) could be out of scope initially.\n\n### Open questions\n\n- Should this live as a core CLI command or as an optional extension?\n- How should endpoints with required parameters or authentication be handled?\n- Should the command fail on the first error or report all failures?\n\nFeedback on the general direction and scope would be appreciated before\nstarting an implementation."
  },
  {
    "name": "Add Copy-to-Clipboard button for code examples in documentation",
    "description": "### Feature Request: Add copy button to code blocks in documentation\n\n#### Problem\n\nMany pages in the Flask documentation contain multi-line code snippets, but there is no built-in way to copy the entire block with one click. Users currently need to manually select text, which is slower and error-prone.\n\n#### Proposed Solution\n\nAdd a small \"Copy\" button to each `div.highlight` element using a small JavaScript file included via Sphinx.\n\n- Implemented using the native Clipboard API (no external library)\n- Minimal UI impact\n- Applied only to docs frontend, not core package\n\n#### Motivation & Benefits\n\n- Easier copy-paste for beginners following tutorials\n- Consistent with other frameworks (FastAPI, Django, Click docs, etc.)\n- Improves developer experience without affecting core Flask code\n\n#### Status\n\nI have already implemented a working version locally and can submit a PR once approved.\n\nLet me know if this is useful and if I should proceed with a pull request."
  },
  {
    "name": "Add copy-to-clipboard button for code blocks in Flask documentation.",
    "description": "### Description\n\nI propose to add a \"copy to clipboard\" button next to all code blocks in the Flask documentation. This will allow users to easily copy example code or command-line snippets with a single click instead of manually selecting the text. This feature greatly improves usability and convenience, especially on mobile devices.\n\nFlask documentation uses Sphinx for static site generation, and there is an existing Sphinx extension called [sphinx-copybutton](https://sphinx-copybutton.readthedocs.io/en/latest/) that provides this functionality out of the box. It can be enabled by simply adding it to the `extensions` list in the Sphinx `conf.py` file.\n\n\nThis extension also supports stripping common prompts (like `>>>`, `$`) from copied content, making the copied snippets cleaner.\n\n### Problem this resolves\n\nCurrently, users have to manually select and copy code blocks, which can be error-prone and inconvenient. Adding a copy button makes the process faster and less error-prone.\n\n### Additional notes\n\n- The change is additive and does not affect existing documentation content or structure.\n- This enhancement aligns Flask docs with modern documentation practices seen in many popular projects.\n\nI am happy to implement this feature and create a pull request once the issue is approved."
  },
  {
    "name": "Really, I can't close connection???",
    "description": "Jesus Christ, what kind of library doesn\u2019t allow forcefully closing a connection? Even after returning or using abort, it still continues to process data in the background. This is disappointing, there\u2019s no way, for example, to forcefully stop a file upload after checking its header. It will still continue to process the remaining 100 GB of the file.\n\n```\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    print(1)\n    return \"wtf\", 400 # still continues to process data\n```"
  },
  {
    "name": "Test failures with click 8.3.1",
    "description": "Click 8.3.1 was [tagged](https://github.com/pallets/click/releases/tag/8.3.1) but not released on PyPI. Running flask's test suite with that version results in\n```\n============================= test session starts ==============================\nplatform linux -- Python 3.13.8, pytest-8.4.2, pluggy-1.6.0\nrootdir: /build/flask-3.1.2\nconfigfile: pyproject.toml\ntestpaths: tests\ncollected 490 items                                                            \n\ntests/test_appctx.py ..............                                      [  2%]\ntests/test_async.py ........                                             [  4%]\ntests/test_basic.py .................................................... [ 15%]\n........................................................................ [ 29%]\n......                                                                   [ 31%]\ntests/test_blueprints.py ............................................... [ 40%]\n.............                                                            [ 43%]\ntests/test_cli.py ...................................................F.. [ 54%]\n...F                                                                     [ 55%]\ntests/test_config.py ...................                                 [ 58%]\ntests/test_converters.py ..                                              [ 59%]\ntests/test_helpers.py ...................................                [ 66%]\ntests/test_instance_config.py .......                                    [ 67%]\ntests/test_json.py ...............................                       [ 74%]\ntests/test_json_tag.py ..............                                    [ 77%]\ntests/test_logging.py ......                                             [ 78%]\ntests/test_regression.py .                                               [ 78%]\ntests/test_reqctx.py .......ss.....                                      [ 81%]\ntests/test_request.py ...                                                [ 82%]\ntests/test_session_interface.py .                                        [ 82%]\ntests/test_signals.py .......                                            [ 83%]\ntests/test_subclassing.py .                                              [ 83%]\ntests/test_templating.py ................................                [ 90%]\ntests/test_testing.py .........................                          [ 95%]\ntests/test_user_error_handler.py .........                               [ 97%]\ntests/test_views.py .............                                        [100%]\n\n=================================== FAILURES ===================================\n______________________________ test_run_cert_path ______________________________\n\n    def test_run_cert_path():\n        # no key\n        with pytest.raises(click.BadParameter):\n            run_command.make_context(\"run\", [\"--cert\", __file__])\n    \n        # no cert\n>       with pytest.raises(click.BadParameter):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       Failed: DID NOT RAISE <class 'click.exceptions.BadParameter'>\n\n\ntests/test_cli.py:590: Failed\n__________________________ test_run_exclude_patterns ___________________________\n\n    def test_run_exclude_patterns():\n>       ctx = run_command.make_context(\"run\", [\"--exclude-patterns\", __file__])\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\ntests/test_cli.py:701: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/nix/store/53y968aagcaaq46hq3rg2a3mmxwmzvm1-python3.13-click-8.3.1/lib/python3.13/site-packages/click/core.py:1216: in make_context\n    self.parse_args(ctx, args)\n        args       = []\n        ctx        = <click.core.Context object at 0x7ffff4c02b10>\n        extra      = {}\n        info_name  = 'run'\n        parent     = None\n        self       = <Command run>\n/nix/store/53y968aagcaaq46hq3rg2a3mmxwmzvm1-python3.13-click-8.3.1/lib/python3.13/site-packages/click/core.py:1227: in parse_args\n    _, args = param.handle_parse_result(ctx, opts, args)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        _          = 5000\n        args       = []\n        ctx        = <click.core.Context object at 0x7ffff4c02b10>\n        opts       = {'exclude_patterns': '/build/flask-3.1.2/tests/test_cli.py'}\n        param      = <Option key>\n        param_order = [<Option exclude_patterns>]\n        parser     = <click.parser._OptionParser object at 0x7ffff4b08bd0>\n        self       = <Command run>\n/nix/store/53y968aagcaaq46hq3rg2a3mmxwmzvm1-python3.13-click-8.3.1/lib/python3.13/site-packages/click/core.py:2548: in handle_parse_result\n    value = self.process_value(ctx, value)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        args       = []\n        ctx        = <click.core.Context object at 0x7ffff4c02b10>\n        opts       = {'exclude_patterns': '/build/flask-3.1.2/tests/test_cli.py'}\n        self       = <Option key>\n        source     = <ParameterSource.DEFAULT: 3>\n        value      = Sentinel.UNSET\n/nix/store/53y968aagcaaq46hq3rg2a3mmxwmzvm1-python3.13-click-8.3.1/lib/python3.13/site-packages/click/core.py:3283: in process_value\n    return super().process_value(ctx, value)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        __class__  = <class 'click.core.Option'>\n        ctx        = <click.core.Context object at 0x7ffff4c02b10>\n        self       = <Option key>\n        value      = Sentinel.UNSET\n/nix/store/53y968aagcaaq46hq3rg2a3mmxwmzvm1-python3.13-click-8.3.1/lib/python3.13/site-packages/click/core.py:2443: in process_value\n    value = self.callback(ctx, self, value)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        ctx        = <click.core.Context object at 0x7ffff4c02b10>\n        self       = <Option key>\n        value      = None\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nctx = <click.core.Context object at 0x7ffff4c02b10>, param = <Option key>\nvalue = None\n\n    def _validate_key(ctx: click.Context, param: click.Parameter, value: t.Any) -> t.Any:\n        \"\"\"The ``--key`` option must be specified when ``--cert`` is a file.\n        Modifies the ``cert`` param to be a ``(cert, key)`` pair if needed.\n        \"\"\"\n        cert = ctx.params.get(\"cert\")\n        is_adhoc = cert == \"adhoc\"\n    \n        try:\n            import ssl\n        except ImportError:\n            is_context = False\n        else:\n            is_context = isinstance(cert, ssl.SSLContext)\n    \n        if value is not None:\n            if is_adhoc:\n                raise click.BadParameter(\n                    'When \"--cert\" is \"adhoc\", \"--key\" is not used.', ctx, param\n                )\n    \n            if is_context:\n                raise click.BadParameter(\n                    'When \"--cert\" is an SSLContext object, \"--key\" is not used.',\n                    ctx,\n                    param,\n                )\n    \n            if not cert:\n                raise click.BadParameter('\"--cert\" must also be specified.', ctx, param)\n    \n            ctx.params[\"cert\"] = cert, value\n    \n        else:\n            if cert and not (is_adhoc or is_context):\n>               raise click.BadParameter('Required when using \"--cert\".', ctx, param)\nE               click.exceptions.BadParameter: Required when using \"--cert\".\n\ncert       = Sentinel.UNSET\nctx        = <click.core.Context object at 0x7ffff4c02b10>\nis_adhoc   = False\nis_context = False\nparam      = <Option key>\nssl        = <module 'ssl' from '/nix/store/cfapjd2rvqrpry4grb0kljnp8bvnvfxz-python3-3.13.8/lib/python3.13/ssl.py'>\nvalue      = None\n\n/nix/store/12gfdjrf93rixvjrkj9pbfy5349q3j4m-python3.13-flask-3.1.2/lib/python3.13/site-packages/flask/cli.py:870: BadParameter\n=========================== short test summary info ============================\nFAILED tests/test_cli.py::test_run_cert_path - Failed: DID NOT RAISE <class 'click.exceptions.BadParameter'>\nFAILED tests/test_cli.py::test_run_exclude_patterns - click.exceptions.BadParameter: Required when using \"--cert\".\n================== 2 failed, 486 passed, 2 skipped in 13.73s ===================\n```"
  },
  {
    "name": "Document 415 on the receiving json section",
    "description": "Documentation on \"json\" request throwing a 415 as well as 400.  \n\nThe problem text is on https://flask.palletsprojects.com/en/stable/patterns/javascript/#receiving-json-in-views\n\n```\nReceiving JSON in Views\n\nUse the [json](https://flask.palletsprojects.com/en/stable/api/#flask.Request.json) property of the [request](https://flask.palletsprojects.com/en/stable/api/#flask.request) object to decode the request\u2019s body as JSON. If the body is not valid JSON, or the Content-Type header is not set to application/json, a 400 Bad Request error will be raised\n```\nI believe you want to indicate that it's a 415 instead of 400 when the Content-Type header doesn't match as it's done on https://flask.palletsprojects.com/en/stable/api/#flask.Request.json\n\n400 is still valid for when the body isn't json (even though the content-type is json).\n\nSo maybe something like this ? \n```\nReceiving JSON in Views\n\nUse the [json](https://flask.palletsprojects.com/en/stable/api/#flask.Request.json) property of the [request](https://flask.palletsprojects.com/en/stable/api/#flask.request) object to decode the request\u2019s body as JSON. If the body is not valid JSON, a 400 Bad Request error will be raised.  If the Content-Type header is not set to application/json, a 415 Unsupported Media Type error will be raised\n```\n\nEnvironment:\n\n- Python version: N/A\n- Flask version: latest doc\n\n\n\nThis is in no way a bug, but it's for being aligned everywhere in your doc :).  I'm just trying to make things better, I love the product ! thanks for the great work you've done !"
  },
  {
    "name": "prosper_web.py",
    "description": "<!--\nThis issue tracker is a tool to address bugs in Flask itself. Please use\nGitHub Discussions or the Pallets Discord for questions about your own code.\n\nReplace this comment with a clear outline of what the bug is.\n-->\n\n<!--\nDescribe how to replicate the bug.\n\nInclude a minimal reproducible example that demonstrates the bug.\nInclude the full traceback if there was an exception.\n-->\n\n<!--\nDescribe the expected behavior that should have happened but didn't.\n-->\n\nEnvironment:\n\n- Python version:\n- Flask version:"
  },
  {
    "name": "Enrich Flask\u2019s public API type hints using typing.Annotated with Doc metadata",
    "description": "## Feature Request\n\n### Summary\n\nEnhance selected public Flask APIs (for example `add_url_rule` and `route`) by adding\n`typing.Annotated` with `Doc` metadata **and `Literal` types for HTTP methods** to provide\nricher IDE assistance and stronger static type checking.\n\n### Example\n\n```python\nfrom typing import Annotated, Literal\nfrom typing_extensions import Doc\n\n# Strict HTTP method type for auto-complete and validation\nMethod = Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"OPTIONS\", \"HEAD\"]\n\ndef add_url_rule(\n    rule: Annotated[str, Doc(\"URL path such as '/home' or '/api/items/<int:id>'\")],\n    *,\n    methods: Annotated[list[Method], Doc(\"Allowed HTTP methods, e.g. ['GET', 'POST']\")],\n    ...\n) -> None:\n    ...\n```\n\n### Benefits\n\n* **Better editor/IDE hints and documentation** for route paths and HTTP methods.\n* **Static validation of HTTP verbs** so type checkers catch typos like `\"GEET\"`.\n* **Backward compatible** with existing type checkers and runtime behavior (`typing_extensions.Annotated` and `Literal` are supported across Python versions Flask supports).\n\n### Questions for Maintainers\n\n* Would you accept `Annotated` + `Doc` and `Literal` types in public API signatures?\n* If yes, should I begin with a small pilot (e.g., only `add_url_rule`) before expanding to other decorators such as `route`?"
  },
  {
    "name": "asyncio is not compatible with gevent",
    "description": "See #5256\n\nasync-await: <https://flask.palletsprojects.com/en/stable/async-await/>\n\n> Using async with greenlet\n> When using gevent or eventlet to serve an application or patch the runtime, greenlet>=1.0 is required. When using PyPy, PyPy>=7.3.7 is required.\n\nFlask uses asgiref, which is not compatible with gevent: <https://github.com/django/asgiref/issues/443>\n\n\nBTW:\nASGI: <https://flask.palletsprojects.com/en/stable/deploying/asgi/>\n\n> The asgiref [WsgiToAsgi](https://github.com/django/asgiref#wsgi-to-asgi-adapter) adapter is recommended as it integrates with the event loop used for Flask\u2019s [Using async and await](https://flask.palletsprojects.com/en/stable/async-await/#async-await) support.\n\nThis makes it seem like the event loop integration means a single worker could process multiple requests concurently, which it can't, the worker will block on `await`.\n\n---\n\nI'm not the only one that found this the hard way: [Flask, Gunicorn, Gevent and Asyncio Don't Mix](https://github.com/moogah/flask-gunicorn-gevent-asyncio/blob/29be08051cd663fa68d982f27232d2a5215a399c/README.md)"
  },
  {
    "name": "deprecate `should_ignore_error`",
    "description": "This was added in f1918093ac70d589a4d67af0d77140734c06c13d as part of the original code to keep the context around for use in the debugger, tests, etc. It was not part of a PR, and there's no linked issue or explanation on why it was added.\n\nThe intention seems to be to allow ignoring certain errors during debugging, so that cleanup is still run immediately. That's not how context preservation works anymore.  It also causes the exception to not be passed to teardown handlers, but there doesn't seem to be any reason to hide that, and handlers can already choose what to do if they're passed an error.\n\nThe method is only documented in the API, not in any other pages. There's no test for it. I have a feeling this isn't used. It results in an extra function call every single request, only to always return false. This can be deprecated then removed."
  },
  {
    "name": "pass context internally instead of using contextvars",
    "description": "Currently, there are a bunch of different methods on the `Flask` class that run to dispatch each request. Many of these access `request` and other context proxies. We should update them to pass the `AppContext` everywhere instead. This is more convenient after #5812 with only one context object instead of two. As @pgjones pointed out in #5229, not having to access the contextvar is a significant speedup to ASGI Quart, although it doesn't appear to affect WSGI Flask as much. Perhaps if we were serving with greenlets and so contexts were switching more, it would be more noticeable in Flask too.\n\nThe obvious problem is that it is a breaking change to the signatures of all these methods. I'm unsure how many methods are affected, but I'm also unsure how many projects are even subclassing `Flask` to override any of the methods. The fact that the methods are public seems unintentional, compared to the much more common ways of configuring and customizing an app.\n\nWe could continue to support both signatures, showing a deprecation warning for the old one, by adding some detection to `Flask.__init__`. This would add some amount of time during app setup, but wouldn't affect runtime performance of the app.\n\n---\n\nAnother idea I had for the future was to have `@route` and other decorators inspect the signature of the decorated function, to allow injecting `request`, `app`, etc. if users would rather use that pattern than import the proxies. Having the request object directly available in all the dispatch methods would make this more straightforward. User code would be able to choose what pattern they want, the current proxy pattern would never be deprecated for user code."
  },
  {
    "name": "3.1.2 regression: `stream_with_context` triggers `teardown_request()` calls before response generation",
    "description": "<!--\nThis issue tracker is a tool to address bugs in Flask itself. Please use\nGitHub Discussions or the Pallets Discord for questions about your own code.\n\nReplace this comment with a clear outline of what the bug is.\n-->\n\n<!--\nDescribe how to replicate the bug.\n\nInclude a minimal reproducible example that demonstrates the bug.\nInclude the full traceback if there was an exception.\n-->\n\n<!--\nDescribe the expected behavior that should have happened but didn't.\n-->\nHello,\n\nI believe the changes to `stream_with_context()` in https://github.com/pallets/flask/pull/5799/commits/9822a0351574790cb66c652fcc396ad7aa2b09d8 introduced a bug where the `teardown_request()` callables are invoked too early in the request/response lifecycle (and actually invoked twice, before generating the response and a second time after the end of the request). Take the following example:\n\n```python\n# flask_teardown_stream_with_context.py\nfrom flask import Flask, g, stream_with_context\n\n\ndef _teardown_request(_):\n    print(\"do_teardown_request() called\")\n    g.pop(\"hello\")\n\n\napp = Flask(__name__)\n\napp.teardown_request(_teardown_request)\n\n\n@app.get(\"/stream\")\ndef streamed_response():\n    g.hello = \"world\"\n\n    def generate():\n        print(\"Starting to generate response\")\n        yield f\"<p>Hello {g.hello} !</p>\"\n\n    return stream_with_context(generate())\n\n\napp.run(debug=True)\n```\n\nIn 3.1.1:\n\n```\n% /tmp/venv/bin/flask --version           \nPython 3.13.7\nFlask 3.1.1\nWerkzeug 3.1.3\n% /tmp/venv/bin/python flask_teardown_stream_with_context.py \n[\u2026]\nStarting to generate response\n127.0.0.1 - - [01/Sep/2025 16:07:05] \"GET /stream HTTP/1.1\" 200 -\ndo_teardown_request() called\n```\n\nIn 3.1.2:\n\n```\n% /tmp/venv/bin/flask --version                             \nPython 3.13.7\nFlask 3.1.2\nWerkzeug 3.1.3\n% /tmp/venv/bin/python flask_teardown_stream_with_context.py\ndo_teardown_request() called\nStarting to generate response\ndo_teardown_request() called\nDebugging middleware caught exception in streamed response at a point where response headers were already sent.\nTraceback (most recent call last):\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/helpers.py\", line 132, in generator\n    yield from gen\n  File \"/tmp/flask_teardown_stream_with_context.py\", line 21, in generate\n    yield f\"<p>Hello {g.hello} !</p>\"\n                      ^^^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/ctx.py\", line 56, in __getattr__\n    raise AttributeError(name) from None\nAttributeError: hello\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/venv/lib/python3.13/site-packages/werkzeug/wsgi.py\", line 256, in __next__\n    return self._next()\n           ~~~~~~~~~~^^\n  File \"/tmp/venv/lib/python3.13/site-packages/werkzeug/wrappers/response.py\", line 32, in _iter_encoded\n    for item in iterable:\n                ^^^^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/helpers.py\", line 130, in generator\n    with app_ctx, req_ctx:\n                  ^^^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/ctx.py\", line 443, in __exit__\n    self.pop(exc_value)\n    ~~~~~~~~^^^^^^^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/ctx.py\", line 410, in pop\n    self.app.do_teardown_request(exc)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/app.py\", line 1356, in do_teardown_request\n    self.ensure_sync(func)(exc)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/tmp/flask_teardown_stream_with_context.py\", line 7, in _teardown_request\n    g.pop(\"hello\")\n    ~~~~~^^^^^^^^^\n  File \"/tmp/venv/lib/python3.13/site-packages/flask/ctx.py\", line 88, in pop\n    return self.__dict__.pop(name)\n           ~~~~~~~~~~~~~~~~~^^^^^^\nKeyError: 'hello'\n127.0.0.1 - - [01/Sep/2025 16:09:35] \"GET /stream HTTP/1.1\" 200 -\n```\n\nSpecifically,\n\n```\ndo_teardown_request() called\nStarting to generate response\ndo_teardown_request() called\n```\n\nSo `_teardown_request()` is called before flask start to iterate on the response generator.\n\nThis is a simplified version of our own code; I'm not sure we can actually expect `g` to still be available during response generators, but given it worked in 3.1.1 and the phrasing / intent of `teardown_request()`, I'd expect it not to be called before the response is actually generated. Note also that removing the code which causes the error, i.e. the `g` access, and keeping just the `print()` for debugging, will still show `_teardown_request()` being called twice.\n\nIt's not obvious to me where exactly the bug is triggered. Adding a `traceback.print_stack()` call to `_teardown_request()`:\n* in 3.1.1, the only call, once the request is done, is triggered by https://github.com/pallets/flask/blob/7fff56f5172c48b6f3aedf17ee14ef5c2533dfd1/src/flask/helpers.py#L115 \u21d2 https://github.com/pallets/flask/blob/330123258e8c3dc391cbe55ab1ed94891ca83af3/src/flask/ctx.py#L443 \n* in 3.1.2, the (new) first call before entering the response generator is triggered by https://github.com/pallets/flask/blob/330123258e8c3dc391cbe55ab1ed94891ca83af3/src/flask/app.py#L1527 ; the second is similar to 3.1.1, i.e. https://github.com/pallets/flask/blob/330123258e8c3dc391cbe55ab1ed94891ca83af3/src/flask/helpers.py#L130 \u21d2 https://github.com/pallets/flask/blob/330123258e8c3dc391cbe55ab1ed94891ca83af3/src/flask/ctx.py#L443\n\nEnvironment:\n\n- Python version: 3.13\n- Flask version: 3.1.2"
  },
  {
    "name": "`flask.testing` misleadingly appears to import `TestResponse`",
    "description": "`testing.py` contains the following:\n```python\nif t.TYPE_CHECKING:  # pragma: no cover\n    from werkzeug.test import TestResponse\n```\nAs such, any IDE or static checker will consider this to be valid code:\n```python\nimport flask.testing as _flask_test\n\ndef my_test_helper(flask_test_client: _flask_test.FlaskClient) -> _flask_test.TestResponse:\n    ...\n```\n\nHowever, at runtime, this code will fail, because `TestResponse` is not actually imported into the `flask.testing` namespace.\n\nPossible solutions:\n- Move `from werkzeug.test import TestResponse` outside the `if t.TYPE_CHECKING` block; you're already importing `werkzeug.test` in that file anyway\n- Remove `from werkzeug.test import TestResponse` entirely and replace `TestResponse` with `werkzeug.test.TestResponse` in type-hints\n- Change `from werkzeug.test import TestResponse` to `from werkzeug.test import TestResponse as _TestResponse`; the leading underscore signals marks the import as private, at which point it doesn't matter if its really imported or not because nobody should be using it from outside the file"
  },
  {
    "name": "Test",
    "description": "<!--\nThis issue tracker is a tool to address bugs in Flask itself. Please use\nGitHub Discussions or the Pallets Discord for questions about your own code.\n\nReplace this comment with a clear outline of what the bug is.\n-->\n\n<!--\nDescribe how to replicate the bug.\n\nInclude a minimal reproducible example that demonstrates the bug.\nInclude the full traceback if there was an exception.\n-->\n\n<!--\nDescribe the expected behavior that should have happened but didn't.\n-->\n\nEnvironment:\n\n- Python version:\n- Flask version:"
  },
  {
    "name": "Session is not updated on redirect target endpoint in tests",
    "description": "Hello team.\nDuring writing tests, I noticed a strange session behavior.\n\nI want to verify that after a redirect, the target location sets valid session properties, and it seems like in tests, the session properties when following redirects are not saved/exposed to the test environment.\n\nHere's an example:\n```python\nimport pytest\nfrom flask import Flask, redirect, session\n\n@pytest.fixture\ndef app():\n    app = Flask(__name__)\n    app.debug = True\n    app.secret_key = 'test_secret'  # noqa: S105 \u2013 fake secret key for testing purposes\n\n    @app.get('/redirect')\n    def do_redirect():\n        print(\"Redirecting to target endpoint\", flush=True)\n        session[\"redirected\"] = \"true\"\n        return redirect('/target', 302)\n\n    @app.get('/target')\n    def target():\n        print(\"Target endpoint reached\", flush=True)\n        session['foo'] = 'bar'\n        return 'Target', 200\n\n    return app\n\n@pytest.fixture()\ndef client(app):\n    return app.test_client()\n\ndef test_session(client):\n    with client:\n        response = client.get(\"/redirect\", follow_redirects=True)\n\n        print(f\"Session after a redirect: {session.items()}\", flush=True)\n        assert response.status_code == 200\n        assert response.data == b\"Target\"\n\n        assert session.get(\"redirected\") == \"true\"\n        assert session.get('foo') == 'bar'\n\n        client.get(\"/target\", follow_redirects=True)\n        assert session.get('foo') == 'bar'\n        assert response.data == b\"Target\"\n        print(f\"Session after a direct request: {session.items()}\", flush=True)\n```\n\nThe test fails on the first assertion `assert session.get('foo') == 'bar'`. \nIf I comment it out, the test output is the following:\n\n```\nRedirecting to the target endpoint\nTarget endpoint reached\nSession after a redirect: dict_items([('redirected', 'true')])\nTarget endpoint reached\nSession after a direct request: dict_items([('foo', 'bar'), ('redirected', 'true')])\n```\n\n**IMPORTANT**: The described behavior is not reproduced when running the application with the real HTTP requests.\n\nEnvironment:\n\n- Python version: 3.13\n- Flask version: 3.10"
  },
  {
    "name": "Add a \"Scroll to Top\" Button for Long Documentation Pages",
    "description": "Some pages in the Flask documentation\u2014such as [API](https://flask.palletsprojects.com/en/stable/api/) and [Configuration Handling](https://flask.palletsprojects.com/en/stable/config/)\u2014are very long, requiring users to manually scroll all the way back up to access the top navigation links.\n\nAdding a persistent \"Scroll to Top\" button (e.g. fixed to the bottom-left corner) would enhance usability by making navigation easier for users reading on smaller screens or mobile devices. This is a common accessibility and UX pattern on many documentation sites and could improve the overall reading experience.\n\nThis proposal does not affect Flask\u2019s core library\u2014it\u2019s a suggestion for the documentation site UI only. I\u2019m happy to help with a pull request if the maintainers are open to the addition."
  },
  {
    "name": "Looser type annotations for send_file() path_or_file argument",
    "description": "`path_or_file` argument of `flask.helpers.send_file` is typed as `os.PathLike[t.AnyStr] | str | t.BinaryIO`. This prevents passing some objects that are `t.IO[bytes]`, but not `t.BinaryIO`. The underlying`werkzeug.utils.send_file` already allows `t.IO[bytes]` due to pallets/werkzeug#2209 since version 2.0.2.\n\n\nReproduction:\n```python\nfrom tempfile import NamedTemporaryFile\nfrom typing import IO\n\nfrom flask.helpers import send_file\n\n\n# The return type cannot be \"BinaryIO\" because \"NamedTemporaryFile\" is incompatible with it according to Mypy.\ndef some_function() -> IO[bytes]:\n    file = NamedTemporaryFile()\n    ...\n    return file\n\n\nfile = some_function()\nsend_file(file)\n```\n\nRaises the following exception with Mypy 1.16.1.\n```\nerror: Argument 1 to \"send_file\" has incompatible type \"IO[bytes]\"; expected \"PathLike[str] | str | BinaryIO\"  [arg-type]\n```\n\nI could simply change the return value of `some_function` to `_TemporaryFileWrapper[bytes]` or cast it to `BinaryIO`.\nHowever, I would like to allow `t.IO[bytes]`.\n\nSide note: `_TemporaryFileWrapper[bytes]` conforms to `PathLike[str]` due to python/typeshed#7840. That is why it is accepted by `flask.helpers.send_file`.\n\nAllowing `t.IO[bytes]` would be backwards compatible change as all `t.BinaryIO` are also `t.IO[bytes]`.\n[Mypy Playground](https://mypy-play.net/?mypy=latest&python=3.10&gist=4d7e2a43a847df9d332f5b7daba7a9fe)\n\nEnvironment:\n\n- Python version: 3.12.11\n- Flask version: 3.1.1"
  },
  {
    "name": "`stream_with_context` does not work with async routes",
    "description": "Consider this trivial route + test in `app.py`:\n\n```python\nimport flask\nfrom flask import Flask\nfrom flask import Response\n\nimport pytest\n\napp = Flask(__name__)\n\n\n@app.route(\"/foo\")\nasync def foo():\n    def gen():\n        yield \"bar\"\n\n    return Response(flask.stream_with_context(gen()))\n\n\ndef test_foo():\n    with app.test_client() as client:\n        client.get(\"/foo\")\n```\n\nWith the following `requirements.txt`:\n\n```\nflask[async]==3.1.1\npytest==8.4.1\nwerkzeug==3.1.3\n```\n\nRunning `pytest app.py` results in:\n\n```\n============================= test session starts ==============================\nplatform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0\nrootdir: /private/tmp/stream_with_context\ncollected 1 item\n\napp.py F                                                                 [100%]\n\n=================================== FAILURES ===================================\n___________________________________ test_foo ___________________________________\n\n    def test_foo():\n        with app.test_client() as client:\n>           client.get(\"/foo\")\n\napp.py:20:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nvenv/lib/python3.12/site-packages/werkzeug/test.py:1162: in get\n    return self.open(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.12/site-packages/flask/testing.py:235: in open\n    response = super().open(\nvenv/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\n    app_rv = app(environ, start_response)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.12/site-packages/flask/app.py:1536: in __call__\n    return self.wsgi_app(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nvenv/lib/python3.12/site-packages/flask/app.py:1527: in wsgi_app\n    ctx.pop(error)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <RequestContext 'http://localhost/foo' [GET] of app>, exc = None\n\n    def pop(self, exc: BaseException | None = _sentinel) -> None:  # type: ignore\n        \"\"\"Pops the request context and unbinds it by doing that.  This will\n        also trigger the execution of functions registered by the\n        :meth:`~flask.Flask.teardown_request` decorator.\n\n        .. versionchanged:: 0.9\n           Added the `exc` argument.\n        \"\"\"\n        clear_request = len(self._cv_tokens) == 1\n\n        try:\n            if clear_request:\n                if exc is _sentinel:\n                    exc = sys.exc_info()[1]\n                self.app.do_teardown_request(exc)\n\n                request_close = getattr(self.request, \"close\", None)\n                if request_close is not None:\n                    request_close()\n        finally:\n            ctx = _cv_request.get()\n            token, app_ctx = self._cv_tokens.pop()\n>           _cv_request.reset(token)\nE           ValueError: <Token var=<ContextVar name='flask.request_ctx' at 0x103cd9a30> at 0x104b23a40> was created in a different Context\n\nvenv/lib/python3.12/site-packages/flask/ctx.py:418: ValueError\n=========================== short test summary info ============================\nFAILED app.py::test_foo - ValueError: <Token var=<ContextVar name='flask.request_ctx' at 0x103cd9a30>...\n============================== 1 failed in 0.11s ===============================\n```\n\nSimilarly, `python -m flask run` and then `curl http://127.0.0.1:5000/foo` results in:\n\n```\n127.0.0.1 - - [15/Jul/2025 13:40:14] \"GET /foo HTTP/1.1\" 500 -\nError on request:\nTraceback (most recent call last):\n  File \"/private/tmp/stream_with_context/venv/lib/python3.12/site-packages/werkzeug/serving.py\", line 370, in run_wsgi\n    execute(self.server.app)\n  File \"/private/tmp/stream_with_context/venv/lib/python3.12/site-packages/werkzeug/serving.py\", line 331, in execute\n    application_iter = app(environ, start_response)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/private/tmp/stream_with_context/venv/lib/python3.12/site-packages/flask/app.py\", line 1536, in __call__\n    return self.wsgi_app(environ, start_response)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/private/tmp/stream_with_context/venv/lib/python3.12/site-packages/flask/app.py\", line 1527, in wsgi_app\n    ctx.pop(error)\n  File \"/private/tmp/stream_with_context/venv/lib/python3.12/site-packages/flask/ctx.py\", line 418, in pop\n    _cv_request.reset(token)\nValueError: <Token var=<ContextVar name='flask.request_ctx' at 0x1029ccea0> at 0x1049e1980> was created in a different Context\n```\n\nIf the route is defined `def foo():` without `async`, then this works without issue.\n\nOr if we remove `stream_with_context` and just `return Response(gen())`, also works.\n\nBut the combination of `async` + `stream_with_context` fails.\n\nThere's also no issue with e.g. flask 2.1.0; I believe this was introduced in flask 2.2.0, specifically by #4682.\n\nEnvironment:\n\n- Python version: 3.12.11\n- Flask version: 3.1.1"
  },
  {
    "name": "Flask documentation website does not support dark mode",
    "description": "**Describe the issue**\nThe official Flask documentation at https://flask.palletsprojects.com currently does not support dark mode. For many developers, especially those working at night or with light sensitivity, this can create accessibility and eye strain issues.\n\n**Expected behavior**\nIt would be helpful if the documentation site could support an optional dark theme, either by detecting system preferences (prefers-color-scheme) or allowing manual switching.\n\n**Additional context**\nMany popular frameworks like: \n- Django\n- FastAPI\n- Pydantic\n- Typer\nhave added dark mode support in their docs, improving readability and user comfort."
  },
  {
    "name": "Tt",
    "description": "<!--\nReplace this comment with a description of what the feature should do.\nInclude details such as links to relevant specs or previous discussions.\n-->\n\n<!--\nReplace this comment with an example of the problem which this feature\nwould resolve. Is this problem solvable without changes to Flask, such\nas by subclassing or using an extension?\n-->"
  },
  {
    "name": "document that `request.full_path` leaves query string percent encoded",
    "description": "WHAT IS THE BUG?\n\n`request.full_path` is documented as *\u201cRequested path as unicode, including the query string.\u201d*  \nHowever, Werkzeug URL-unquotes only the **path** (before \u201c?\u201d); the **query string** is returned **raw**.  \nCode that searches or whitelists on `full_path` can therefore be bypassed by percent-encoding a slash inside a parameter.\n\n----------------------------------------------------------------------------------------------------------------\n\nHOW TO REPLICATE\n\n```\n```python\nfrom flask import Flask, request\napp = Flask(__name__)\n\n@app.route(\"/show_picture\")\ndef show_picture():\n    # na\u00efve traversal filter\n    if \"../\" in request.full_path:\n        return \"Blocked\", 400\n    return f\"full_path = {request.full_path}\", 200\n\n# run:  flask --app poc run\n\n\n# bypass: \"../\" hidden as \"%2F\"\ncurl \"http://127.0.0.1:5000/show_picture?img=..%2F..%2Fetc%2Fpasswd\"\nResult \u2192 200 OK; the filter misses the traversal because ../ appears as ..%2F.\n```\n\n----------------------------------------------------------------------------------------------------------------\n\n\nEXPECTED BEHAVIOR\nDocumentation (and the full_path docstring) should warn clearly:\n\n\"Only the portion before \u201c?\u201d is URL-unquoted. The query string is returned raw; unquote it manually (e.g. urllib.parse.unquote_plus) before using it for validation or filtering.\"\n\nWithout that notice, filters like\n\nif \"../\" in request.full_path:\n    ...\nif request.full_path.startswith(\"/admin\"):\n    ...\n\ncan be bypassed with %2F inside the query string.\n\n----------------------------------------------------------------------------------------------------------------\n\nPython version: 3.12.3\n\nFlask version: 2.3.3\n\nWerkzeug version: 3.1.3\n\nOS: Ubuntu 24.04 LTS\n\n----------------------------------------------------------------------------------------------------------------\n\nAcknowledgements\nDiscovery: Jefferson Gozanels GHomez\nProof-of-concept & report: Sebasti\u00e1n Alba Vives"
  },
  {
    "name": "404 Flask cannot find /security/login API",
    "description": "We are using airflow 2.8.4, and recently we upgrade it to 2.10.5\n\n### Environment for Airflow 2.8.4\n\n- Python version: 3.10\n- Flask version: \n\n```\nFlask-AppBuilder==4.3.11\nFlask-Babel==2.0.0\nFlask-Bcrypt==1.0.1\nFlask-Caching==2.1.0\nFlask-JWT-Extended==4.6.0\nFlask-Limiter==3.5.1\nFlask-Login==0.6.3\nFlask-SQLAlchemy==2.5.1\nFlask-Session==0.5.0\nFlask-WTF==1.2.1\nFlask==2.2.5\n```\n\nCall /security/login API:\n\n<img width=\"1541\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ef976819-1ac1-4bd6-b84b-3ceab844a998\" />\n\n\n### Environment for Airflow 2.10.5\n\n- Python version: 3.10\n- Flask version: \n\n```\nFlask-AppBuilder==4.5.2\nFlask-Babel==2.0.0\nFlask-Bcrypt==1.0.1\nFlask-Caching==2.3.0\nFlask-JWT-Extended==4.7.1\nFlask-Limiter==3.10.1\nFlask-Login==0.6.3\nFlask-SQLAlchemy==2.5.1\nFlask-Session==0.5.0\nFlask-WTF==1.2.2\nFlask==2.2.5\n```\n\nCall /security/login API:\n\n<img width=\"1552\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e6ca32db-cb56-49e6-a266-8edb82e8d0b2\" />\n\n\nError Log:\n```\n{\n  \"detail\": \"The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\",\n  \"status\": 404,\n  \"title\": \"Not Found\",\n  \"type\": \"about:blank\"\n}\n```\n\n/security/login API in  Flask-AppBuilder==4.5.2\n\nhttps://github.com/dpgaspar/Flask-AppBuilder/blob/v4.5.2/flask_appbuilder/security/api.py#L32\n\n/security/login API in  Flask-AppBuilder==4.3.11\n\nhttps://github.com/dpgaspar/Flask-AppBuilder/blob/v4.3.11/flask_appbuilder/security/api.py#L32"
  },
  {
    "name": "Flask cannot find /security/logic API",
    "description": "We are using airflow 2.8.4, and recently we upgrade it to 2.10.5\n\nEnvironment for Airflow 2.8.4\n\n- Python version: 3.10\n- Flask version: \n\n```\nFlask-AppBuilder==4.5.2\nFlask-Babel==2.0.0\nFlask-Bcrypt==1.0.1\nFlask-Caching==2.3.0\nFlask-JWT-Extended==4.7.1\nFlask-Limiter==3.10.1\nFlask-Login==0.6.3\nFlask-SQLAlchemy==2.5.1\nFlask-Session==0.5.0\nFlask-WTF==1.2.2\nFlask==2.2.5\n```\n\nCall /security/logic API:\n\n<img width=\"1541\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ef976819-1ac1-4bd6-b84b-3ceab844a998\" />\n\n\nEnvironment for Airflow 2.10.5\n\n- Python version: 3.10\n- Flask version: \n\n```\nFlask-AppBuilder==4.3.11\nFlask-Babel==2.0.0\nFlask-Bcrypt==1.0.1\nFlask-Caching==2.1.0\nFlask-JWT-Extended==4.6.0\nFlask-Limiter==3.5.1\nFlask-Login==0.6.3\nFlask-SQLAlchemy==2.5.1\nFlask-Session==0.5.0\nFlask-WTF==1.2.1\nFlask==2.2.5\n```\n\nCall /security/logic API:\n\n<img width=\"1552\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e6ca32db-cb56-49e6-a266-8edb82e8d0b2\" />"
  },
  {
    "name": "Typing mismatch between Flask and Werkzeug \"Response\" objects",
    "description": "There is an odd typing issue where Werkzeug `Response` != Flask `Response`. Please see below:\n\n```python\nfrom flask import Response, redirect, url_for\n\ndef foo() -> Response:\n    return redirect(url_for(\"some.route\"))\n```\n\nThis example returns:\n\n```\nbasedpyright: Type \"Response\" is not assignable to return type \"Response\"                                \n\u00a0\u00a0\"werkzeug.wrappers.response.Response\" is not assignable to \"flask.wrappers.Response\" [reportReturnType]\n```\n\nEnvironment:\n\n- Python version: 3.13\n- Flask version: 3.1.1"
  },
  {
    "name": "MAX_FORM_MEMORY_SIZE not being picked up correctly",
    "description": "## ISSUE\nSetting app.config[\"MAX_FORM_MEMORY_SIZE\"] = VALUE\ndoes not have any effect on the actual configuration and the default 500000 bytes from werkzeug request is stilled used\n\n\n## REPLICATION\nSetup a minimum flask project\n\napp = Flask(__name__)\napp.config[\"MAX_FORM_MEMORY_SIZE\"] = 1024 * 1024 * 1024\n\nprint(app.request_class.max_form_memory_size)\n\n## MISSBEHAVIOUR\nI would expect if I set the MAX FORM MEMORY SIZE in my config it is properly applied and I can send requests with a content length appropriate to the setting but flask defaults to 500kb\n\nEnvironment:\n\n- Python version:  3.11\n- Flask version: 2.3.3"
  },
  {
    "name": "Model Context Protocol (MCP) Support for Flask",
    "description": "## Feature Request: Model Context Protocol (MCP) Support for Flask\n\n### Description\n\nThis feature request proposes adding native support for the Model Context Protocol (MCP) to Flask. MCP is an open protocol that enables secure connections between host applications and AI model providers, allowing for standardized communication patterns in AI-powered applications.\n\n**Key aspects of MCP integration would include:**\n- Built-in MCP server capabilities for Flask applications\n- Middleware for handling MCP message routing\n- Authentication and authorization mechanisms for MCP connections\n- WebSocket and HTTP transport support as per MCP specification\n- Integration with Flask's existing request/response cycle\n\n**Relevant links:**\n- [Model Context Protocol Specification](https://modelcontextprotocol.io/introduction)\n- [MCP GitHub Repository](https://github.com/modelcontextprotocol/python-sdk)\n\n### Problem Statement\n\nCurrently, developers who want to build AI-powered Flask applications that communicate via MCP must implement the protocol manually or rely on third-party libraries. This creates several challenges:\n\n1. **Boilerplate Code**: Developers need to write significant boilerplate to handle MCP message formatting, routing, and transport\n2. **Inconsistent Implementation**: Without standardized Flask integration, different projects implement MCP differently\n3. **Security Concerns**: Manual implementation may miss important security considerations built into the MCP spec\n4. **Maintenance Overhead**: Keeping custom MCP implementations up-to-date with protocol changes\n\n**Example use case:**\nA Flask application serving as an AI assistant backend needs to:\n- Accept MCP connections from client applications\n- Handle tool calls and resource requests\n- Maintain secure, authenticated sessions\n- Process streaming responses\n\nCurrently, this requires extensive custom code that could be standardized.\n\n### Proposed Solution\n\nAdd Flask-MCP as either a core extension or built-in functionality that provides:\n\n```python\nfrom flask import Flask\nfrom flask_mcp import MCPServer\n\napp = Flask(__name__)\nmcp = MCPServer(app)\n\n@mcp.tool(\"get_weather\")\ndef get_weather(location: str) -> str:\n    # Tool implementation\n    return f\"Weather in {location}\"\n\n@mcp.resource(\"user_preferences\")\ndef get_user_prefs():\n    # Resource implementation\n    return {\"theme\": \"dark\"}\n```\n\nThis would make Flask a first-class citizen in the MCP ecosystem alongside other frameworks.\n\n### Alternative Approaches\n\nWhile this could be implemented as a third-party extension (and some exist), native Flask support would:\n- Ensure better integration with Flask's ecosystem\n- Provide official maintenance and security updates  \n- Encourage broader MCP adoption in Python web development\n- Offer performance optimizations through deeper Flask integration\n\nWhat do you think?  \ud83d\ude80"
  },
  {
    "name": "Allow multiple template folders for large projects",
    "description": "Jinja2 supports multiple search paths for templates so Flask should do the same. This will allow template inheriting without copying files.\n\n\nhttps://github.com/pallets/jinja/blob/main/src/jinja2/loaders.py#L190"
  },
  {
    "name": "The `template_filter` decorator doesn't work if you don't pass an argument",
    "description": "## What's the issue?\n\nYou can use `template_filter` as a decorator, but it only registers the filter if you write an explicit name or an empty set of parentheses. If you call it without parens, the filter doesn't get registered.\n\nIt's a small difference and can be confusing.\n\n## Minimal example\n\nConsider the following program:\n\n```python\nfrom flask import Flask, render_template_string\n\n\napp = Flask(__name__)\n\n\n@app.template_filter\ndef double(x):\n    return x * 2\n\n\n@app.route(\"/\")\ndef index():\n    return render_template_string(\"2 times 2 is {{ 2 | double }}\")\n```\n\nIf you run this app (`flask run --port 8008 --debug`) and then open it in your browser (`http://localhost:8008`) you'll get an error:\n\n```\njinja2.exceptions.TemplateAssertionError: No filter named 'double'.\n```\n\nThis is confusing, and it took me a while to realise the missing parentheses in `app.template_filter` were at fault.\n\n## Suggested fix\n\nI think it would be helpful if the decorator either:\n\n* Supported being called without parentheses, or\n* Printed an explicit warning if called this way, e.g. `Did you use 'template_filter' as a decorator without parentheses? You need to call it with 'template_filter()'`\n\nThis is caught by type checkers, but not everybody type checks their Python and the error message is less obvious:\n\n```\nArgument 1 to \"template_filter\" of \"App\" has incompatible type \"Callable[[Any], Any]\"; expected \"str | None\"\n```\n\nI've had a look at the relevant code, and I'd be happy to provide a patch if you think this is a useful change.\n\n## Environment\n\n- Python version: Python 3.11.11\n- Flask version: Flask 3.1.0"
  },
  {
    "name": "Adding Architecture Diagram Contribution",
    "description": "Hello maintainers!\nI've created an architecture diagram that visualizes Flask's core components and their relationships. This visualization could help new contributors understand the codebase structure and assist users in grasping Flask's high-level architecture.\n\n![Image](https://github.com/user-attachments/assets/143025cf-6094-4c19-93f5-ac06bfa59738)\n\n[Flask Architecture Diagram on Kloudfarm.io](https://app.kloudfarm.io/share/379c2674f0b240c387a5135690c8f01c8c18a988878e99846fb36ea18ad7e1ef)\nThe diagram shows:\n- Core Flask classes and relationships\n- Request/response lifecycle\n- Extension integration points\n- Blueprint organization\n\nI'd be happy to:\n\n1. Submit a PR to add this to documentation\n2. Provide different file formats\n3. Create more focused diagrams of specific subsystems\n\nWould this be a helpful addition to the project? Open to any feedback or suggestions.\nThanks!"
  },
  {
    "name": "Ihmuukin_ai",
    "description": "<!--\nThis issue tracker is a tool to address bugs in Flask itself. Please use\nGitHub Discussions or the Pallets Discord for questions about your own code.\n\nReplace this comment with a clear outline of what the bug is.\n-->\n\n<!--\nDescribe how to replicate the bug.\n\nInclude a minimal reproducible example that demonstrates the bug.\nInclude the full traceback if there was an exception.\n-->\n\n<!--\nDescribe the expected behavior that should have happened but didn't.\n-->\n\nEnvironment:\n\n- Python version:\n- Flask version:"
  },
  {
    "name": "Recommend Warning and Safer Defaults for url_for(..., _external=True)",
    "description": "Hi Flask team,\n\nWe recently analyzed several Flask-based applications and noticed a recurring security concern related to url_for(..., _external=True) when used in untrusted request contexts. Specifically, since it uses request.host by default to construct the external URL, applications that do not explicitly configure SERVER_NAME or sanitize headers can be vulnerable to host header injection.\n\nTo improve developer awareness and reduce misuse, we suggest: Add a warning to the url_for documentation about the risk of relying on request.host, and recommend the use of SERVER_NAME or trusted_hosts when generating external URLs.\n\nWe\u2019d be happy to help draft the relevant documentation or contribute a pull request if this direction aligns with the maintainers' goals.\n\nBest regards,\nRui Yang and Zhengyu Liu\nJohns Hopkins University"
  },
  {
    "name": "close resources before reload (option --reload)",
    "description": "I have a Flask application that establishes connections to multiple SQLite databases.  When the local development server (werkzeug) is started with option ``--reload`` the connections are not closed on reload, since Python 3.13 this is also logged with a [ResourceWarning](https://docs.python.org/3/whatsnew/3.13.html#sqlite3):\n\n    ResourceWarning: unclosed database in <sqlite3.Connection object at ...>\n\nSince the application (since each thread) establishes various DB connections, the (debug) log is flooded with such warnings during development.\n\nI am aware that the DB connectors can be closed when the context is [torn down](https://flask.palletsprojects.com/en/stable/patterns/sqlite3/), but this is not desired. The DB connections should be open over the entire lifetime of the server (thread).\n\nAnd the problem is not limited to sqlite3, in general, it would be better to close resources before reloading the modules.\n\nIs there perhaps a hook or another chance with which it would be possible to close open resources?\n\n\n-----\n\n*I placed it here as a feature-request .. may we better move this issue to werkzeug?*"
  },
  {
    "name": "Reference cycle caused by exception handling",
    "description": "A reference cycle is introduced by Flask's exception handling mechanism. Specifically, the exception object retains a reference to the stack frame, which in turn holds a reference back to the exception. This creates a reference cycle between the traceback and the exception instance.\n\nThis becomes particularly problematic in applications where the garbage collector is disabled (gc.disable()). Even when the GC is enabled, this cycle puts unnecessary pressure on memory management.\n\nThe issue originates from the following line in the Flask source code: https://github.com/pallets/flask/blob/9225f8bb28d291b1610c8a58e3233aa6bbdc1808/src/flask/app.py#L1507\n\nThe fix is simple: the reference to the exception object should be explicitly released once it is no longer needed.\n\nA minimal reproducible example demonstrating this issue can be found here:\n\ud83d\udc49 https://github.com/tomasz-pankowski/exception-memory-leak/tree/main\n\nEnvironment:\n\n- Python version: any\n- Flask version: 3.1.0 (earlier ones too)"
  },
  {
    "name": "Jija2 security vulnerability",
    "description": "There is a security vulnerability reported for Jinja2 https://github.com/advisories/GHSA-cpwx-vrp4-4pq7\n\nSolution is to update the Jinja2 version to 3.1.6\n\n\nEnvironment:\n\n- Python version: latest\n- Flask version: latest"
  },
  {
    "name": "'pkgutil.get_loader' is removed from Python 3.14",
    "description": "Per What's new in Python 3.14:\n> Remove deprecated pkgutil.get_loader() and pkgutil.find_loader(). These had previously raised a DeprecationWarning since Python 3.12. (Contributed by B\u00e9n\u00e9dikt Tran in gh-97850.)\n\nIt manifests in flask's tests: \n\n```\n______________ ERROR at setup of test_prefix_package_paths[True] _______________\n\nrequest = <SubRequest 'limit_loader' for <Function test_prefix_package_paths[True]>>\nmonkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7f1fe6cb9b70>\n\n    @pytest.fixture(params=(True, False))\n    def limit_loader(request, monkeypatch):\n        \"\"\"Patch pkgutil.get_loader to give loader without get_filename or archive.\n    \n        This provides for tests where a system has custom loaders, e.g. Google App\n        Engine's HardenedModulesHook, which have neither the `get_filename` method\n        nor the `archive` attribute.\n    \n        This fixture will run the testcase twice, once with and once without the\n        limitation/mock.\n        \"\"\"\n        if not request.param:\n            return\n    \n        class LimitedLoader:\n            def __init__(self, loader):\n                self.loader = loader\n    \n            def __getattr__(self, name):\n                if name in {\"archive\", \"get_filename\"}:\n                    raise AttributeError(f\"Mocking a loader which does not have {name!r}.\")\n                return getattr(self.loader, name)\n    \n>       old_get_loader = pkgutil.get_loader\nE       AttributeError: module 'pkgutil' has no attribute 'get_loader'\n\nERROR tests/test_instance_config.py::test_installed_module_paths[True] - Attr...\nERROR tests/test_instance_config.py::test_installed_package_paths[True] - Att...\nERROR tests/test_instance_config.py::test_prefix_package_paths[True] - Attrib...\n```\n\nRun the tests with Python 3.14\n\nEnvironment:\n\n- Python version: 3.14.0a6\n- Flask version: 3.1.0"
  },
  {
    "name": "flask.__init__.__getattr__ can hide typing errors",
    "description": "admittedly this is a bit of an edge case -- but perhaps if the `__getattr__` pattern is implemented next time for deprecations this can be taken into consideration!\n\nI have an unfortunately-named bit in my webapp called \"flash\" and I've used flask for years and my fingers tend to typo one as the other.\n\nI had a file like this (simplified!):\n\n```python\nimport flask\n\nflask.FlashMessage\n```\n\nwhich is obviously wrong -- flask doesn't have a `FlashMessage`!  but it was passing mypy due to `def __getattr__(name: str) -> t.Any:` in `__init__.py`\n\nit _might_ be better to guard that behind a `if not t.TYPE_CHECKING: ...` block such that mypy doesn't pick that up and lead to misleading results\n\nnot a big deal though and I see that the `__getattr__`  is already removed in mainline\n\nanother idea would be to use `@overload` on `__getattr__` such that it only allows the particular Literals? but it looks like mypy doesn't even support that so nevermind! https://github.com/python/mypy/issues/8203#issuecomment-1670344931\n\n\nEnvironment:\n\n- Python version: 3.13.1\n- Flask version: 3.1.0"
  },
  {
    "name": "Is flask still being maintained?",
    "description": "Is flask still being maintained, I see the git was updated 2 month back, can I still use flask for my new project?"
  },
  {
    "name": "Error using sample code in favicon documentation",
    "description": "The code sample gives the assert `view_func is not None, \"expected view func if endpoint is not provided.\"`.\n\nDocumentation in question: https://flask.palletsprojects.com/en/stable/patterns/favicon/\n\nSample code in question:\n```\napp.add_url_rule('/favicon.ico',\n                 redirect_to=url_for('static', filename='favicon.ico'))\n```"
  },
  {
    "name": "Inaccurate documentation for adding a favicon redirect",
    "description": "The code sample in the existing documentation on how to redirect a favicon request assumes that add_url_rule takes a parameter named redirect_to. It does not.\n\nDocumentation in question: https://flask.palletsprojects.com/en/stable/patterns/favicon/\n\nSample code in question:\n```\napp.add_url_rule('/favicon.ico',\n                 redirect_to=url_for('static', filename='favicon.ico'))\n```"
  },
  {
    "name": "How to solve",
    "description": "```python\nresponse = {\n        \"status\": \"ok\",\n        \"message\": \"Success\",\n        \"response\": url\n    }\n    return jsonify(response)\n```\n\nIn this getting output like this \n\n```json\n{\"message\": \"success\", \"response\": url, \"status\": \"old\"}\n```\n\nHow to solve that getting incorrect output\n\nI need this like\n\n```\n{\n        \"status\": \"ok\",\n        \"message\": \"Success\",\n        \"response\": url\n    }\n```\n\nHow to solve"
  },
  {
    "name": "Admin endpoint seems to be reserved",
    "description": "```python\n@app.route(\"/admin/<password>/<int:page>\", methods=[\"GET\"])\ndef admin(password: str, page: int = 1) -> Any:\n    print(page)\n    files, total = [], 2\n    return render_template(\"admin.html\", files=files, password=password, total=total)\n```\n\nThis fails when page is 1,2,3 or 8.\n\nAt least. it might fail in more cases.\n\nIf page is 4 or 5, 9,  etc, it works.\n\nBy works I mean, it uses the actual argument instead of the default: 1\n\nI'm guessing the admin endpoint is being reserved in some way, but should it?\n\nIf I replace 'admin' to 'pok' for example:\n\n`@app.route(\"/pok/<password>/<int:page>\", methods=[\"GET\"])`\n\nThen it works as expected.\n\n\nEnvironment:\n\n- Python version: 3.13.1\n- Flask version: 3.1.0"
  },
  {
    "name": "Custom commands not listed",
    "description": "When I run the `flask` command by itself I get help output with all of my custom commands listed. But when I run `flask --help` I get the help output with only the built-in flask commands (`routes`, `run`, `shell`). I am using the `FLASK_APP` environment variable to load the app.\n\nI would think I should get the same help output (showing my custom commands) whether or not I add the `--help` parameter?\n\nEnvironment:\n\n- Python 3.12.2\n- Flask 3.1.0"
  },
  {
    "name": "int route converters are actually uint, but not documented as such",
    "description": "In #2643, a user noted that flask routes with an `int` converter only accepts unsigned integers.\r\n\r\nI'll summarize a few different opinions and options in that thread:\r\n1. a user can create their own converter that accepts unsigned integers, or modifies the existing converter (but only on their codebase). An example code snippet was provided in the issue.\r\n2. The user's report could be treated as a bug in the `int` converter, so the fix would be to modify the flask `int` converter to accept the other ~50% of legitimate integers. Assumptions were made about which part of the integer space most people would use. Concerns were raised about how users may be relying on this bug, and that it should not be fixed for fear of breaking said users.\r\n3. The user's report could be treated as a bug in the documentation, so a presumed fix would be to amend [the documentation](https://flask.palletsprojects.com/en/stable/api/#url-route-registrations) which currently states \"The following converters are available: [...] `int`: accepts integers\"; it could be changed to \"`int`: accepts _unsigned_ integers\".\r\n\r\nMy expectation would have been that either option 2 or 3 would be selected, but instead it appears that the issue was locked to further comment and no further action was taken. So, the documentation is still at odds with the behavior of flask vs the math and computer science definition of integer. I'm not wedded to either solution, but would the pallets team be open to implementing a fix in either direction?\r\n\r\nThank you,\r\n-Andrew"
  },
  {
    "name": "ok",
    "description": "<!--\r\nReplace this comment with a description of what the feature should do.\r\nInclude details such as links to relevant specs or previous discussions.\r\n-->\r\n\r\n<!--\r\nReplace this comment with an example of the problem which this feature\r\nwould resolve. Is this problem solvable without changes to Flask, such\r\nas by subclassing or using an extension?\r\n-->"
  },
  {
    "name": "Consider supporting HOT reload with `hmr`?",
    "description": "Hot Module Replacement has been existed in JavaScript ecosystem for a long time, but Python web applications are still using full reloads by default.\r\n\r\nTo solve this situation, I implemented the `hmr` package, which does the fine-grained dependency tracking, on-demand reloading jobs. Here is a demo of what the results look like:\r\n\r\nhttps://github.com/user-attachments/assets/7937d1c2-ed41-4f38-be18-9cc29b9961a1\r\n\r\nObviously the `a.py` runs everytime in `flask run --reload` but reloads only when needed in `hmr app.py`. But everything else works fine (at least for this small demo)\r\n\r\nYou may say that user should use lazy imports and cache expensive calculations themselves, but in fact not every function can be cached, and lazy imports may lead to ugly code style. This can be solved with on-demand hot reloading without overhead. So I believe this is the future.\r\n\r\nAlthough this is just a minimal demo, but I value hmr's potential in the Python ecosystem, and believe that with our efforts this is a reachable goal.\r\n\r\n> [!TIP]\r\n> You can try the example above [here](https://github.com/promplate/hmr/tree/readme/examples/flask) with `hmr app.py`\r\n\r\n---\r\n\r\nAbout `hmr`, you can refer to its [README](https://github.com/promplate/hmr) for details."
  },
  {
    "name": "intersphinx broken for flask.Flask.cli",
    "description": "https://flask.palletsprojects.com/en/stable/api/#flask.Flask.cli has a type of `Group`, but it doesn't link to further docs."
  },
  {
    "name": "Containerization",
    "description": "Hello  team I am planning to containerized the entire flask framework using docker so whenever user needs to work on the project they can able to run entire framework just by running docker container so can i work on this feature."
  },
  {
    "name": "Design Enhancement",
    "description": "Hello team, planning to develop flask documentation with web interface with great and clean UI, also trying to make the concept more simple and understandable can you assign me this task."
  },
  {
    "name": "app.run(debug=True)\uff0c\u4f1a\u5bfc\u81f4flask_sqlalchemy\u9519\u8bef!",
    "description": "\u5f00\u542fdebug\u6a21\u5f0f\u540e\uff0cdebug=Ture\u7684\u65f6\u5019\uff0cflask_sqlalchemy\u7684with self.app.app_context()\u4f1a\u91cd\u590d\u6267\u884c\u3002\r\n\u4f8b\u5982\u6211\u5199\u5165\u4e00\u6761\u6570\u636e {user:\"123\",name:\"youname\"}, \u5728debug\u6a21\u5f0f\u4e0b with self.app.app_context() \u4f1a\u6267\u884c\u5b8c\u5199\u5165\u5b8c\u6bd5\u540e\uff0c\u4f1a\u91cd\u65b0\u6267\u884c\u4e00\u6b21\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u9000\u51fa\uff0c\u63d0\u793a\u91cd\u590d\u5199\u5165\uff01\r\n\u6211\u4ee5\u4e3a\u662f\u6211\u4ee3\u7801\u95ee\u9898\uff0c\u6211\u68c0\u67e5\u4e86\u65e0\u6570\u6b21\uff0c\u90fd\u65e0\u6cd5\u627e\u5230\u6211\u4ee3\u7801\u95ee\u9898\uff0c\u6700\u540e\u5173\u95eddebug\u6a21\u5f0f\u540e\uff0c\u5c31\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\u3002\r\n\r\n\u590d\u73b0\u529e\u6cd5\uff1a\r\napp.py:\r\napp.run(debug=True)\r\n\r\nmodel.py\uff1a\r\npass\r\n\r\nuserdb.py:\r\n        with self.app.app_context():\r\n            new_user = userdbmodel(userdb)(user='11eee',\r\n                                           name=input(\"input\")\r\n                                           )\r\n            userdb.session.add(new_user)\r\n            userdb.session.commit()\r\n\r\n\u4f1a\u51fa\u73b0\u4e24\u6b21input\u8f93\u5165\uff0c\u4e4b\u540e\u4f1a\u63d0\u793a\u91cd\u590d\u540e\u7a0b\u5e8f\u505c\u6b62\u3002\r\n\r\n\r\n\r\n\r\n\r\n\u76ee\u524d\u53ea\u8981\u628aapp.run(debug=True)\u6539\u4e3aapp.run(debug=False)\uff0c\u4f8b\u5982\uff1a\r\napp.py:\r\napp.run(debug=False)\r\n\u5c31\u53ea\u4f1a\u6267\u884c\u4e00\u6b21\uff0c\u4e0d\u4f1a\u62a5\u9519\u3002"
  },
  {
    "name": "Mypy error when accessing flask.request.json",
    "description": "I use mypy to validate my project, I upgraded from flask 2 to flask 3 and now I see plenty of errors when I try to access the json property or flask.request\r\n\r\nI just have a top level\r\n\r\nimport flask\r\n\r\nThen:\r\n\r\nj = flask.request.json  # \"Request\" has no attribute \"json\"\r\n\r\nMypy: \"Request\" has no attribute \"json\" [attr-defined]\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.13.0\r\n- Flask version: 3.1.0"
  },
  {
    "name": "Flash message with Markup fails",
    "description": "Upgrading my platform from Python 3.8 + Flask 3.0.3 +  Flask-Session 0.8.0 with redis backend, to Python 3.11 + Flask 3.1.0 +  Flask-Session 0.8.0 with redis backend. Same user code.\r\n\r\nIssue: fancy flash message breaks on the new platform (work fine on the old platform).\r\n\r\nFlash message:\r\n`flash(Markup('press the play button <i class=\"bi-play-btn-fill black\"></i> below'), 'info')`\r\n\r\nError:\r\n```\r\n[2024-12-10 19:01:28,998] ERROR in base: Failed to serialize session data: Encoding objects of type Markup is unsupported\r\n[2024-12-10 19:01:28,998] ERROR in app: Exception on / [POST]\r\nTraceback (most recent call last):\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 1511, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 920, in full_dispatch_request\r\n    return self.finalize_request(rv)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 941, in finalize_request\r\n    response = self.process_response(response)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 1322, in process_response\r\n    self.session_interface.save_session(self, ctx.session, response)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 305, in save_session\r\n    self._upsert_session(app.permanent_session_lifetime, session, store_id)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/redis/redis.py\", line 78, in _upsert_session\r\n    serialized_session_data = self.serializer.encode(session)\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 132, in encode\r\n    return self.encoder.encode(dict(session))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: Encoding objects of type Markup is unsupported\r\n[2024-12-10 19:01:29,002] ERROR in base: Failed to serialize session data: Encoding objects of type Markup is unsupported\r\n[2024-12-10 19:01:29,002] ERROR in app: Request finalizing failed with an error while handling an error\r\nTraceback (most recent call last):\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 1511, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 920, in full_dispatch_request\r\n    return self.finalize_request(rv)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 941, in finalize_request\r\n    response = self.process_response(response)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 1322, in process_response\r\n    self.session_interface.save_session(self, ctx.session, response)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 305, in save_session\r\n    self._upsert_session(app.permanent_session_lifetime, session, store_id)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/redis/redis.py\", line 78, in _upsert_session\r\n    serialized_session_data = self.serializer.encode(session)\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 132, in encode\r\n    return self.encoder.encode(dict(session))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: Encoding objects of type Markup is unsupported\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 941, in finalize_request\r\n    response = self.process_response(response)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask/app.py\", line 1322, in process_response\r\n    self.session_interface.save_session(self, ctx.session, response)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 305, in save_session\r\n    self._upsert_session(app.permanent_session_lifetime, session, store_id)\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/redis/redis.py\", line 78, in _upsert_session\r\n    serialized_session_data = self.serializer.encode(session)\r\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/var/www/lib/python3.11/site-packages/flask_session/base.py\", line 132, in encode\r\n    return self.encoder.encode(dict(session))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: Encoding objects of type Markup is unsupported\r\n```"
  },
  {
    "name": "flask==2.2.4 incompatible with werkzeug==3.1.3",
    "description": "I use flask==2.2.4 for my project and when I install flask, it pulls **any** werkzeug>=2.2.2 (which is the default behaviour). After [werkzeug==3.1.3](https://pypi.org/project/Werkzeug/3.1.3/) got released on 8 Nov, 2024, flask pulls the latest version of it. With this new version of werkzeug, while executing unit tests, I get an error saying `module 'werkzeug' has no attribute '__version__'`\r\n\r\n**A small example to reproduce the issue:**\r\n(myenv) root@6426d8424cca:~# python3\r\nPython 3.9.16 (main, Sep  2 2024, 12:46:28) \r\n[GCC 10.2.1 20210110] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n **\">>> from flask import Flask\r\n \">>> app = Flask(__name__)\r\n \">>> my_test_client = app.test_client()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/myenv/lib/python3.9/site-packages/flask/app.py\", line 1252, in test_client\r\n    return cls(  # type: ignore\r\n  File \"/root/myenv/lib/python3.9/site-packages/flask/testing.py\", line 116, in __init__\r\n    \"HTTP_USER_AGENT\": f\"werkzeug/{werkzeug.__version__}\",\r\n**AttributeError: module 'werkzeug' has no attribute '__version__'****\r\n\r\n\r\nI shouldn't be seeing any error like above.\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.9.16\r\n- Flask version: 2.2.4"
  },
  {
    "name": "Remove unnecessary whitespace in slice notation to align with PEP 8 and Black standards",
    "description": "File: flask/src/flask/cli.py\r\nLine: 701\r\n\r\nThe return statement in the _path_is_ancestor function can be updated to improve readability and align with PEP 8 and Black formatting standards for slice notation. Currently, there is unnecessary whitespace around the colon (:) in the slice, which is inconsistent with these guidelines.\r\n\r\nCurrent Code:\r\n```return os.path.join(path, other[len(path) :].lstrip(os.sep)) == other```\r\n\r\nSuggested Change:\r\nRemove the unnecessary whitespace around the colon (:) in the slice notation to improve readability:\r\n```return os.path.join(path, other[len(path):].lstrip(os.sep)) == other```\r\n\r\nThis change ensures compliance with PEP 8 and Black standards, contributing to cleaner and more consistent formatting across the codebase."
  },
  {
    "name": "Type hint for FlaskCliRunner.invoke is not specific",
    "description": "The type hint of the return type of `FlaskCliRunner.invoke` ist `typing.Any` but should use `click.testing.Result`. This leads to missing auto completions in the IDE as well as errors from the type checker if not additional type narrowing is done.\r\n\r\nThe Problem is here: https://github.com/pallets/flask/blob/main/src/flask/testing.py#L276\r\n\r\nTo fix this `Result` must be imported from `click.testing` and used as a return type annotation.\r\n\r\nThe documentation already specifies `click.testing.Result` as return type.\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.12.2\r\n- Flask version: 3.1.0 but is also present in current main branch."
  },
  {
    "name": "PROVIDE_AUTOMATIC_OPTIONS causes KeyError if not set",
    "description": "https://github.com/pallets/flask/blob/bc098406af9537aacc436cb2ea777fbc9ff4c5aa/src/flask/sansio/app.py#L641C12-L641C86\r\n\r\nSimply changing this to : `self.config.get(\"PROVIDE_AUTOMATIC_OPTIONS\", False)` should resolve the problem.\r\n\r\nThis change now released is causing upstream trouble in other packages such as Quart:\r\nhttps://github.com/pallets/quart/issues/371"
  },
  {
    "name": "merge app and request contexts into a single context",
    "description": "Right now we have two separate contexts managed separately, the app and request contexts. This makes the implementation pretty complicated, as we need to maintain two context var stacks, and do a bunch of checks in the request context to make sure we're managing the correct app context. It makes an already confusing topic more complicated to explain: app context is active for requests and cli commands, don't push an app context before making a request, etc.\r\n\r\nI think merging the two contexts could be possible. The single context (`ExecutionContext`?) would have the `g`, `request`, and `session` attributes, but accessing `request` or `session` when not in a request would raise an error."
  },
  {
    "name": "configure and check `request.trusted_hosts`",
    "description": "If `request.trusted_hosts` is set, it will be checked when accessing `request.host`/`url`/etc. Add `TRUSTED_HOSTS` config, and check it during routing."
  },
  {
    "name": "dotenv file arg should take precedence over default dotenv files",
    "description": "As pointed out in #5532, `-e file` is loaded after the default `.env` and `.flaskenv` files, which means it won't override env vars that are set by the defaults. It will be a little tricky to track during CLI startup, but I think it should be possible to fix this so that precedence is: `os.environ` > `-e path` > `.env` > `.flaskenv`."
  },
  {
    "name": "extend config for Request form memory settings",
    "description": "Werkzeug provides three settings on `Request` to control form parsing: `max_content_length`, `max_form_memory_size`, and `max_form_parts`. Only `max_content_length` is exposed as a Flask config right now, and the way it's implemented means it can only be set for the whole application, not per-request. Fix all that. Document these in the security page as well as the config page."
  },
  {
    "name": "Rotating secret keys",
    "description": "I'd like to have a couple of secret keys available, like so\r\n```\r\napp.secret_keys = [\r\n   'key-1',\r\n   'key-2',\r\n]\r\n```\r\nTo allow them to rotate.\r\n\r\nI've seen this issue which seems to have auto-closed\r\nhttps://github.com/pallets/flask/issues/1574\r\n\r\nIt seems like itsdangerous now supports this\r\nhttps://github.com/pallets/itsdangerous/pull/141\r\n\r\nThis would allow secret keys to be rotated regularly without sessions being invalidated."
  },
  {
    "name": "Error: No such command 'db'.",
    "description": "[OJ.zip](https://github.com/user-attachments/files/17025798/OJ.zip)\r\n```\r\n(onlinejudge_env) kimi@KimideMacBook-Air OJ % flask db init\r\n\r\nError: While importing 'run', an ImportError was raised:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/kimi/OJ/onlinejudge_env/lib/python3.9/site-packages/flask/cli.py\", line 218, in locate_app\r\n    __import__(module_name)\r\n  File \"/Users/kimi/OJ/run.py\", line 3, in <module>\r\n    app = create_app()\r\n  File \"/Users/kimi/OJ/app/__init__.py\", line 25, in create_app\r\n    from app.auth import auth as auth_blueprint\r\n  File \"/Users/kimi/OJ/app/auth.py\", line 6, in <module>\r\n    from werkzeug.urls import url_parse\r\nImportError: cannot import name 'url_parse' from 'werkzeug.urls' (/Users/kimi/OJ/onlinejudge_env/lib/python3.9/site-packages/werkzeug/urls.py)\r\n\r\n\r\nUsage: flask [OPTIONS] COMMAND [ARGS]...\r\nTry 'flask --help' for help.\r\n\r\nError: No such command 'db'.\r\n(onlinejudge_env) kimi@KimideMacBook-Air OJ % \r\n```"
  },
  {
    "name": "Allow choice of reloader type",
    "description": "I know it was closed in #5007, but would it be possible to reconsider this? I'm having a lot of trouble with `watchdog` despite following all the fixes proposed  such as #2603. The reloader constantly detects spurious file changes, whereas I've never experienced such issues with `stat`. If you really dislike the idea of a new flag, maybe an environment variable would be an option?\r\n\r\nThanks a lot for the work on this awesome package!"
  },
  {
    "name": "`Config` `type: ignore` causing downstream error reports",
    "description": "The `# type: ignore` in the code here https://github.com/pallets/flask/blob/8a6cdf1e2a5efa81c30f6166602064ceefb0a35b/src/flask/config.py#L50\r\nis causing type errors to be reported in user code with pyright strict settings.\r\n\r\n```python\r\nfrom flask import Flask\r\n\r\n\r\nFlask(\"x\").config.update({\"A\": \"3\"})  # 'Type of \"update\" is partially unknown - reportUnknownMemberType'\r\n```\r\n\r\nBased on other typing in the same class, it looks like it could be:\r\n```python\r\nclass Config(dict[str, t.Any]):\r\n```\r\n\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.12\r\n- Flask version: 3.0.3"
  },
  {
    "name": "Test failures in tutorial with python3.12",
    "description": "The tests runned with `pytest` works with python3.11 and not in 3.12:\r\n\r\nHow to reproduce:\r\n```\r\ngit clone git@github.com:pallets/flask.git\r\ncd flask/examples/tutorial\r\npython3.11 -m venv venv3.11\r\npython3.12 -m venv venv3.12\r\n# install dependencies\r\n# setup db\r\n```\r\nIn both cases, the webservice runs properly with `./venv3.1x/bin/flask --app flaskr run --debug`.\r\n\r\nHowever, running the tests show different results. In `venv3.11`, the 24 tests are green. In `venv3.12` virtualenv, there is 6 failures (and 18 green). The failures are due to a `DeprecationWarning` which becomes an error:\r\n```\r\nDeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n```\r\n\r\nThe full output is provided at the end of the bug report.\r\n\r\nThe `DeprecationWarning` is documented in the [3.12 release](https://docs.python.org/3/whatsnew/3.12.html): '[default adapters and converters](https://docs.python.org/3/library/sqlite3.html#sqlite3-default-converters) are now deprecated. Instead, use the [Adapter and converter recipes](https://docs.python.org/3/library/sqlite3.html#sqlite3-adapter-converter-recipes) and tailor them to your needs.' Copy-pasting blindly the recipes in `tests/conftest.py`, `flaskr/__init__.py` and `flaskr/db.py` does not fix the errors.\r\n\r\nThe errors can be fixed by adding thoses lines in conftest.py:\r\n```python\r\ndef convert_timestamp(val):\r\n    \"\"\"Convert Unix epoch timestamp to datetime.datetime object.\"\"\"\r\n    return datetime.datetime.strptime(val.decode(\"utf-8\"), \"%Y-%m-%d %H:%M:%S\").replace(tzinfo=datetime.timezone.utc)\r\n\r\nsqlite3.register_converter(\"timestamp\", convert_timestamp)\r\n```\r\n\r\nIt's probably not the best fix but it's a start.\r\n\r\n\r\nPytest output:\r\n\r\n```\r\n(venv3.12) $ ./venv3.12/bin/pytest\r\n========================================= test session starts ==========================================\r\nplatform linux -- Python 3.12.4, pytest-8.3.2, pluggy-1.5.0\r\nrootdir: /home/stephane/src/flasktuto/flask/examples/tutorial\r\nconfigfile: pyproject.toml\r\ntestpaths: tests\r\ncollected 24 items                                                                                     \r\n\r\ntests/test_auth.py ....F...                                                                      [ 33%]\r\ntests/test_blog.py F...F...F.FF                                                                  [ 83%]\r\ntests/test_db.py ..                                                                              [ 91%]\r\ntests/test_factory.py ..                                                                         [100%]\r\n\r\n=============================================== FAILURES ===============================================\r\n______________________________________________ test_login ______________________________________________\r\n\r\nclient = <FlaskClient <Flask 'flaskr'>>, auth = <conftest.AuthActions object at 0x7f297abc6900>\r\n\r\n    def test_login(client, auth):\r\n        # test that viewing the page renders without template errors\r\n        assert client.get(\"/auth/login\").status_code == 200\r\n    \r\n        # test that successful login redirects to the index page\r\n        response = auth.login()\r\n        assert response.headers[\"Location\"] == \"/\"\r\n    \r\n        # login request set the user_id in the session\r\n        # check that the user is loaded from the session\r\n        with client:\r\n>           client.get(\"/\")\r\n\r\ntests/test_auth.py:50: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1162: in get\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/blog.py:24: in index\r\n    ).fetchall()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n______________________________________________ test_index ______________________________________________\r\n\r\nclient = <FlaskClient <Flask 'flaskr'>>, auth = <conftest.AuthActions object at 0x7f297ad26390>\r\n\r\n    def test_index(client, auth):\r\n>       response = client.get(\"/\")\r\n\r\ntests/test_blog.py:7: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1162: in get\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/blog.py:24: in index\r\n    ).fetchall()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n_________________________________________ test_author_required _________________________________________\r\n\r\napp = <Flask 'flaskr'>, client = <FlaskClient <Flask 'flaskr'>>\r\nauth = <conftest.AuthActions object at 0x7f297ad3e210>\r\n\r\n    def test_author_required(app, client, auth):\r\n        # change the post author to another user\r\n        with app.app_context():\r\n            db = get_db()\r\n            db.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\r\n            db.commit()\r\n    \r\n        auth.login()\r\n        # current user can't modify other user's post\r\n>       assert client.post(\"/1/update\").status_code == 403\r\n\r\ntests/test_blog.py:34: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1167: in post\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/auth.py:27: in wrapped_view\r\n    return view(**kwargs)\r\nflaskr/blog.py:90: in update\r\n    post = get_post(id)\r\nflaskr/blog.py:48: in get_post\r\n    .fetchone()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n_____________________________________________ test_update ______________________________________________\r\n\r\nclient = <FlaskClient <Flask 'flaskr'>>, auth = <conftest.AuthActions object at 0x7f297abc7920>\r\napp = <Flask 'flaskr'>\r\n\r\n    def test_update(client, auth, app):\r\n        auth.login()\r\n>       assert client.get(\"/1/update\").status_code == 200\r\n\r\ntests/test_blog.py:59: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1162: in get\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/auth.py:27: in wrapped_view\r\n    return view(**kwargs)\r\nflaskr/blog.py:90: in update\r\n    post = get_post(id)\r\nflaskr/blog.py:48: in get_post\r\n    .fetchone()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n________________________________ test_create_update_validate[/1/update] ________________________________\r\n\r\nclient = <FlaskClient <Flask 'flaskr'>>, auth = <conftest.AuthActions object at 0x7f297ace5160>\r\npath = '/1/update'\r\n\r\n    @pytest.mark.parametrize(\"path\", (\"/create\", \"/1/update\"))\r\n    def test_create_update_validate(client, auth, path):\r\n        auth.login()\r\n>       response = client.post(path, data={\"title\": \"\", \"body\": \"\"})\r\n\r\ntests/test_blog.py:71: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1167: in post\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/auth.py:27: in wrapped_view\r\n    return view(**kwargs)\r\nflaskr/blog.py:90: in update\r\n    post = get_post(id)\r\nflaskr/blog.py:48: in get_post\r\n    .fetchone()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n_____________________________________________ test_delete ______________________________________________\r\n\r\nclient = <FlaskClient <Flask 'flaskr'>>, auth = <conftest.AuthActions object at 0x7f297ada5610>\r\napp = <Flask 'flaskr'>\r\n\r\n    def test_delete(client, auth, app):\r\n        auth.login()\r\n>       response = client.post(\"/1/delete\")\r\n\r\ntests/test_blog.py:77: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1167: in post\r\n    return self.open(*args, **kw)\r\nvenv3.12/lib/python3.12/site-packages/flask/testing.py:235: in open\r\n    response = super().open(\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1116: in open\r\n    response_parts = self.run_wsgi_app(request.environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:988: in run_wsgi_app\r\n    rv = run_wsgi_app(self.application, environ, buffered=buffered)\r\nvenv3.12/lib/python3.12/site-packages/werkzeug/test.py:1264: in run_wsgi_app\r\n    app_rv = app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1498: in __call__\r\n    return self.wsgi_app(environ, start_response)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1476: in wsgi_app\r\n    response = self.handle_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:1473: in wsgi_app\r\n    response = self.full_dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:882: in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:880: in full_dispatch_request\r\n    rv = self.dispatch_request()\r\nvenv3.12/lib/python3.12/site-packages/flask/app.py:865: in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\r\nflaskr/auth.py:27: in wrapped_view\r\n    return view(**kwargs)\r\nflaskr/blog.py:121: in delete\r\n    get_post(id)\r\nflaskr/blog.py:48: in get_post\r\n    .fetchone()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nval = b'2018-01-01 00:00:00'\r\n\r\n    def convert_timestamp(val):\r\n>       warn(msg.format(what=\"timestamp converter\"), DeprecationWarning, stacklevel=2)\r\nE       DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\r\n\r\n/usr/lib/python3.12/sqlite3/dbapi2.py:76: DeprecationWarning\r\n======================================= short test summary info ========================================\r\nFAILED tests/test_auth.py::test_login - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\nFAILED tests/test_blog.py::test_index - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\nFAILED tests/test_blog.py::test_author_required - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\nFAILED tests/test_blog.py::test_update - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\nFAILED tests/test_blog.py::test_create_update_validate[/1/update] - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\nFAILED tests/test_blog.py::test_delete - DeprecationWarning: The default timestamp converter is deprecated as of Python 3.12; see the sqlite...\r\n===================================== 6 failed, 18 passed in 8.05s =====================================\r\n\r\n``"
  },
  {
    "name": "test_helpers.py.test_open_resource_with_encoding function attribute error",
    "description": "the function test_open_resource_with_encoding, sends encoding arguemnt to app.open_resource() function. the open_resource function doesnt has any enoding parameter in it's signature.\r\n\r\n- Python version: 3.11.5\r\n\r\n- Flask version: current"
  },
  {
    "name": "Revisit SERVER_NAME's impact on routing and external URL generation",
    "description": "#998 was a discussion regarding the impact the `SERVER_NAME` setting has on an application.  IMO, there was consensus in that issue, including by @mitsuhiko, that `SERVER_NAME` being used both in routing and in URL generation was problematic.  \r\n\r\nThat issue was closed by https://github.com/pallets/flask/pull/2635, which made a change regarding subdomain matching.  However, IMO, it did not address the fundamental problem of `SERVER_NAME` impacting two disparate mechanisms in Flask.\r\n\r\nI found two additional issues since #998 that have attempted to point out the problem with `SERVER_NAME`: #2813, #3465\r\n\r\nIn #2813, a minimal, complete, and verifiable example was requested and I have therefore prepared an [example gist that demonstrates the problem](https://gist.github.com/rsyring/c50a500a5d35787ef45c1ff1e78d8898).\r\n\r\nThe gist's third \"currently impossible\" test demonstrates that it's currently impossible to:\r\n\r\n* Set `SERVER_NAME` so external URL generation works outside a request context with only an app context; and\r\n* Have the app respond properly to multiple host names. \r\n\r\nMy particular use case currently is that I need `SERVER_NAME` set so external URLs will be created accurately in distributed tasks where no web request is present.  Additionally, I need the app to respond to multiple host names 1) the canonical URL the public uses; and 2) the IP address based URL our load balancer uses for health checks.\r\n\r\nIMO, `SERVER_NAME` should be deprecated and two new settings should be created.  One which affects external URL generation and a second that affects routing resolution.\r\n\r\nThe setting for the URL generation could be `CANONICAL_URL` and, because its a URL, it could eliminate the need for `url_for()`'s `_scheme` arg.\r\n\r\nThanks for your consideration.\r\n\r\n- Python version: 3.12\r\n- Flask version: 3.0.3"
  },
  {
    "name": "Pyright Type Errors",
    "description": "We have a number of various Pyright typing errors.\r\n\r\nEnvironment:\r\n\r\n- Python version: `3.12`\r\n- Flask version: `3.1.0`"
  },
  {
    "name": "Pyright type errors: `src/flask/helpers.py`",
    "description": "Pyright reports type errors for `src/flask/helpers.py`:\r\n\r\n```\r\nflask/src/flask/helpers.py\r\n  flask/src/flask/helpers.py:590:27 - error: Cannot access attribute \"get_filename\" for class \"Loader\"\r\n  \u00a0\u00a0Attribute \"get_filename\" is unknown (reportAttributeAccessIssue)\r\n```\r\n\r\nCommand which was run:\r\n\r\n```shell\r\n.venv/bin/pyright --pythonpath .venv/bin/python3 --project pyproject.toml\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: `3.12`\r\n- Flask version: `3.1.0`"
  },
  {
    "name": "Pyright type errors: `src/flask/blueprint.py`",
    "description": "Pyright reports type errors for `src/flask/blueprint.py`:\r\n\r\n```\r\nflask/src/flask/blueprints.py\r\n  flask/src/flask/blueprints.py:126:20 - error: Expression of type \"BufferedReader\" is incompatible with return type \"IO[AnyStr@open_resource]\"\r\n  \u00a0\u00a0\"BufferedReader\" is incompatible with \"IO[AnyStr@open_resource]\"\r\n  \u00a0\u00a0\u00a0\u00a0Type parameter \"AnyStr@IO\" is invariant, but \"bytes\" is not the same as \"AnyStr@open_resource\" (reportReturnType)\r\n```\r\n\r\nCommand which was run:\r\n\r\n```shell\r\n.venv/bin/pyright --pythonpath .venv/bin/python3 --project pyproject.toml\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: `3.12`\r\n- Flask version: `3.1.0`"
  },
  {
    "name": "Pyright type errors: `src/flask/app.py`",
    "description": "Pyright reports type errors for `src/flask/app.py`:\r\n\r\n```\r\nflask/src/flask/app.py\r\n  flask/src/flask/app.py:352:20 - error: Expression of type \"BufferedReader\" is incompatible with return type \"IO[AnyStr@open_resource]\"\r\n  \u00a0\u00a0\"BufferedReader\" is incompatible with \"IO[AnyStr@open_resource]\"\r\n  \u00a0\u00a0\u00a0\u00a0Type parameter \"AnyStr@IO\" is invariant, but \"bytes\" is not the same as \"AnyStr@open_resource\" (reportReturnType)\r\n  flask/src/flask/app.py:1178:21 - error: Expression with type \"Tuple[ResponseValue, HeadersValue] | Tuple[ResponseValue, int] | Tuple[ResponseValue, int, HeadersValue]\" cannot be assigned to target tuple\r\n  \u00a0\u00a0Type \"Tuple[ResponseValue, int, HeadersValue]\" is incompatible with target tuple\r\n  \u00a0\u00a0\u00a0\u00a0Tuple size mismatch; expected 2 but received 3 (reportAssignmentType)\r\n  flask/src/flask/app.py:1205:28 - error: Argument of type \"HeadersValue | int | None\" cannot be assigned to parameter \"status\" of type \"int | str | HTTPStatus | None\" in function \"__init__\" (reportArgumentType)\r\n  flask/src/flask/app.py:1240:29 - error: Cannot assign to attribute \"status\" for class \"Response\" (reportAttributeAccessIssue)\r\n  flask/src/flask/app.py:1242:34 - error: Cannot assign to attribute \"status_code\" for class \"Response\"\r\n  \u00a0\u00a0Type \"HeadersValue | int\" is incompatible with type \"int\"\r\n  \u00a0\u00a0\u00a0\u00a0\"Mapping[str, HeaderValue]\" is incompatible with \"int\" (reportAttributeAccessIssue)\r\n```\r\n\r\nCommand which was run:\r\n\r\n```shell\r\nsource .venv/bin/activate\r\npyright\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: `3.12`\r\n- Flask version: `3.1.0`"
  },
  {
    "name": "`pyright` type checking seems to be unused. `mypy` still being used.",
    "description": "We added `pyright` in 3.0.3 (see: https://github.com/pallets/flask/pull/5457), but `mypy` is still being used for type checking https://github.com/pallets/flask/blob/main/tox.ini#L29-L32, and `pyright` is not.\r\n\r\nIf this was an intentional change of type checkers, then would expect that `mypy` be completely removed, and `pyright` be the command run in `tox typing`.\r\n\r\nIt should be noted, however, that running `pyright` yields 90 errors (71 tests/19 src). Is there an upgrade path or was this an oversight?\r\n\r\nEnvironment:\r\n\r\n- Python version:\r\n- 3.12\r\n- Flask version:\r\n- 3.0.3"
  },
  {
    "name": "Encountering Jinja2 SyntaxError after adding another css file",
    "description": "I've been encountering this confusing error; which is Jinja2.exceptions.TemplateSyntaxError: expected token ',', got 'string'.\r\nThis only happened after I added another css file to my html script. But previously, it worked fine. \r\n\r\nHere are the previous errors i've encountered regarding this issue:\r\n\r\n```\r\n<link rel=\"icon\" href=\"{{ url_for('static', filename='images/favicon.png') }}\">\r\njinja2.exceptions.TemplateSyntaxError: expected token ',', got 'static'\r\n    <li onclick=\"redirectUser('/homepage')\">Home</li>\r\njinja2.exceptions.TemplateSyntaxError: expected token ',', got 'string'\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.12.4\r\n- Flask version: 3.0.3"
  },
  {
    "name": "\"catch-all\" endpoint doesn't work with `static_url_path=\"/\"`",
    "description": "### Catch all endpoint for SPA from [docs](https://flask.palletsprojects.com/en/2.2.x/patterns/singlepageapplications/) doesn't work with `static_url_path=\"/\"` or `static_url_path=\"\"`\r\n\r\n### Context\r\n\r\nI'm using Flask to serve React SPA with client-side routing. \r\nFlask worked perfectly, until I added images and decided to serve it from `static/` folder.\r\n\r\n### Expected:\r\n\r\nI can serve images from `static/` folder. \r\nI can reload a non-root page and Flask won't respond with `404 Not Found`.\r\n\r\n### Current:\r\n\r\nI can navigate to `/path` or `/path1/path2` and etc. with no problems, but when I reload any non-root page the Flask gives me `404 Not Found` and the catch-all route didn't trigger.\r\n\r\n\r\n### Example:\r\n\r\n**`folder structure`**\r\n```\r\n.\r\n\u251c\u2500\u2500 __init__.py\r\n\u2514\u2500\u2500 main/\r\n    \u251c\u2500\u2500 static/\r\n    \u2502   \u251c\u2500\u2500 image1.jpg\r\n    \u2502   \u251c\u2500\u2500 image2.jpg\r\n    \u2502   \u251c\u2500\u2500 index.html\r\n    \u2502   \u2514\u2500\u2500 main.js\r\n    \u251c\u2500\u2500 __init__.py\r\n    \u2514\u2500\u2500 routes.py\r\n```\r\n\r\n**`./__init__.py`**\r\n```python\r\ndef create_app(config_class=Config) -> Flask:\r\n  app = Flask(__name__)\r\n  from .main import bp as main_bp\r\n  app.register_blueprint(main_bp)\r\n  return app\r\n```\r\n\r\n**`./main/__init__.py`**\r\n```python\r\nfrom flask import Blueprint\r\n\r\nbp = Blueprint(\"main\", __name__, static_folder=\"static\", static_url_path=\"/\")\r\n\r\nfrom .main import routes\r\n``` \r\n\r\n**`./main/routes.py`**\r\n```python\r\nfrom .main import bp\r\n\r\n@bp.route(\"/\", defaults={\"path\": \"\"})\r\n@bp.route(\"/<path:path>\")\r\ndef catch_all(path):\r\n    return bp.send_static_file(\"index.html\")\r\n```\r\n\r\n### Error message\r\n\r\n```\r\n<!doctype html>\r\n<html lang=en>\r\n<title>404 Not Found</title>\r\n<h1>Not Found</h1>\r\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: `Python 3.9.2`\r\n- Flask version: `Flask 2.2.5`"
  },
  {
    "name": "Update Werkzeug to 3.0.3",
    "description": "Update Werkzeug to 3.0.3 to address vulnerability found in Werkzeug debugger.\r\nhttps://werkzeug.palletsprojects.com/en/3.0.x/changes/\r\n\r\nUpdating Werkzeug to version 3.0.3 fixes the debugger vulnerability."
  },
  {
    "name": "false redirect from /name to /name/ causes 404",
    "description": "<!--\r\nroute name redirects to /name/ when the canonical name is /name\r\nthis causes the page to be 404 Not Found, since when using /name as a canonical name, / isn't avaliable\r\n-->\r\n\r\n<!--\r\nhow to replicate:\r\ndownload flask on ubuntu 24.04 using the command:\r\nsudo apt install python3-flask\r\n\r\nrun the code from the terminal:\r\nflask --app my_app run --debug\r\nin this order:\r\n1). first using @app.route(\"/hello\")\r\n2). then using @app.route(\"/hello/\")\r\n3). then using @app.route(\"/hello\")\r\nthe page should now autocomplete /hello to /hello/\r\n\r\nexample end code picture: \r\n![Screenshot from 2024-07-08 22-01-58](https://github.com/pallets/flask/assets/87893493/1a91c062-4739-497d-8446-32089db91017)\r\n-->\r\n\r\n<!--\r\nafter changing back the code to @app.route(\"/name\") the code should have not autocompleted the url to /name/\r\n-->\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.12.3\r\n- Flask version: 3.0.2"
  },
  {
    "name": "Flask incorrectly parses chained query parameters",
    "description": "When accessing the /test route with both category and location query parameters (http://localhost:5000/test?category=fish&location=water), Flask correctly parses category but ignores location.\r\n\r\n### Steps to Reproduce\r\nAccess the URL http://localhost:5000/test?category=fish&location=water.\r\nExpected behavior: Both category and location parameters should be correctly parsed by Flask.\r\nActual behavior: Flask parses category correctly (\"excavator\"), but location is ignored.\r\n\r\n```(python)\r\nfrom flask import Flask, request, jsonify\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route(\"/test\")\r\ndef test():\r\n    category = request.args.get(\"category\")\r\n    location = request.args.get(\"location\")\r\n    return {\"category\": category, \"location\": location}\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(debug=True)\r\n```\r\n\r\nExpected:\r\n```(json)\r\n{\r\n    \"category\": \"fish\",\r\n    \"location\": \"water\"\r\n}\r\n```\r\n\r\nGot:\r\n```(json)\r\n{\r\n    \"category\": \"fish\",\r\n    \"location\": null\r\n}\r\n```\r\n### Environment:\r\n\r\n- Python version: 3.8.10\r\n- Flask version: 3.0.3"
  },
  {
    "name": "Handler `before_request` doesn't work as expected.",
    "description": "I want to implement CORS processing without any third party libraries (like flask-cors). My desire is simple: assign multiple headers to any response, access and process the pre-flight OPTIONS request.\r\n\r\n```python\r\nclass CORSMiddleware:\r\n    def before_request(self):\r\n        if request.method == \"OPTIONS\":\r\n            response = jsonify({\"status\": \"ok\"})\r\n            return self.process_response(response)\r\n\r\n    def after_request(self, response):\r\n        return self.process_response(response)\r\n\r\n    # noinspection PyMethodMayBeStatic\r\n    def process_response(self, response):\r\n        if response:\r\n            response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\r\n            response.headers.add(\"Access-Control-Allow-Headers\", \"Content-Type, Authorization\")\r\n            response.headers.add(\"Access-Control-Allow-Methods\", \"GET, POST, OPTIONS, DELETE, PUT\")\r\n        return response\r\n\r\n    def register(self, app: Flask):\r\n        app.before_request(self.before_request)\r\n        app.after_request(self.after_request)\r\n```\r\n\r\nIt can be tested like this\r\n\r\n```py\r\n\r\n# Note: helper function to reduce view logic\r\ndef add_cors_headers(response):\r\n    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\r\n    response.headers.add(\"Access-Control-Allow-Headers\", \"Content-Type,Authorization\")\r\n    response.headers.add(\"Access-Control-Allow-Methods\", \"GET,POST,OPTIONS,DELETE,PUT\")\r\n    return response\r\n\r\n\r\nuser_blueprint = Blueprint(\"user\", __name__, url_prefix=\"/users\")\r\n\r\n\r\n@user_blueprint.route(\"/signup\", methods=[\"POST\", \"OPTIONS\"])\r\ndef create_user_handler():\r\n    # Note: uncomment me to see a desired behaviour\r\n    # if request.method == \"OPTIONS\":\r\n    #     response = jsonify({\"status\": \"ok\"})\r\n    #     return add_cors_headers(response)\r\n    # response = jsonify({\"message\": \"registered\"})\r\n    # return add_cors_headers(response)\r\n    return {\"message\": \"registered\"}\r\n\r\n\r\ndef create_app() -> Flask:\r\n    app = Flask(__name__)\r\n    app.register_blueprint(user_blueprint)\r\n    # Note: comment me to see a desired behaviour\r\n    CORSMiddleware().register(app)\r\n    return app\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app = create_app()\r\n    app.run(host=\"127.0.0.1\", port=5000, debug=True\r\n```\r\n\r\nWith the middleware, it handles a preflight OPTIONS request as an original one:\r\n<img width=\"650\" alt=\"Screenshot 2024-07-01 at 10 59 27\" src=\"https://github.com/pallets/flask/assets/45794357/b49f736d-5b1b-46c7-8569-aa6dd178266c\">\r\n\r\nBut the desired behaviour should be like that (you can comment middleware registration and uncomment logic in the view-handler):\r\n<img width=\"653\" alt=\"Screenshot 2024-07-01 at 11 00 15\" src=\"https://github.com/pallets/flask/assets/45794357/9668ea85-de6a-46c2-8c8c-39b9b1ffd7b7\">\r\n\r\nAlso, I was able to implement desired functionality with the middleware, but with the Werkzeug one:\r\n\r\n```py\r\nfrom werkzeug.wrappers import Request, Response\r\n\r\n\r\nclass CORSMiddleware:\r\n    def __init__(self, app):\r\n        self.app = app\r\n\r\n    def __call__(self, environ, start_response):\r\n        request_ = Request(environ)\r\n\r\n        if request_.method == \"OPTIONS\":\r\n            response = Response(status=200)\r\n            response = self.process_response(response)\r\n            return response(environ, start_response)\r\n\r\n        def custom_start_response(status, headers, exc_info=None):\r\n            response_headers = self.process_headers(headers)\r\n            return start_response(status, response_headers, exc_info)\r\n\r\n        return self.app(environ, custom_start_response)\r\n\r\n    def process_response(self, response):\r\n        response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\r\n        response.headers.add(\"Access-Control-Allow-Headers\",\r\n                             \"Content-Type, Authorization\")\r\n        response.headers.add(\"Access-Control-Allow-Methods\",\r\n                             \"GET, POST, OPTIONS, DELETE, PUT\")\r\n        return response\r\n\r\n    def process_headers(self, headers):\r\n        headers.append((\"Access-Control-Allow-Origin\", \"*\"))\r\n        headers.append((\"Access-Control-Allow-Headers\", \"Content-Type, Authorization\"))\r\n        headers.append(\r\n            (\"Access-Control-Allow-Methods\", \"GET, POST, OPTIONS, DELETE, PUT\"))\r\n        return headers\r\n\r\n\r\n# and registering it like that\r\ndef create_app() -> Flask:\r\n    app = Flask(__name__)\r\n    app.register_blueprint(user_blueprint)\r\n    app.wsgi_app = CORSMiddleware(app.wsgi_app)\r\n    return app\r\n```\r\n\r\nSo, my question is: what could be wrong with the Flask middleware?"
  },
  {
    "name": "Document behaviour of __init__ in cli.prepare_import()",
    "description": "The code in [cli](https://github.com/pallets/flask/blob/main/src/flask/cli.py#L215) is a bit confusing: if you have an __init__ file in the project it causes the cli to step out of the directory and import it from outside the directory.\r\n\r\nE.h. I was working in directory /Users/visgean/p/social-flask/chapter2/models_app and the flask run command tried to import the app as chapter2.models_app.app, which did not work:\r\n\r\n```\r\nError: While importing 'chapter2.models_app.app', an ImportError was raised:\r\n```\r\n\r\nI had to dig into the code to see that the solution is to delete the init file, apparently i am not the only one: https://stackoverflow.com/questions/70835630/while-importing-myapp-app-an-import-error-was-raised\r\n\r\n\r\nI think this feature is a bit undcomented and to be honest I am not sure what the goal is. If I run flask run in a directory I dont want it to work at all with the parent directory. E.g. flask should only be aware of the current directory.\r\n\r\nProposed fixes: \r\n- Specifically mention this behavior on start e.g. \"importing app as chapter2.models_app.app due to presence of __init__.py\"\r\n- Mention this behaviour in docs somewhere\r\n- Remove this behavior unless there is a clear need for it."
  },
  {
    "name": "get \u8bf7\u6c42\uff0c\u5b9a\u4e49\u53c2\u6570\u7c7b\u578b\u7684\u95ee\u9898",
    "description": "\u4e3a\u4ec0\u4e48\u5728 get \u8bf7\u6c42\u4e2d\uff0c\u8bbe\u7f6e\u67d0\u4e2a\u53c2\u6570\u7c7b\u578b\u4e3a int \u7c7b\u578b\u3002\r\n![image](https://github.com/pallets/flask/assets/38832100/6f2ad22b-63b1-4ea1-8567-975d146c061c)\r\n\r\n\r\n\u800c\u5728\u6d4f\u89c8\u5668\u8fd9\u6837\u8bf7\u6c42\u65f6\uff0c\u5374\u62a5 404  \u9519\u8bef\u3002\r\n![image](https://github.com/pallets/flask/assets/38832100/0c28c5c7-bb27-4669-b877-c90fb7b0e68a)\r\n\r\n\r\nFlaks \u7248\u672c\r\n![image](https://github.com/pallets/flask/assets/38832100/244e1622-2ea0-42ef-98f9-7710c55d83f9)"
  },
  {
    "name": "allow setting encoding in open_resource()",
    "description": "This is a duplicate of #1740 &mdash; that may have been closed for lack of a clear rationale, however, and I'd like to suggest it again with the following reasoning.\r\n\r\nThe documentation currently gives this example for using `open_resource()`:\r\n\r\n```python\r\nwith app.open_resource(\"schema.sql\") as f:\r\n    conn.executescript(f.read())\r\n```\r\n\r\nOn Windows, however, this can fail to open a file encoded in UTF-8, which most are these days, and safer code looks like this:\r\n\r\n```python\r\nwith app.open_resource(\"schema.sql\", mode=\"rb\") as f:\r\n   conn.executescript(f.read().decode(\"utf-8\"))  # type: ignore [attr-defined]\r\n```\r\n\r\n(The type comment is needed to prevent mypy from complaining about `f.read()` possibly being a string with no `.decode()` method, as it can't tell that the file was opened in 'rb' mode.)\r\n\r\nIt would be cleaner and more flexible to be able to write:\r\n\r\n```python\r\nwith app.open_resource(\"schema.sql\", encoding=\"utf-8\") as f:\r\n   conn.executescript(f.read())\r\n```"
  },
  {
    "name": "IPP",
    "description": "."
  },
  {
    "name": "request parsing special parameters",
    "description": "I want to know when i send \"http://127.0.0.1:5000/?name=xiaojiann+&age=16\" , why request.query_string can get b'name=xiaojiann+' but request.arg get ImmutableMultiDict([('name', 'xiaojiann ')]), request.args use blank space replace +\r\n\r\n<img width=\"739\" alt=\"image\" src=\"https://github.com/pallets/flask/assets/61368566/0a58880c-a70b-4455-a949-4af5ff55ca5b\">\r\n\r\nI try flask version 2.0.3 , 3.0.3 and python version 3.6 , 3.10"
  },
  {
    "name": "Avoid opening sessions for static resources",
    "description": "Every HTTP request currently results in an HTTP session being opened via `SessionInterface.open_session`. When a server-side session implementation is used, like [Flask-Session](https://flask-session.readthedocs.io/), this results in storage writes to update the expiry date. Requests for static resources are also subject to this, which can degrade performance.\r\n\r\nIt would be useful to easily opt static resources out of sessions. Currently this is only possible with a custom session interface, for example:\r\n\r\n```python\r\nclass StaticRequestFilteringSessionInterface(SessionInterface):\r\n    def __init__(self, app):\r\n        self._delegate = app.session_interface\r\n        self._exclude_path_prefix = app.static_url_path + \"/\"\r\n\r\n    def open_session(self, app, request):\r\n        if request.path.startswith(self._exclude_path_prefix):\r\n            return self.make_null_session(app)\r\n\r\n        return self._delegate.open_session(app, request)\r\n\r\n    def save_session(self, app, session, response):\r\n        return self._delegate.save_session(app, session, response)\r\n```\r\n\r\nConfigured with:\r\n\r\n```python\r\nfrom flask_session import Session\r\n\r\n...\r\nSession(app)\r\napp.session_interface = StaticRequestFilteringSessionInterface(app)\r\n```"
  },
  {
    "name": "Flask ignores multiple slashes at the beginning of the path",
    "description": "<!--\r\nThis issue tracker is a tool to address bugs in Flask itself. Please use\r\nGitHub Discussions or the Pallets Discord for questions about your own code.\r\n\r\nReplace this comment with a clear outline of what the bug is.\r\n-->\r\n\r\n<!--\r\nDescribe how to replicate the bug.\r\n\r\nInclude a minimal reproducible example that demonstrates the bug.\r\nInclude the full traceback if there was an exception.\r\n-->\r\n\r\n<!--\r\nDescribe the expected behavior that should have happened but didn't.\r\n-->\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.8.14\r\n- Flask version: 2.3.2\r\n\r\nI have a paths with prefix /admin. if you follow a path containing this prefix, proxy  before app see this and asks mTLS auth. But when i follow this path with multiple slashes at the beginning (ex //admin, ///admin) proxy ignore this, but flask think that this is the same path as /admin and return values, client must not see. I think this should not work like this"
  },
  {
    "name": "send_from_directory unclosed file error",
    "description": "When warnings are set to error, pytest claims there is an unclosed file when `send_from_directory` is called.  I assume this indicates there is a resource leak.  Even if Python garbage collects this at some point, this causes issues with testing flask applications that use send_from_directory with warnings as errors enabled.\r\n\r\n1. Create app\r\n\r\napp.py:\r\n```\r\nfrom flask import Flask, send_from_directory\r\napp = Flask(__name__)\r\n\r\n@app.route('/<path:name>')\r\ndef download_resources(name):\r\n    return send_from_directory('resources', name)\r\n```\r\n\r\n\r\n2. Add `resources/file.txt`\r\n\r\n```bash\r\nmkdir resources\r\necho file > resources/file.txt\r\n```\r\n\r\n\r\n3. Add tests\r\n\r\ntests/conftest.py:\r\n```\r\nimport pytest\r\n\r\nfrom app import app as flask_app\r\n\r\n@pytest.fixture\r\ndef app():\r\n    yield flask_app\r\n\r\n@pytest.fixture\r\ndef client(app):\r\n    return app.test_client()\r\n```\r\n\r\ntests/test_app.py:\r\n```\r\ndef test_download(app, client):\r\n    res = client.get('/file.txt')\r\n    assert res.status_code == 200\r\n```\r\n\r\n4. Run tests\r\n\r\nExecute `pytest -Werror`.\r\n\r\nThe results ([results.txt](https://github.com/pallets/flask/files/15325158/results.txt)) include the following error:\r\n` ResourceWarning: unclosed file <_io.BufferedReader name='/flask-test/resources/file.txt'>`\r\n\r\nExpected Results:\r\n\r\nExecute `pytest` without warnings as errors.  This is the expected result:\r\n\r\n```\r\n============================= test session starts ==============================\r\nplatform darwin -- Python 3.11.9, pytest-8.2.0, pluggy-1.5.0\r\nrootdir: /flask-test\r\ncollected 1 item\r\n\r\ntests/test_app.py .                                                      [100%]\r\n\r\n============================== 1 passed in 0.02s ===============================\r\n```\r\n\r\nEnvironment:\r\n\r\n- Python version: 3.11.9\r\n- Flask version: 3.0.3"
  },
  {
    "name": "Change of query string encoding behaviour in flask 3.0",
    "description": "Flask until 2.0 decoded %-encoded entities from query strings, while it seems that Flask 3.0 does not.\r\n\r\nGiven this `print_args.py`:\r\n\r\n```py\r\nfrom flask import Flask, request\r\n\r\napp = Flask(__name__)\r\n\r\n\r\n@app.route(\"/\")\r\ndef hello_world():\r\n    return request.args.get(\"test\")\r\n```\r\n\r\nOk Flask 2:\r\n\r\n```\r\n$ flask --version\r\nPython 3.11.2\r\nFlask 2.2.2\r\nWerkzeug 2.2.2\r\n$ flask --app print_args run\r\n...\r\ncurl http://127.0.0.1:5000?test=%A0+++a\r\n\ufffd   a\r\n```\r\n\r\nOn Flask 3:\r\n\r\n```\r\n$ flask --version\r\nPython 3.11.9\r\nFlask 3.0.3\r\nWerkzeug 3.0.2\r\n$ flask --app print_args run\r\n...\r\ncurl http://127.0.0.1:5000?test=%A0+++\r\n%A0   a\r\n```\r\n\r\nWhile I understand  %A0 is not a valid unicode sequence, this is a change of behaviour may be worth documenting."
  },
  {
    "name": "Add support for partitioned session cookies",
    "description": "Related issue with historical context: https://github.com/pallets/werkzeug/issues/2797\r\n\r\nPartitioned cookies are now supported in Werkzeug: https://github.com/pallets/werkzeug/pull/2855\r\n\r\nIt would be nice to have first-class support for the flask session cookie using the `Partitioned` flag to provide support for Chrome's CHIPS and allow sessions to continue to work within iframes.\r\n\r\nExample usage might be:\r\n\r\n```python\r\napp.config['SESSION_COOKIE_SAMESITE'] = \"None\"\r\napp.config['SESSION_COOKIE_SECURE'] = True\r\napp.config['SESSION_COOKIE_PARTITIONED'] = True # add this?\r\n```"
  },
  {
    "name": "Failing to start (due to port unavailable) cannot be directly determined by the current python api",
    "description": "I believe that failing to start (due to the designated port being unavailable) cannot be directly determined by the current python api, if not mistaken.\r\n\r\nWhen the port specified for startup is taken, a python code starting the server with Werkzeug cannot know if startup has failed, since no exception is raised, and no api is there to check on its status, while the following is written to stderr:\r\n\r\n> Address already in use\r\n> Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\r\n\r\nexample code:\r\n\r\n```python\r\nfrom flask import Flask\r\nfrom werkzeug.serving import run_simple\r\n\r\napp = Flask(__name__)\r\n\r\ntry:\r\n    run_simple('127.0.0.1', 5000, app)  \r\nexcept Exception as e:\r\n    start_success = False\r\n```\r\n\r\nEven though traditionally most web servers frameworks did not touch up their startup and shutdown usability from code to those levels, I think it would be implied to provide the caller with this basic level of transparency being robustly enabled for them. Apologies in advance if I've missed any obvious way. \r\n\r\n# Environment:\r\n\r\n- Python version: 3.10.13\r\n- Flask version: 3.0.3\r\n- Platform: Ubuntu 22.04\r\n\r\n# Mildly related on the same theme of api startup and shutdown: \r\nhttps://stackoverflow.com/questions/72824420/how-to-shutdown-flask-server"
  },
  {
    "name": "DRA Resource Health Status: the object has been modified; please apply your changes to the latest version and try again",
    "description": "### Which jobs are flaking?\n\nVarious E2E node tests:\n- https://testgrid.k8s.io/sig-node-dynamic-resource-allocation#ci-node-e2e-containerd-1-7-dra\n- https://testgrid.k8s.io/sig-node-dynamic-resource-allocation#ci-node-e2e-containerd-2-0-dra-alpha-beta-features\n\n\n### Which tests are flaking?\n\nE2eNode Suite: [It] [sig-node] [DRA] [Feature:DynamicResourceAllocation] [FeatureGate:DynamicResourceAllocation] Resource Health [FeatureGate:ResourceHealthStatus] [Alpha] [Feature:OffByDefault] [Serial] should handle concurrent DRA operations and health monitoring without connection issues\n\nE2eNode Suite: [It] [sig-node] [DRA] [Feature:DynamicResourceAllocation] [FeatureGate:DynamicResourceAllocation] Resource Health [Serial] should not add health status to Pod when feature gate is disabled \n\n### Since when has it been flaking?\n\nKubernetes 1.35 ?\n\n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\nProbably lacks a RetryOnConflict.\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig node"
  },
  {
    "name": "Potential goroutines leak in staging/src/k8s.io/client-go/tools/cache/shared_informer.go",
    "description": "### What happened?\n\nGoroutine 1:If the method distribute() is invoked first, a goroutine holds the **RLock()** and calls listener.add(obj).\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L863-L874\nThe goroutine get blocked at the `p.addCh <- notification `\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1015-L1020\n\nGoroutine 2:After that, The method shouldResync() is invoked, and the goroutine get blocked at `p.resyncLock.Lock()` , waiting for the **RLock()** to be released.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1090-L1099\n\nGoroutine 3: Because the goroutine 2 tries to get Lock() and Write lock has a higher priority. So p.listenersLock.RLock() get blocked, which results in p.wg.Start(listener.pop) can't get executed.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L881-L890\n\nAnd the `<-p.addCh` in listener.pop() can't be executed.\n\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L1037\n\nIn the situation, these three goroutines are all blocked. The interleaving sequence that triggers the bug is as follows:\n```\nG1                                            G2                             G3\np.resyncLock.RLock()\np.addCh <- notification \n                                            p.resyncLock.Lock()\n                                                                               p.resyncLock.RLock()                                 \n                                                                                <-p.addCh                  \n```\n\n### What did you expect to happen?\n\nNo goroutines leak.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Adding time.Sleep(time.Second * 7) before `p.listenersLock.RLock()`. This step is to ensure that Lock is executed before RLock.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer.go#L881-L883\n\n```\ntime.Sleep(time.Second * 7)\np.listenersLock.RLock()\n```\n2. Using goleak to detect the bug in the test function.\nhttps://github.com/kubernetes/kubernetes/blob/f36be36c76192c9ccaf8c22d61ec9ed3c0a2f978/staging/src/k8s.io/client-go/tools/cache/shared_informer_test.go#L210\n\n```\nfunc TestListenerResyncPeriods(t *testing.T) {\n\tdefer goleak.VerifyNone(t)\n```\n\nThe bug report is as follows,\n\n```\ngoroutine 20 [chan send]:\nk8s.io/client-go/tools/cache.(*processorListener).add(0x24c00e0, {0xdb2360, 0x2409480})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1022 +0x6e\nk8s.io/client-go/tools/cache.(*sharedProcessor).distribute(0x2498280, {0xdb2360, 0x2409480}, 0x0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:873 +0xed\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).OnAdd(0x24c0000, {0xe4d1c0, 0x2632008}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:739 +0x82\nk8s.io/client-go/tools/cache.processDeltas({0xfdc284, 0x24c0000}, {0xfe010c, 0x24900a0}, {0x2409460, 0x1, 0x1}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:576 +0x1b3\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).HandleDeltas(0x24c0000, {0xda8220, 0x2409470}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:729 +0x116\nk8s.io/client-go/tools/cache.(*RealFIFO).Pop(0x2614000, 0x2602058)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/the_real_fifo.go:234 +0x464\nk8s.io/client-go/tools/cache.(*controller).processLoop(0x2616000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:211 +0x50\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1({0xfdeb24, 0x249a2c0}, 0x24b5ee8)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x57\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext({0xfdeb24, 0x249a2c0}, 0x24b5ee8, {0xfd9650, 0x26000c0}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xa5\nk8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0xfdeb24, 0x249a2c0}, 0x24b5ee8, 0x3b9aca00, 0x0, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x94\nk8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172\nk8s.io/client-go/tools/cache.(*controller).RunWithContext(0x2616000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:183 +0x3c4\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext(0x24c0000, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:587 +0x354\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).Run(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:526\nk8s.io/client-go/tools/cache.TestListenerResyncPeriods.(*Group).StartWithChannel.func2()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:55 +0x20\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 18\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 35 [sync.RWMutex.RLock]:\nsync.runtime_SemacquireRWMutexR(0x2498290, 0x0, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:100 +0x3d\nsync.(*RWMutex).RLock(0x2498284)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:74 +0x50\nk8s.io/client-go/tools/cache.(*sharedProcessor).run.func1(0x2498280)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:886 +0x4e\nk8s.io/client-go/tools/cache.(*sharedProcessor).run(0x2498280, {0xfdeb98, 0x2618000})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:893 +0x31\nk8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x27\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 36 [sync.Mutex.Lock]:\ninternal/sync.runtime_SemacquireMutex(0x2614004, 0x0, 0x2)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:95 +0x3d\ninternal/sync.(*Mutex).lockSlow(0x2614000)\n\tD:/Program Files (x86)/Go/src/internal/sync/mutex.go:149 +0x245\ninternal/sync.(*Mutex).Lock(0x2614000)\n\tD:/Program Files (x86)/Go/src/internal/sync/mutex.go:70 +0x4a\nsync.(*Mutex).Lock(...)\n\tD:/Program Files (x86)/Go/src/sync/mutex.go:46\nsync.(*RWMutex).Lock(0x2614000)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:150 +0x28\nk8s.io/client-go/tools/cache.(*RealFIFO).Close(0x2614000)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/the_real_fifo.go:69 +0x2f\nk8s.io/client-go/tools/cache.(*controller).RunWithContext.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:152 +0x4d\ncreated by k8s.io/client-go/tools/cache.(*controller).RunWithContext in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/controller.go:150 +0xcf\n\ngoroutine 37 [sync.WaitGroup.Wait]:\nsync.runtime_SemacquireWaitGroup(0x2606068, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:114 +0x4d\nsync.(*WaitGroup).Wait(0x2606060)\n\tD:/Program Files (x86)/Go/src/sync/waitgroup.go:206 +0xa1\nk8s.io/apimachinery/pkg/util/wait.(*Group).Wait(...)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:48\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:473 +0x30\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync(0x26040a8, {0xfdeb24, 0x249a2c0}, {0x0, 0x0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:478 +0x174\nk8s.io/client-go/tools/cache.(*Reflector).ListAndWatchWithContext(0x26040a8, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:433 +0x3eb\nk8s.io/client-go/tools/cache.(*Reflector).RunWithContext.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:358 +0x38\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1({0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x17\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1({0xfdeb24, 0x249a2c0}, 0x2621f68)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x57\nk8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext({0xfdeb24, 0x249a2c0}, 0x2621f68, {0xfd9660, 0x2626000}, 0x1)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xa5\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x2621fa4, {0xfd9660, 0x2626000}, 0x1, 0x249a2c0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:233 +0x5c\nk8s.io/client-go/tools/cache.(*Reflector).RunWithContext(0x26040a8, {0xfdeb24, 0x249a2c0})\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:357 +0x200\nk8s.io/client-go/tools/cache.(*controller).RunWithContext.(*Group).StartWithContext.func3()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x27\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 20\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n\ngoroutine 40 [sync.RWMutex.Lock]:\nsync.runtime_SemacquireRWMutex(0x249828c, 0x0, 0x0)\n\tD:/Program Files (x86)/Go/src/runtime/sema.go:105 +0x3d\nsync.(*RWMutex).Lock(0x2498284)\n\tD:/Program Files (x86)/Go/src/sync/rwmutex.go:155 +0x83\nk8s.io/client-go/tools/cache.(*sharedProcessor).shouldResync(0x2498280)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:915 +0x4f\nk8s.io/client-go/tools/cache.(*Reflector).startResync(0x26040a8, {0xfdeb98, 0x26181e0}, 0x26261c0)\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:451 +0x144\nk8s.io/client-go/tools/cache.(*Reflector).watchWithResync.func2()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/client-go/tools/cache/reflector.go:476 +0x33\nk8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x4a\ncreated by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 37\n\te:/goProjects/src/kubernetes-master/kubernetes-master/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x77\n```\nFAIL\tk8s.io/client-go/tools/cache\t32.469s\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nLatest\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "containerStatuses ready stays true while started is false after container restart at the first time",
    "description": "### What happened?\n\nIn pods' ContainerStatuses, there are two state fields, ready: controlled by readinessProbe, started: controlled by startupProbe.\n\nAs designed, readinessProbe won't start working until startupProbe is succeed.\n\nNormally the field 'ready' won't be true if 'started' is still false.\n\nBut if the container in the pod exited, and the restartPolicy is Always or OnFailure, kubelet will try restart the container process immediately without changing the state of the pod. \n\nThat cause ContainerStatuses 'ready' stays true while 'started' is false after container restart at the first time.\n\n### What did you expect to happen?\n\nSet the ready state to false when the container restart at the first time.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI reproduced this issue with a busybox configured as below, the startupProbe will pass after 30s, livenessProbe will kill the container after 60s, you can get the pod yaml after it restarted at the first time to see the fields: \n`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: busybox\n  name: busybox\nspec:\n  selector:\n    matchLabels:\n      app: busybox\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: busybox\n    spec:\n      containers:\n      - name: busybox\n        image: busybox:latest\n        command:\n          - bash\n          - -c\n          - sleep 30 && echo 1 > /started && sleep 3600\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          exec:\n            command:\n            - cat\n            - /started\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - ls\n        livenessProbe:\n          exec:\n            command:\n            - cat\n            - /live\n          failureThreshold: 6\n          periodSeconds: 10\n      restartPolicy: Always`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.31.1\nKustomize Version: v5.4.2\nServer Version: v1.31.1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n\n$ uname -a\n\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Failing Test - CRD Publish OpenAPI Test Flake",
    "description": "### Which jobs are failing?\n\nci-kubernetes-e2e-gci-gce-flaky \n\n### Which tests are failing?\n\n[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] [It] [Flaky] kubectl explain works for CR with the same resource name as built-in object is failing intermittently with a resource type not found error.\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-flaky/2008258219528425472\n\n### Since when has it been failing?\n\n1,512\n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\nSeems like the test creates a CRD and immediately runs kubectl explain, but the OpenAPI schema hasnt fully propagated to all API servers in an HA setup, or kubectl\u2019s discovery cache hasn\u2019t refreshed.\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig"
  },
  {
    "name": "[client-go] add a pod-logs example",
    "description": "**What would you like to be added?** I'd like to add a new pod-logs example to `staging/src/k8s.io/client-go/examples`. It's a simple script that lists pods across namespaces and streams the last few lines of their container logs to the terminal.\n\n**Why is this needed?** I noticed we don't actually have a basic \"how to get pod logs\" example. It's such a common thing for people to do, but there isn't a dedicated example yet!\n\nI've based this on the official [Python client's logs example](https://github.com/kubernetes-client/python/blob/master/examples/pod_logs.py)."
  },
  {
    "name": "ConfigMaps and Secrets should not have world-writeable directories",
    "description": "### What happened?\n\nI was debugging a container that was broken for some time and found the following after fixing a bug I caused:\n```\nmod_auth_file/1.0: unable to use AuthUserFile from world-writable directory '/etc/proftpd/secret' (perms 1777): Operation not permitted\n```\napparently here the problem is that even through I have set a `defaultMode` the volume it self, symlinks and directories are all created with the permissions 777\n\n### What did you expect to happen?\n\nThe directory not to be world writable but either based on the default mode or adjustable in another way.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n* create a `ConfigMap` or `Secret`\n* add a `defaultMode` like `0o400`\n* run `ls -la /path/to/dir`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.35.0\nKustomize Version: v5.7.1\nServer Version: v1.34.3+k3s1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nProxmox\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n$ uname -a\nLinux <node-name> 5.15.0-164-generic #174-Ubuntu SMP Fri Nov 14 20:25:16 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\ninstalled via k3s\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd\n\n```console\n$ /var/lib/rancher/k3s/data/ffd82c56f7aadb3263c7708547a6027705a0eb9a638534465002ba309cae9990/bin/containerd-shim-runc-v2 -v\ncontainerd-shim-runc-v2:\n  Version:  v2.1.5-k3s1\n  Revision:\n  Go version: go1.24.11\n```\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "[Failing-test] Kubelet panic in NodeDeclareFeatures code",
    "description": "### Which jobs are failing?\n\nalpha jobs\n\n### Which tests are failing?\n\nEntire job.\n\n\n### Since when has it been failing?\n\nTest grid for some reason doesn't go very far back.\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-node-containerd#ci-node-e2e-containerd-alpha-features\n\n### Reason for failure (if possible)\n\n```\nI0105 07:36:00.527576    2667 kubelet.go:2611] \"SyncLoop DELETE\" source=\"api\" pods=[\"mirror-pod-6526/static-pod-909c75fb-fb8e-4443-b1d1-79f09ce6219a-tmp-node-e2e-9ee50a67-ubuntu-gke-2404-1-34-amd64-v20251218a\"]\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x1d6e0b9]\n\ngoroutine 234 [running]:\nk8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodUpdates(0xc000359008, {0xc00112cfa8, 0x1, 0x0?})\n\tk8s.io/kubernetes/pkg/kubelet/kubelet.go:2830 +0x459\nk8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration(0xc000359008, {0x295a2d0, 0x3b54280}, 0xc0000f5ce0, {0x2963408, 0xc000359008}, 0xc00043ee70, 0xc00043ef50, 0xc00033ff10)\n\tk8s.io/kubernetes/pkg/kubelet/kubelet.go:2613 +0xa22\nk8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop(0xc000359008, {0x295a2d0, 0x3b54280}, 0xc0000f5ce0, {0x2963408, 0xc000359008})\n\tk8s.io/kubernetes/pkg/kubelet/kubelet.go:2542 +0x34c\nk8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run(0xc000359008, 0xc0000f5ce0)\n\tk8s.io/kubernetes/pkg/kubelet/kubelet.go:1896 +0x8db\ncreated by k8s.io/kubernetes/cmd/kubelet/app.startKubelet in goroutine 1\n\tk8s.io/kubernetes/cmd/kubelet/app/server.go:1264 +0xa9\n```\n\n### Anything else we need to know?\n\nhttps://github.com/kubernetes/kubernetes/blob/b2ac9e206fdd912f35f2ab5b3c5b5243303ba14b/pkg/kubelet/kubelet.go#L2830\n\nLooks like oldPod may not always be set so this triggers the panic.\n\n### Relevant SIG(s)\n\n/sig node"
  },
  {
    "name": "Support compression for WATCH responses",
    "description": "### What would you like to be added?\n\nI propose adding compression support for WATCH responses in the Kubernetes API server. This feature is necessary to maintain parity with the bandwidth efficiency of the legacy LIST method as clients transition to WatchList.\n\nThe proposal includes:\n\n1. **Compression for WATCH:** Implement the capability for the server to provide compressed streaming data for WATCH connections.\n2. **Algorithm Registration Interface:** Introduce an interface allowing custom compression algorithms to be registered. This ensures client-go remains lightweight and does not force new mandatory dependencies on all users.\n3. **Default Realtime Compression:** Kubernetes binaries (e.g., kube-apiserver, kube-controller-manager, kubectl) should include and support a realtime data compression like [s2] by default to maximize throughput.\n\n[s2]: https://github.com/klauspost/compress/tree/master/s2\n\n### Why is this needed?\n\nThe default enablement of WatchList for clients in Kubernetes 1.35 has fundamentally changed how initial object sets are fetched. While LIST operations are compressed, WATCH operations are not, leading to a massive spike in control plane outbound traffic.\n\n### Key Impacts:\n* Bandwidth Explosion: For most clusters communicating over a network, control plane outbound traffic can increase by 5x.\n* Scalability Bottleneck: This increased data transfer is a critical blocker for the \"Resource Size\" scalability dimension proposed in [issue #134375](https://github.com/kubernetes/kubernetes/issues/134375).\n\n### Why not gzip?\n* Performance Regression: Standard library GZIP implementation often slows data transfer by 2x due to CPU overhead. However, modern algorithms like [s2] can actually increase throughput by 2.5x while significantly reducing volume.\n* Breaking Change Risk: Most clients are configured to send Accept-Encoding: gzip by default, but they do not expect a compressed response for WATCH. Enabling GZIP without a negotiation mechanism would break these clients.\n\n### Comparison: Performance and Volume\n\n| Method | Compression | Data Volume (lower better) | Throughput (higher better) |\n| :--- | :--- | :--- | :--- |\n| **LIST** (Legacy) | GZIP | 1x (Baseline) | 1x (Baseline) |\n| **WATCH** (1.35 Default) | None | **5x Increase** | Network-bottlenecked |\n| **WATCH** (Proposed [s2]) | [s2] | **8x Decrease** | **2.5x Increase** |\n\n[s2] was selected as it showed much better throughput over other tested algorithms (pgzip, zstd,br,lz4)\n\n### **Goals**\n*   Avoid the 5x increase in data transfer volume as clients switch to `WatchList`.\n*   Ensure no new mandatory dependencies are added to the `client-go` library.\n\n### **Non-Goals**\n*   Supporting every available compression algorithm.\n*   Improving the performance of the standard Go GZIP library.\n\n### **Open Questions**\n*   How should this effort be tracked and coordinated given that the original GZIP KEP ([#2338](https://github.com/kubernetes/enhancements/issues/2338)) is currently stale?\n*  Should we directly go to https://github.com/minio/minlz instead of [s2] as it written by the same author based on the collected experience.\n\n/kind feature\n/sig api-machinery\n/sig scalability\n/cc @liggitt @p0lyn0mial @wojtek-t @mborsz @jpbetz"
  },
  {
    "name": "Error-level logs incorrectly use verbosity levels",
    "description": "Currently, some components in Kubernetes have been migrated (or partially migrated) to [contextual logging](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md). However, an incorrect pattern was introduced during the migration: Error-level logs are being called with verbosity methods (e.g., .V()).\n\nThe underlying package go-logr/logr, which contextual logging relies on, ignores the verbosity level when printing Error-level logs. For example, in the statement `logger.V(8).Error(nil, \"Readiness probe already exists for container\", \"pod\", klog.KObj(pod), \"containerName\", c.Name)`, the error log will be printed regardless of the verbosity level set.\n\nThis can lead to error log flooding, which may hinder users' ability to review normal logs for troubleshooting. More critically, according to the [Kubernetes logging policy](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md#what-method-to-use), logs should only be recorded when the information requires action from an administrator. This directly contradicts the practice of setting verbosity levels for Error logs, as administrators using the default logging level might miss these actionable errors. Therefore, these instances need to be fixed.\n\nUsing grep, I identified the following instances in the k/k repository where verbosity levels are incorrectly set for Error logs.\n- [ ] /pkg/controller/garbagecollector\n```\n./pkg/controller/garbagecollector/garbagecollector.go:363:\t\t\tlogger.V(5).Error(err, \"error syncing item\", \"item\", n.identity)\n./pkg/controller/garbagecollector/graph_builder.go:225:\t\tlogger.V(4).Error(err, \"unable to use a shared informer\", \"resource\", resource, \"kind\", kind)\n```\n- [ ] /pkg/controller/resourcequota\n```\n./pkg/controller/resourcequota/resource_quota_monitor.go:178:\tlogger.V(4).Error(err, \"QuotaMonitor unable to use a shared informer\", \"resource\", resource.String())\n```\n- [ ] /pkg/controller/storageversionmigrator/\n```\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:227:\t\tlogger.V(4).Error(err, \"resource does not exist in our rest mapper\", \"gvr\", gvr.String())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:244:\t\tlogger.V(4).Error(errMonitor, \"resource does not exist in GC\", \"gvr\", gvr.String())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:311:\t\t\tlogger.V(4).Error(err, \"Unable to compare the resource version of the resource\", \"namespace\", accessor.GetNamespace(), \"name\", accessor.GetName(), \"gvr\", gvr.String(), \"accessorRV\", accessor.GetResourceVersion(), \"listResourceVersion\", listResourceVersion, \"error\", err.Error())\n./pkg/controller/storageversionmigrator/storageversionmigrator.go:361:\t\t\tlogger.V(4).Error(errPatch, \"Failed to migrate the resource\", \"namespace\", accessor.GetNamespace(), \"name\", accessor.GetName(), \"gvr\", gvr.String(), \"reason\", apierrors.ReasonForError(errPatch))\n```\n- [ ] ./pkg/controller/volume/\n```\n./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:458:\t\t\tlogger.V(4).Error(err, \"failed to get SELinux label\", \"pod\", klog.KObj(pod), \"volume\", mount)\n./pkg/controller/volume/selinuxwarning/selinux_warning_controller.go:514:\t\t\tlogger.V(2).Error(err, \"failed to get first pod for event\", \"pod\", conflict.Pod)\n```\n- [ ] ./pkg/kubelet/volumemanager/\n```\n./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:673:\tlogger.V(4).Error(err, \"Please report this error in https://github.com/kubernetes/enhancements/issues/1710, together with full Pod yaml file\")\n./pkg/kubelet/volumemanager/cache/desired_state_of_world.go:688:\t\tlogger.V(4).Error(err, \"failed to get CSI driver name from volume spec\")\n./pkg/kubelet/volumemanager/reconciler/reconstruct.go:188:\t\tlogger.V(4).Error(fetchErr, \"Failed to get Node status to reconstruct device paths\")\n```\n\n\n- [ ] ./pkg/kubelet https://github.com/kubernetes/kubernetes/pull/136028\n```\n./pkg/kubelet/stats/helper.go:502:\t\tlogger.V(6).Error(err, \"Unable to fetch pod log stats\", \"pod\", klog.KRef(podNs, podName))\n./pkg/kubelet/stats/helper.go:510:\t\tlogger.V(6).Error(err, \"Unable to fetch pod etc hosts stats\", \"pod\", klog.KRef(podNs, podName))\n./pkg/kubelet/stats/cri_stats_provider.go:168:\t\tlogger.V(5).Error(err,\n./pkg/kubelet/cm/devicemanager/plugin/v1beta1/client.go:107:\t\t\tlogger.V(2).Error(err, \"Failed to close grpc connection\", \"resource\", c.Resource())\n./pkg/kubelet/cm/dra/plugin/dra_plugin_manager.go:398:\t\t\t\t\tlogger.V(3).Error(err, \"Failed to establish WatchResources stream, will retry\")\n./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:431:\t\t\tlogger.V(5).Error(fmt.Errorf(\"failed to parse /proc/self/cgroup: %w\", err), warn)\n./pkg/kubelet/kuberuntime/kuberuntime_container_linux.go:442:\t\t\tlogger.V(5).Error(err, warn)\n./pkg/kubelet/lifecycle/handlers.go:85:\t\t\tlogger.V(1).Error(err, \"Exec lifecycle hook for Container in Pod failed\", \"execCommand\", handler.Exec.Command, \"containerName\", container.Name, \"pod\", klog.KObj(pod), \"message\", string(output))\n./pkg/kubelet/lifecycle/handlers.go:93:\t\t\tlogger.V(1).Error(err, \"HTTP lifecycle hook for Container in Pod failed\", \"path\", handler.HTTPGet.Path, \"containerName\", container.Name, \"pod\", klog.KObj(pod))\n./pkg/kubelet/lifecycle/handlers.go:101:\t\t\tlogger.V(1).Error(err, \"Sleep lifecycle hook for Container in Pod failed\", \"sleepSeconds\", handler.Sleep.Seconds, \"containerName\", container.Name, \"pod\", klog.KObj(pod))\n./pkg/kubelet/lifecycle/handlers.go:152:\t\tlogger.V(1).Error(err, \"HTTPS request to lifecycle hook got HTTP response, retrying with HTTP.\", \"pod\", klog.KObj(pod), \"host\", req.URL.Host)\n./pkg/kubelet/prober/prober_manager.go:197:\t\t\t\tlogger.V(8).Error(nil, \"Startup probe already exists for container\",\n./pkg/kubelet/prober/prober_manager.go:209:\t\t\t\tlogger.V(8).Error(nil, \"Readiness probe already exists for container\",\n./pkg/kubelet/prober/prober_manager.go:221:\t\t\t\tlogger.V(8).Error(nil, \"Liveness probe already exists for container\",\n./pkg/kubelet/prober/prober.go:106:\t\tlogger.V(1).Error(err, \"Probe errored\", \"probeType\", probeType, \"pod\", klog.KObj(pod), \"podUID\", pod.UID, \"containerName\", container.Name, \"probeResult\", result)\n./pkg/kubelet/metrics/collectors/cri_metrics.go:110:\t\tlogger.V(5).Error(err, \"Descriptor not present in pre-populated list of descriptors\", \"name\", m.Name)\n./pkg/kubelet/pleg/generic.go:300:\t\t\tg.logger.V(4).Error(err, \"PLEG: Ignoring events for pod\", \"pod\", klog.KRef(pod.Namespace, pod.Name))\n./pkg/kubelet/allocation/allocation_manager.go:576:\t\tlogger.V(3).Error(err, \"Failed to delete pod allocation\", \"podUID\", uid)\n```"
  },
  {
    "name": "Support cpu.idle (SCHED_IDLE) for BestEffort QoS to improve isolation and node utilization",
    "description": "**Is this a BUG REPORT or FEATURE REQUEST?**: \n\n- /kind feature \n\n- /sig node\n\n**Motivation**:\nCurrently, Kubelet sets `cpu.shares` to `2` for the top-level BestEffort QoS cgroup. The goal is to allow BestEffort tasks to utilize free CPU cycles (improving node utilization) while minimizing the impact on Guaranteed and Burstable workloads.\n\nHowever, relying solely on `cpu.shares` (even with a minimal value) provides \"weight-based\" isolation. In high-load scenarios, this can still lead to latency interference for high-priority tasks.\n\nSince Linux Kernel 5.4, significant improvements have been made to `SCHED_IDLE` (Reference Patch: `sched/fair: Improve the scheduling of SCHED_IDLE tasks`).\n* **Better Isolation**: `cpu.idle` (SCHED_IDLE) ensures that tasks are treated with truly \"idle\" priority. They only run when there is absolutely no other higher-priority task to run. This offers significantly better isolation for Guaranteed/Burstable pods compared to simply lowering `cpu.shares`.\n* **Safe Colocation**: It allows safe colocation of BestEffort batch jobs with latency-sensitive services without the risk of performance degradation for the latter.\n\n**Current Barrier (The Conflict)**:\nWe are currently attempting to utilize `cpu.idle` for BestEffort pods using external agents/extensions on Cgroup v2 nodes. However, this conflicts with Kubelet's current initialization logic, causing Kubelet to fail to start.\n\nIn Cgroup v2, when `cpu.idle` is set to `1`:\n1.  The kernel forces `cpu.shares` to `1` (minimal weight).\n2.  The kernel locks `cpu.shares` from being modified.\n\nBecause Kubelet hardcodes the enforcement of `cpu.shares = 2` for BestEffort, it crashes during startup if `cpu.idle` is already enabled by an external tool or previous configuration:\n\n\n> \"Failed to start ContainerManager\" err=\"failed to initialize top level QOS containers: failed to update top level BestEffort QOS cgroup : failed to write \\\"2\\\": write /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-besteffort.slice/cpu.shares: invalid argument\"\n\n\n\n**What you expected to happen:** Kubelet should start successfully even if `cpu.idle` is enabled for the BestEffort cgroup. Kubelet should either detect that `cpu.idle` is enabled and skip setting `cpu.shares`, or handle the write error gracefully.\n\n**How to reproduce it (as minimally and precisely as possible):**\n\n1. On a node with a Kernel supporting `cpu.idle` interface (Linux).\n\n2. Manually (or via an external agent) set `cpu.idle` to `1` for the BestEffort slice:\n\n```\nBash\n\necho 1 > /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-besteffort.slice/cpu.idle\n\n```\n3. Restart Kubelet.\n4. Observe the startup failure.\n\n5. Manual verification of the conflict:\n\n```\nBash\n\n# echo 2 > /sys/fs/cgroup/cpuacct/kubepods.slice/kubepods-besteffort.slice/cpu.shares\n-bash: echo: write error: Invalid argument\n```\n**Anything else we need to know:**\n\n- **Context:** We use `cpu.idle` (SCHED_IDLE) for BestEffort tasks to utilize the enhanced scheduling isolation provided by Linux Kernel 5.4+ (Patch: `sched/fair: Improve the scheduling of SCHED_IDLE tasks`). This is often a superior alternative to low `cpu.shares` for maximizing node utilization without interfering with high-priority workloads.\n\n- **Kernel Behavior:** When `cpu.idle` is configured to `1`, the kernel forces `cpu.shares` to `1` and forbids modification, leading to the Invalid argument error when Kubelet tries to write `2.`\n\n- **Workaround:** Setting `cpu.idle` back to `0` allows Kubelet to start.\n\n**Proposed Solution:**\n\n1. **Short-term fix (Resilience)**: Modify the Kubelet `qos_container_manager` logic. Before attempting to set `cpu.shares` for the BestEffort cgroup, check if cpu.idle is configured (if the interface exists).\n\n    If `cpu.idle == 1`: Skip setting `cpu.shares` (accept the kernel's forced value).\n\n    If `cpu.idle == 0` or file does not exist: Proceed with setting `cpu.shares` to `2`.\n\n2. **Long-term feature**: Add native support in Kubelet configuration to support `cpu.idle` (SCHED_IDLE) for BestEffort pods, allowing users to opt-in to this scheduling policy directly via Kubelet rather than relying on external agents.\n\n**Benefits:**\n\n**Superior QoS**: Provides strict priority isolation for Guaranteed/Burstable pods, reducing tail latency in colocation scenarios.\n\n**Resilience**: Prevents Kubelet startup failures for users attempting to use SCHED_IDLE via custom tuning agents."
  },
  {
    "name": "kubelet panic when allcoate gpu parallelly",
    "description": "### What happened?\n\nAviod kubelet panic when allocate gpu parallelly. such as the following scenarios. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/manager.go#L365-L404\n\n- t0: pod1 starts to allocate gpu and setting m.devicesToReuse[pod1.uid]=aaa,and then allocateContainerResources call rpc;\n- t1: pod2 starts to allocate gpu and setting m.devicesToReuse[pod2.uid]=bbb,meanwhile delete(m.m.devicesToReuse, pod1.uid);\n- t2: pod1 runs to m.podDevices.removeContainerAllocatedResources and m.devicesToReuse[pod1.uid] would be nil. Those will call panic.\n\n### What did you expect to happen?\n\nkubelet do not panic.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n  https://github.com/kubernetes/kubernetes/pull/136023 fixes this. \nUt will reproduce it. \n```\nfunc TestDeviceAllocateParallel(t *testing.T) {\n\tres1 := TestResource{\n\t\tresourceName:     \"domain3.com/resource3\",\n\t\tresourceQuantity: *resource.NewQuantity(int64(2), resource.DecimalSI),\n\t\tdevs:             checkpoint.DevicesPerNUMA{0: []string{\"dev5\", \"dev6\"}},\n\t\ttopology:         false,\n\t}\n\tres2 := TestResource{\n\t\tresourceName:     \"domain1.com/resource1\",\n\t\tresourceQuantity: *resource.NewQuantity(int64(2), resource.DecimalSI),\n\t\tdevs:             checkpoint.DevicesPerNUMA{0: []string{\"dev1\", \"dev2\"}},\n\t\ttopology:         false,\n\t}\n\n\tas := require.New(t)\n\tpodsStub := activePodsStub{\n\t\tactivePods: []*v1.Pod{},\n\t}\n\ttmpDir, err := os.MkdirTemp(\"\", \"checkpoint\")\n\tas.NoError(err)\n\tdefer os.RemoveAll(tmpDir)\n\n\ttestManager, err := getTestManager(tmpDir, podsStub.getActivePods, []TestResource{res1, res2})\n\tas.NoError(err)\n\ttestManager.endpoints[res1.resourceName] = endpointInfo{\n\t\te: &MockEndpoint{\n\t\t\tallocateFunc: func(devs []string) (*pluginapi.AllocateResponse, error) {\n\t\t\t\tresp := new(pluginapi.ContainerAllocateResponse)\n\t\t\t\tresp.Envs = make(map[string]string)\n\t\t\t\tfor _, dev := range devs {\n\t\t\t\t\tswitch dev {\n\t\t\t\t\tcase \"dev5\":\n\t\t\t\t\t\tresp.Envs[\"key2\"] = \"val2\"\n\n\t\t\t\t\tcase \"dev6\":\n\t\t\t\t\t\tresp.Envs[\"key2\"] = \"val3\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tresps := new(pluginapi.AllocateResponse)\n\t\t\t\tresps.ContainerResponses = append(resps.ContainerResponses, resp)\n\t\t\t\ttime.Sleep(2 * time.Second)\n\t\t\t\treturn resps, nil\n\t\t\t},\n\t\t},\n\t\topts: nil,\n\t}\n\n\tpod1 := makePod(v1.ResourceList{\n\t\tv1.ResourceName(res1.resourceName): res1.resourceQuantity})\n\tpod2 := makePod(v1.ResourceList{\n\t\tv1.ResourceName(res2.resourceName): res2.resourceQuantity})\n\tactivePods := []*v1.Pod{}\n\tactivePods = append(activePods, pod1, pod2)\n\tpodsStub.updateActivePods(activePods)\n\n\twg := sync.WaitGroup{}\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\terr1 := testManager.Allocate(pod1, &pod1.Spec.Containers[0])\n\t\tas.NoError(err1)\n\t}()\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\ttime.Sleep(500 * time.Millisecond)\n\t\terr2 := testManager.Allocate(pod2, &pod2.Spec.Containers[0])\n\t\tas.NoError(err2)\n\t}()\n\n\twg.Wait()\n}\n```\n\n\n\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nall versions\n\n### Cloud provider\n\n<details>\n\n</details>"
  },
  {
    "name": "client-go: the title of subgraph is overlapped by nodes",
    "description": "Due to [mermaid #3806](https://github.com/mermaid-js/mermaid/issues/3806), the mermaid diagram in [staging/src/k8s.io/client-go/ARCHITECTURE.md](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/ARCHITECTURE.md) cannot be rendered properly. The title of  `subgraph \"client-go: Informer Mechanism\"` is overlapped by nodes.\n\nScreenshot:\n<img width=\"379\" height=\"849\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6e64f96b-e68a-4205-a00f-37076f4a483b\" />"
  },
  {
    "name": "Failure cluster Lifecycle sleep action zero value when create a pod with lifecycle hook using sleep action with a duration of zero seconds prestop hook using sleep action with zero duration",
    "description": "### Failure cluster [227e0df417839dc5d5ed](https://go.k8s.io/triage#227e0df417839dc5d5ed)\n\n##### Error text:\n```\n[FAILED] unexpected delay duration before killing the pod, cost = 30.207689386s\nIn [It] at: k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:741 @ 12/22/25 10:31:04.983\n\n```\n#### Recent failures:\n[1/1/2026, 11:26:21 AM e2e-kops-gce-cni-calico](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-gce-cni-calico/2006763877311713280)\n[12/30/2025, 5:21:24 PM e2e-kops-aws-distro-u2510](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-aws-distro-u2510/2006128433188311040)\n[12/29/2025, 10:56:59 PM e2e-kops-grid-gce-ipalias-deb13-k35](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-ipalias-deb13-k35/2005850514016702464)\n[12/29/2025, 11:23:58 AM e2e-kops-grid-gce-kubenet-rhel10-k34](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-kubenet-rhel10-k34/2005676112801173504)\n[12/25/2025, 11:23:20 AM e2e-kops-grid-gce-kubenet-rhel10-k34](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/e2e-kops-grid-gce-kubenet-rhel10-k34/2004226404379529216)\n\n\n/kind failing-test\n<!-- If this is a flake, please add: /kind flake -->\n\n/sig node"
  },
  {
    "name": "Improve WatchList Observability: Addressing Logging, Tracing, and Metric Gaps",
    "description": "### What would you like to be added?\n\nI would like to propose addressing several observability gaps in the WatchList feature (introduced in [KEP-3157](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/3157-watch-list)). In Kubernetes 1.35 the controllers transitioned from the standard LIST operation to WatchList for fetching initial object sets, the  feature currently lacks the parity in logging and tracing required for effective production debugging.\n\n**Overview of WatchList:**\nWatchList allows controllers to bypass the initial `LIST` request in the List-Watch protocol by initiating a single `WATCH` request that includes the initial set of objects. While this reduces server-side memory spikes and improves scalability, the current streaming nature of the response bypasses existing discrete-step tracing and logging mechanisms.\n\n### **Observability Comparison: LIST vs. WatchList**\n\n| Feature | LIST (Legacy) | WatchList (New) | OSS Gaps to Address |\n| :--- | :--- | :--- | :--- |\n| **Initial log** | N/A | Emitted as a standard `WATCH` request. | Missing `audit-ID` field in the starting log entry, making it impossible to correlate with other logs. |\n| **Access log** | Reports the `LIST` method with accurate latency. | Reported as `WATCH` after session completion. Latency reflects the entire session. | No specific log entry for the completion of the *initial listing* phase. `apf_init_latency` excludes serialization time. |\n| **Trace log** | Detailed logs for each step exceeding 500ms. | No trace logs generated due to streaming processing. | Missing traces that can break down streamed response writing into Network, Compress, and Serialize stages. |\n| **Latency metric**| `apiserver_request_duration_seconds` (STABLE). | `apiserver_watch_list_duration_seconds` (ALPHA). | Metric needs to reach **BETA** stability for reliable production use. |\n\n\nTasks:\n* Add audit-ID to Watch starting logs\n* Add log for initial list being completed\n* https://github.com/kubernetes/kubernetes/issues/136002\n* Graduate apiserver_watch_list_duration_seconds metric to Beta\n\n\n### Why is this needed?\n\nWith Kubernetes 1.35, controllers will switch to WatchList by default. Without these improvements, performance debugging for large clusters (e.g., clusters with >40,000 nodes) becomes nearly impossible, as we lose the ability to determine if a slow initial sync is caused by serialization, compression, or network bottlenecks.\n\n### **Example Logs for Reference**\n\n#### **LIST Method (Legacy)**\n**Access log:**\n```text\nI0102 09:50:22.075010 1 httplog.go:134] \"HTTP\" verb=\"LIST\" URI=\"/api/v1/pods?limit=500&resourceVersion=0\" latency=\"4.508309573s\" userAgent=\"kube-controller-manager/v1.36.0\" audit-ID=\"c736a782-3402-4072-8a25-c17f89598189\" resp=200\n```\n\n**Trace log:**\n```text\nTrace[1456271344]: \"List\" audit-id:c736a782-3402-4072-8a25-c17f89598189,verb:LIST (total time: 4507ms):\nTrace[1456271344]: [\"cacher.GetList\" 4507ms]\nTrace[1456271344]: [\"SerializeObject\" 4502ms]\n```\n\n#### **WatchList Method (Current Gaps)**\n**Initial log (Missing correlation):**\n```text\nI0102 09:37:41.848779 1 get.go:263] \"Starting watch\" path=\"/api/v1/pods\" resourceVersion=\"\"\n```\n\n**Access log (Latency reflects total session, not just initial listing):**\n```text\nI0102 09:39:25.708328 1 httplog.go:134] \"HTTP\" verb=\"WATCH\" URI=\"/api/v1/pods?sendInitialEvents=true...\" latency=\"1m43.859s\" audit-ID=\"2f3e2ef1-e9ed-49ed-8963-65ddc0c50244\" apf_init_latency=\"3.985s\" resp=200\n```\n\n\n/sig api-machinery\n/sig instrumentation\n/sig scalability\n/cc @p0lyn0mial @wojtek-t @jpbetz @liggitt @richabanker"
  },
  {
    "name": "Improve tracing observability for streamed responses (KEP-3157 and KEP-5116)",
    "description": "### What would you like to be added?\n\nI would like to propose an improvement to the Kubernetes tracing system to provide more granular visibility into streamed responses. Specifically, I propose wrapping each io.Writer in a wrapper that measures both the latency and the number of bytes processed at each stage of the response writing process.\n\nThis improvement would allow the tracing system to break down the overall latency into distinct categories:\n\n1. Network: Time spent sending data over the network.\n2. Compress: Time spent in the compression layer (e.g., gzip).\n3. Serialize: Time spent serializing objects into the response format.\n\nEach stage should report the accurate size of the data processed and its individual latency. \n\nProof of concept: https://github.com/kubernetes/kubernetes/pull/136001\n\n**Integration with Tracing:** Initial PoC hardcodes the steps, however we could make the implementation generic and properly integrate streamed/concurrent operation into the tracing framework.\n\n**Performance Consideration:** Adding time.Now() and time.Since() calls to each writer call introduces a negligible overhead of approximately 100ns per call. For a response containing 100,000 objects, the total cost would be roughly 10ms.\n\n/cc @wojtek-t @mborsz @richabanker @p0lyn0mial @jpbetz \n/sig scalability\n/sig api-machinery\n/sig instrumentation\n\n### Why is this needed?\n\nThe current implementation of Kubernetes tracing assumes that all computation is executed in clear, discrete steps. However, with the introduction of KEP-5116 (Streaming Encoding for LIST Responses) and KEP-3157 (WatchList), the existing traces have become ineffective for performance debugging.\n\nCurrently, traces for streamed responses:\n\n* Mix compression, encoding, and network latencies into a single reported value.\n* Show inaccurate values for object sizes (e.g., reporting only the size of the first chunk instead of the total data).\n\nWithout this improvement, it is impossible to determine whether a slow response is caused by network bottlenecks, serialization overhead, or compression delays.\n\n**Before**\n\nStreamed response writing results in traces mixing compressing, encoding and network. As result it only shows combined latency and inaccurate values of sizes. As measured on client side, the size presented in \"About to start writing response\" writer:*gzip.Writer,size:201240 227ms\u201d is incorrect as it\u2019s not the amount of data compressed, but the size of the first chunk. \n\n```\nTrace[1461829279]: [\"SerializeObject\" audit-id:e2ed459c-4f3c-47e4-abb3-099e8a4ad38d,method:GET,url:/api/v1/namespaces/0/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{\"encodeGV\":\"v1\",\"encoder\":\"protobuf\",\"name\":\"versioning\"} 9230ms (12:49:16.965)\nTrace[1461829279]:  ---\"About to start writing response\" writer:*gzip.Writer,size:201240 227ms (12:49:17.192)\nTrace[1461829279]:  ---\"Write call succeeded\" size:1006056694 9003ms (12:49:26.196)]\nTrace[1461829279]: ---\"Writing http response done\" count:10000 0ms (12:49:26.196)\nTrace[1461829279]: [9.244573577s] [9.244573577s] END\n```\n\n**After:**\n\nWith improved tracing we see three separate latencies: Network, Compress and Serialize. Each shows individual latency measurement and accurate size of data processed. Here we can see that gzip archives around a 5x compression ratio, which checks out on client side.\n\n```\nTrace[2139214577]: [\"SerializeObject\" audit-id:858bb641-5ba4-44d5-8e6a-d60e6609d4ed,method:GET,url:/api/v1/namespaces/0/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf,encoder:{\"encodeGV\":\"v1\",\"encoder\":\"protobuf\",\"name\":\"versioning\"} 9246ms (13:54:00.023)\nTrace[2139214577]:  ---\"Network\" size:188001850 2796ms\nTrace[2139214577]:  ---\"Compress\" size:1006056312 4966ms\nTrace[2139214577]:  ---\"Serialize\" count:10000 1483ms\nTrace[2139214577]:  ---\"Write call succeeded\" 0ms (13:54:09.270)]\nTrace[2139214577]: [9.251880697s] [9.251880697s] END\n```\n**Bonus WatchList:**\n```\nI0102 15:08:04.102809       1 trace.go:250] Trace[1455537276]: \"SerializeObject\" audit-id:bfdf3082-2666-44b7-b8cc-0208147aa063,method:GET,url:/api/v1/namespaces/0/pods,protocol:HTTP/2.0,mediaType:application/vnd.kubernetes.protobuf;stream=watch,encoder:{\"encodeGV\":\"v1\",\"encoder\":\"raw-protobuf\",\"name\":\"versioning\"} (02-Jan-2026 15:08:00.061) (total time: 4041ms):\nTrace[1455537276]: ---\"Network\" size:1003095731 3031ms\nTrace[1455537276]: ---\"Serialize\" count:10000 986ms\nTrace[1455537276]: [4.041542797s] [4.041542797s] END\n```"
  },
  {
    "name": "Continuously printing logs:Startup probe already exists for container",
    "description": "### What happened?\n\nWhen I upgrade my kubeadm kubectl kubelet to 1.35.0 and kube-apiserver and other images are upgraded to v1.35.0, kubelet will continuously print logs:\n\n```console\n$journalctl -f\nJan 02 17:14:16 host kubelet[23745]: E0102 17:14:16.863412   23745 prober_manager.go:197] \"Startup probe already exists for container\" pod=\"kube-system/etcd-guifanhost\" containerName=\"etcd\"\nJan 02 17:14:21 host kubelet[23745]: E0102 17:14:21.863299   23745 prober_manager.go:197] \"Startup probe already exists for container\" pod=\"kube-system/cilium-48g9s\" containerName=\"cilium-agent\"\nJan 02 17:14:25 host kubelet[23745]: E0102 17:14:25.863487   23745 prober_manager.go:209] \"Readiness probe already exists for container\" pod=\"kube-system/coredns-7d764666f9-8mx4j\" containerName=\"coredns\"\nJan 02 17:14:30 host kubelet[23745]: E0102 17:14:30.863774   23745 prober_manager.go:209] \"Readiness probe already exists for container\" pod=\"kube-system/cilium-operator-5986db688b-sgzdq\" containerName=\"cilium-operator\"\nJan 02 17:14:38 host kubelet[23745]: E0102 17:14:38.865968   23745 prober_manager.go:197] \"Startup probe already exists for container\" pod=\"kube-system/kube-apiserver-guifanhost\" containerName=\"kube-apiserver\"\nJan 02 17:14:44 host kubelet[23745]: E0102 17:14:44.863480   23745 prober_manager.go:209] \"Readiness probe already exists for container\" pod=\"kube-system/coredns-7d764666f9-4gc7h\" containerName=\"coredns\"\nJan 02 17:14:56 host kubelet[23745]: E0102 17:14:56.863559   23745 prober_manager.go:197] \"Startup probe already exists for container\" pod=\"kube-system/kube-controller-manager-guifanhost\" containerName=\"kube-controller-manager\"\n```\n\n### What did you expect to happen?\n\nkubelet will not continuously print these logs, at least not at the error level.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nInitialize Kubernetes using kubeadm version 1.35.0 in a Linux environment.\n\n### Anything else we need to know?\n\nmaybe relate to commit:\nchore(kubelet): migrate prober to contextual logging.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.35.0\nKustomize Version: v5.7.1\nServer Version: v1.35.0\n\n$kubeadm version\nkubeadm version: &version.Info{Major:\"1\", Minor:\"35\", EmulationMajor:\"\", EmulationMinor:\"\", MinCompatibilityMajor:\"\", MinCompatibilityMinor:\"\", GitVersion:\"v1.35.0\", GitCommit:\"66452049f3d692768c39c797b21b793dce80314e\", GitTreeState:\"archive\", BuildDate:\"2025-12-28T11:43:13Z\", GoVersion:\"go1.25.5 X:nodwarf5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n$registry.k8s.io/coredns/coredns                  v1.13.1             aa5e3ebc0dfed       23.6MB\nregistry.k8s.io/etcd                             3.6.6-0             0a108f7189562       23.6MB\nregistry.k8s.io/kube-apiserver                   v1.35.0             5c6acd67e9cd1       27.7MB\nregistry.k8s.io/kube-controller-manager          v1.35.0             2c9a4b058bd7e       23.1MB\nregistry.k8s.io/kube-proxy                       v1.35.0             32652ff1bbe6b       25.8MB\nregistry.k8s.io/kube-scheduler                   v1.35.0             550794e3b12ac       17.2MB\nregistry.k8s.io/pause                            3.10                873ed75102791       320kB\nregistry.k8s.io/pause                            3.10.1              cd073f4c5f6a8       320kBcrictl image ls\n\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nnone,only install local env.\n\n```console\nkubeadm init --config kubeadm.yaml\n```\n\n```yaml\napiVersion: kubeadm.k8s.io/v1beta4\nbootstrapTokens:\n- groups:\n  - system:bootstrappers:kubeadm:default-node-token\n  token: abcdef.0123456789abcdef\n  ttl: 24h0m0s\n  usages:\n  - signing\n  - authentication\nkind: InitConfiguration\nlocalAPIEndpoint:\n  advertiseAddress: 10.0.0.1\n  bindPort: 6443\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  imagePullPolicy: IfNotPresent\n  imagePullSerial: true\n  name: guifanhost\n  taints: null\nskipPhases:\n  [addon/kube-proxy]\ntimeouts:\n  controlPlaneComponentHealthCheck: 4m0s\n  discovery: 5m0s\n  etcdAPICall: 2m0s\n  kubeletHealthCheck: 4m0s\n  kubernetesAPICall: 1m0s\n  tlsBootstrap: 5m0s\n  upgradeManifests: 5m0s\n---\napiServer: {}\napiVersion: kubeadm.k8s.io/v1beta4\ncaCertificateValidityPeriod: 87600h0m0s\ncertificateValidityPeriod: 8760h0m0s\ncertificatesDir: /etc/kubernetes/pki\nclusterName: kubernetes\ncontrollerManager: {}\ndns: {}\nencryptionAlgorithm: RSA-2048\netcd:\n  local:\n    dataDir: /var/lib/etcd\nimageRepository: registry.k8s.io\nkind: ClusterConfiguration\nkubernetesVersion: 1.35.0\nnetworking:\n  dnsDomain: cluster.local\n  serviceSubnet: 10.96.0.0/12\n  podSubnet: \"10.1.0.0/16\"\ncontrolPlaneEndpoint: \"10.0.0.1:6443\"\nproxy: {}\nscheduler: {}\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nfailSwapOn: false\nfeatureGates:\n  NodeSwap: true\nmemorySwap:\n  swapBehavior: LimitedSwap\n```\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nNAME=\"Arch Linux\"\nPRETTY_NAME=\"Arch Linux\"\nID=arch\nBUILD_ID=rolling\nANSI_COLOR=\"38;2;23;147;209\"\nHOME_URL=\"https://archlinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.archlinux.org/groups/archlinux/-/issues\"\nPRIVACY_POLICY_URL=\"https://terms.archlinux.org/docs/privacy-policy/\"\nLOGO=archlinux-logo\n$ uname -a\nLinux guifanhost 6.18.2-arch2-1 #1 SMP PREEMPT_DYNAMIC Wed, 31 Dec 2025 07:12:41 +0000 x86_64 GNU/Linux\n\n\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nkubeadm\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\ncilium\n</details>"
  },
  {
    "name": "Add multiply/divide to the CEL resource library in kubernetes",
    "description": "### What would you like to be added?\n\nThere's currently no support for multiplication and division for the kubernetes `resource.Quantity` type in CEL. Ideally, the two operations should be added as `resource.quantity(10).mul(2)`, `resource.quantity(10).div(2)` and a `resource.quantity(10).div(3, 4)` for specifying decimal rounding\n\n### Why is this needed?\n\nCEL expressions used to generate kubernetes resources can benefit from referencing existing resources and perform such mathematical operations on existing resources to calculate another desired value from them"
  },
  {
    "name": "`kubectl apply` fails to parse YAML configuration files with `%YAML 1.1` directive",
    "description": "<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\n\nIf the matter is security related, please disclose it privately via https://kubernetes.io/security/\n-->\n\n## What happened\n\nI was cleaning up my configuration files, and decided to add the [`%YAML 1.1` directive](https://yaml.org/spec/1.1/current.html#id895631) to the top of all of them, so that I could get better syntax highlighting in Visual Studio Code (which defaults to YAML 1.2, not currently supported by `kubectl`).\n\nAfter adding the directive, `kubectl` stopped being able to parse the files, instead returning the following error, despite the file being valid, as reported by both `yq` and other online tools:\n```text\nerror: error parsing file.yml: error converting YAML to JSON: yaml: line 1: did not find expected <document start>\n```\n\n## What you expected to happen\n\nI expected my configuration files to be processed successfully, as they are valid YAML. \n\n## How to reproduce it\n<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.\n-->\n\nAfter some experimentation, I created the following minimal YAML file to reproduce the issue:\n```yml\n%YAML 1.1\n---\n```\n\nThat file is equivalent to the following YAML 1.1 file:\n```yml\nnull\n```\n\nThe former returns:\n```log\nerror: error parsing directive-test.yml: error converting YAML to JSON: yaml: line 1: did not find expected <document start>\n```\n\nWhile the latter correctly returns:\n```log\nerror: no objects passed to apply\n```\n\n## Anything else we need to know?\n\n### Environment\n\n[`kubectl`](https://archlinux.org/packages/extra/x86_64/kubectl/) installed via Arch Linux repositories.\n\nCluster set up via the [`kubeadm`](https://archlinux.org/packages/extra/x86_64/kubeadm/) provided by the Arch Linux repositories.\n\n#### `kubectl version`\n\n```log\nClient Version: v1.34.3\nKustomize Version: v5.7.1\nServer Version: v1.34.3\n```\n\n#### `cat /etc/os-release`\n\n```env\nNAME=\"Arch Linux\"\nPRETTY_NAME=\"Arch Linux\"\nID=arch\nBUILD_ID=rolling\nANSI_COLOR=\"38;2;23;147;209\"\nHOME_URL=\"https://archlinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.archlinux.org/groups/archlinux/-/issues\"\nPRIVACY_POLICY_URL=\"https://terms.archlinux.org/docs/privacy-policy/\"\nLOGO=archlinux-logo\n```"
  },
  {
    "name": "ImageVolume: Empty image.reference in Pod templates passes API validation but fails at runtime",
    "description": "### What happened?\n\nWhen creating a Deployment or StatefulSet with an ImageVolume that has an empty `image.reference` field, the API validation passes, but Pod creation fails at runtime. This creates a confusing user experience where workloads appear to be created successfully but never produce running Pods\n\n**Direct Pod creation correctly rejects empty reference:**\n\n```\n$ k apply -f pod-with-empty-ref.yaml\nThe Pod \"test\" is invalid: spec.volumes[0].image.reference: Required value\n```\n\n**But Deployment/StatefulSet incorrectly accepts it:**\n\n```\n$ k apply -f deployment-with-empty-ref.yaml\ndeployment.apps/test-imagevolume-empty-ref created  # Should have been rejected\n```\n\nThe error only appears in Events when the ReplicaSet controller tries to create Pods:\n\n```\nWarning  FailedCreate  replicaset-controller  Error creating: Pod \"...\" is invalid:\n  [spec.volumes[0].image.reference: Required value, ...]\n```\n\n### What did you expect to happen?\n\nAPI validation should reject empty `image.reference` in Pod templates (Deployment, StatefulSet, DaemonSet, Job, etc.), not just in direct Pod creation.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Create a kind cluster with Kubernetes v1.35+:\n\n```bash\n$ kind create cluster --name test\n```\n\n2. Apply this Deployment:\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-imagevolume-empty-ref\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n      - name: test\n        image: busybox:1.36\n        command: [\"sleep\", \"3600\"]\n        volumeMounts:\n        - name: img-vol\n          mountPath: /data\n      volumes:\n      - name: img-vol\n        image:\n          reference: \"\"  # Empty\n          pullPolicy: Always\n```\n\n3. Observe\n\n```bash\n$ k get deployment\nNAME                         READY   UP-TO-DATE   AVAILABLE\ntest-imagevolume-empty-ref   0/1     0            0           # Never becomes ready\n\n$ k describe rs -l app=test | grep -A5 Events\nEvents:\n  Warning  FailedCreate  Error creating: Pod \"...\" is invalid:\n    [spec.volumes[0].image.reference: Required value, ...]\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ k version\nClient Version: v1.34.1\nKustomize Version: v5.7.1\nServer Version: v1.35.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nkind (local)\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ uname -a\nDarwin Junyas-MacBook-Pro.local 25.3.0 Darwin Kernel Version 25.3.0: Fri Dec  5 23:13:24 PST 2025; root:xnu-12377.80.260.0.1~42/RELEASE_ARM64_T8142 arm64\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nkind version\nkind v0.31.0 go1.25.5 darwin/arm64\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Consider CSIStorageCapacity when scheduling pods with already-bound PVCs",
    "description": "Is it possible to extend `CSIStorageCapacity` tracking so that the scheduler also considers it when scheduling pods whose PVCs are already bound?\n\nWhen a node hosting a pod is drained (for example, during maintenance or upgrades), the pod needs to be rescheduled. Currently, the scheduler does not use `CSIStorageCapacity` information for pods with bound PVCs. For storage backends that support volume rebuild or migration, it would be useful if the scheduler could prefer nodes with sufficient available storage capacity, so that the new node can accommodate the original volume and keep the pod and volume co-located for better I/O performance. At the moment, this is not supported, and pods can occasionally be scheduled onto nodes where volumes cannot be rebuilt.\n\nDo you think this should be addressed on the Kubernetes side, or does it need to be handled by the storage backend? If on the Kubernetes side, this could potentially be implemented by adding a new field to StorageClass.\n\n/sig storage\n/sig scheduling"
  },
  {
    "name": "kube-aggregator fake clientset deprecated all constructors",
    "description": "### What happened?\n\nI'm using https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/fake/clientset_generated.go in my tests and I now get the following deprecation warning:\n```\npkg/server/plugin/notifier/k8sbundle/k8sbundle_test.go:906:12: SA1019: fakeaggregator.NewSimpleClientset is deprecated: NewClientset replaces this with support for field management, which significantly improves server side apply testing. NewClientset is only available when apply configurations are generated (e.g. via --with-applyconfig). (staticcheck)\n        client := fakeaggregator.NewSimpleClientset()\n```\n\n### What did you expect to happen?\n\nI'd have expected the package to provide a `NewClientset` function, as the deprecation warning specifies. No such functions is available.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nGrep for `NewClientset` in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kube-aggregator/pkg/client/clientset_generated/clientset/fake/clientset_generated.go.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nWas not a problem with k8s.io/kube-aggregator v0.34.2 but it's a problem with v0.35.0\n\n### Cloud provider\n\nN/A\n\n### OS version\n\nN/A\n\n### Install tools\n\nN/A\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Regression in kubeadm v1.31.14: conntrack dependency check failure on Kylin V10 SP3 (ARM64)",
    "description": "### What happened?\n\nA regression was observed in kubeadm version v1.31.14. During the kubeadm init process on Kylin V10 SP3 (ARM64), the preflight check fails because it cannot find conntrack in the system path.\n\nThis issue specifically occurs in v1.31.14. Previous patch versions in the 1.31 series (tested: v1.31.6, v1.31.10 through v1.31.13) work correctly with the same environment and dependencies (including conntrack-tools). Additionally, the v1.32.x series also performs as expected.\n\nThere is no mention of changes to conntrack requirements or preflight logic in the v1.31.14 release notes.\n\n[k8s.log](https://github.com/user-attachments/files/24380496/k8s.log)\n\n### What did you expect to happen?\n\nExpected Behavior: The kubeadm preflight check should correctly recognize the conntrack binary provided by the conntrack-tools package. The initialization process (kubeadm init) should proceed successfully without encountering the [ERROR FileExisting-conntrack] error, maintaining consistency with previous v1.31.x patch releases.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nConfigure the official Kubernetes YUM repository.\n\nInstall the specific versions using the following command: yum install kubelet-1.31.14 kubeadm-1.31.14 kubectl-1.31.14\n\nRun kubeadm init and observe the preflight check error.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n1.31.14\n```\n\n</details>\n\n\n### Cloud provider\n\nnone\n\n\n### OS version\n\n<details>\n\n```console\n[root@lll ~]# cat /etc/os-release\nNAME=\"Kylin Linux Advanced Server\"\nVERSION=\"V10 (Lance)\"\nID=\"kylin\"\nVERSION_ID=\"V10\"\nPRETTY_NAME=\"Kylin Linux Advanced Server V10 (Lance)\"\nANSI_COLOR=\"0;31\"\n\n[root@ops-test-k8s-fcc3 ~]# cat /etc/kylin-release\nKylin Linux Advanced Server release V10 (Lance)\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Add net.ipv4.tcp_slow_start_after_idle and net.ipv4.tcp_notsent_lowat to the safe sysctl list",
    "description": "### What would you like to be added?\n\nI would like to propose adding net.ipv4.tcp_slow_start_after_idle and net.ipv4.tcp_notsent_lowat to the list of \"safe\" sysctls in Kubernetes.\n\nBoth of these sysctls are namespaced in the Linux kernel:\n\ntcp_slow_start_after_idle has been namespaced since Linux 3.11.\n\ntcp_notsent_lowat has been namespaced since Linux 3.12.\n\n\n### Why is this needed?\n\nAs with the recently accepted change for TCP read/write buffers https://github.com/kubernetes/kubernetes/issues/125234, modern high-performance applications require fine-grained control over the networking stack that cannot always be applied globally at the node level.\n\n1. net.ipv4.tcp_slow_start_after_idle for long-lived connections that have idle periods (like database pools or persistent API streams), the default behavior is to reset the congestion window after an idle period. Disabling this (0) is important for applications that need to maintain high throughput immediately upon resuming transmission without waiting for a slow-start ramp-up.\n\n2. net.ipv4.tcp_notsent_lowat This is important for HTTP/2 and gRPC services. It limits the amount of unsent data in the write queue, which significantly reduces \"bufferbloat\" and improves responsiveness/latency for real-time streams. Setting this per-pod allows latency-critical microservices to coexist on the same node as throughput-oriented bulk transfer services.\n\nAllowing these as safe sysctls enables developers to optimize their application's networking behavior via the Pod securityContext without requiring administrative intervention to modify Kubelet configuration or host-level sysctls.\n\nNote: Text was formatted and edited with Google Gemini."
  },
  {
    "name": "EndpointSlice API returns `endpoints: null` even though OpenAPI schema defines `endpoints` as required array",
    "description": "### What happened?\n\nWhen a Service has zero backing Pods (e.g., Deployment scaled to 0 or no Pods ever matched the selector), the EndpointSlice object returned by the API server contains:\n```\n{\n  \"kind\": \"EndpointSliceList\",\n  \"apiVersion\": \"discovery.k8s.io/v1\",\n  \"metadata\": {\n    \"resourceVersion\": \"828044619\"\n  },\n  \"items\": [\n    {\n      \"metadata\": { ... },\n      \"addressType\": \"IPv4\",\n      \"endpoints\": null,  // null here\n      \"ports\": null\n    }\n  ]\n}\n```\nThis breaks clients / validators that rely on the published OpenAPI schema where `endpoints` is treated as a required field and modeled as an array (not nullable).\n\n### What did you expect to happen?\n\nAccording to the API Spec, `endpoints` should be a JSON array. For the \u201cno endpoints\u201d case, it should be an empty array.\n```\n\"endpoints\": []\n```\nRef.: https://github.com/kubernetes/kubernetes/blob/master/api/openapi-spec/v3/apis__discovery.k8s.io__v1_openapi.json#L228-L231\n\n\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n```\nkubectl scale rs/replicaset.apps/demo-5f6c9cb999 --replicas=0\n```\n\n### Anything else we need to know?\n\n- The API type defines `Endpoints []Endpoint` without `omitempty`, so it\u2019s expected to be present and modeled as a list in generated schema/docs: https://pkg.go.dev/k8s.io/api/discovery/v1#EndpointSlice\n- A related client breakage report (Java client) describing the same behavior: https://github.com/kubernetes-client/java/issues/3226 \n\n### Kubernetes version\n\nAWS EKS v1.32\n```\nClient Version: v1.34.1\nKustomize Version: v5.7.1\nServer Version: v1.32.9-eks-3025e55\nWarning: version difference between client (1.34) and server (1.32) exceeds the supported minor version skew of +/-1\n```\n\n### Cloud provider\n\nAWS EKS\n\n### OS version\n\n_No response_\n\n### Install tools\n\n_No response_\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_"
  },
  {
    "name": "app controller unit tests: avoid arbitrary informerSyncTimeout with synctest",
    "description": "### What would you like to be added?\n\nIt was found that tests which make a change and then wait for some period of time for controllers to react to those changes start to flake when cranking up the load on the system. This was first observed in https://github.com/kubernetes/kubernetes/pull/135395 and can be reproduced locally with:\n```console\n$ go test -c ./pkg/controller/replicaset\n$GOPATH/bin/stress -p 512 ./replicaset.test -test.run=TestExpectationsOnRecreate\n```\n\nNote that https://github.com/kubernetes/kubernetes/pull/135959 may be needed to avoid another race.\n\nIncreasing the timeout is a stop-gap for reducing flakiness, but a better solution is to convert to a synctest without `Wait` as replacement for polling (see patch below).\n\nHowever, this currently fails with that patch because app controllers to not support waiting for all spawned goroutines. Exiting a test with running goroutines (even if they will stop later) is a hard failure in a synctest bubble.\n\nThis must be added before the synctest approach is viable.\n\nAlso useful once https://github.com/kubernetes/kubernetes/pull/135395 is merged:\n- WaitForCacheSyncWithContext\n- StartWithContext\n\n```patch\ndiff --git a/pkg/controller/replicaset/replica_set_test.go b/pkg/controller/replicaset/replica_set_test.go\nindex d221366c293..b798771d247 100644\n--- a/pkg/controller/replicaset/replica_set_test.go\n+++ b/pkg/controller/replicaset/replica_set_test.go\n@@ -20,7 +20,6 @@ import (\n \t\"context\"\n \t\"errors\"\n \t\"fmt\"\n-\t\"github.com/onsi/gomega\"\n \t\"math/rand\"\n \t\"net/http/httptest\"\n \t\"net/url\"\n@@ -29,8 +28,11 @@ import (\n \t\"strings\"\n \t\"sync\"\n \t\"testing\"\n+\t\"testing/synctest\"\n \t\"time\"\n \n+\t\"github.com/onsi/gomega\"\n+\n \tapps \"k8s.io/api/apps/v1\"\n \tv1 \"k8s.io/api/core/v1\"\n \tapiequality \"k8s.io/apimachinery/pkg/api/equality\"\n@@ -1196,13 +1198,12 @@ func TestDeleteControllerAndExpectations(t *testing.T) {\n }\n \n func TestExpectationsOnRecreate(t *testing.T) {\n+\tktesting.Init(t).SyncTest(\"\", testExpectationsOnRecreate)\n+}\n+func testExpectationsOnRecreate(tCtx ktesting.TContext) {\n \tclient := fake.NewSimpleClientset()\n-\tstopCh := make(chan struct{})\n-\tdefer close(stopCh)\n \n \tf := informers.NewSharedInformerFactory(client, controller.NoResyncPeriodFunc())\n-\ttCtx := ktesting.Init(t)\n-\tlogger := tCtx.Logger()\n \tmanager := NewReplicaSetController(\n \t\ttCtx,\n \t\tf.Apps().V1().ReplicaSets(),\n@@ -1210,146 +1211,140 @@ func TestExpectationsOnRecreate(t *testing.T) {\n \t\tclient,\n \t\t100,\n \t)\n-\tf.Start(stopCh)\n-\tf.WaitForCacheSync(stopCh)\n+\tf.Start(tCtx.Done())\n+\tf.WaitForCacheSync(tCtx.Done())\n \tfakePodControl := controller.FakePodControl{}\n \tmanager.podControl = &fakePodControl\n \n \tif manager.queue.Len() != 0 {\n-\t\tt.Fatal(\"Unexpected item in the queue\")\n+\t\ttCtx.Fatal(\"Unexpected item in the queue\")\n \t}\n \n \toldRS := newReplicaSet(1, map[string]string{\"foo\": \"bar\"})\n \toldRS, err := client.AppsV1().ReplicaSets(oldRS.Namespace).Create(tCtx, oldRS, metav1.CreateOptions{})\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \n-\terr = wait.PollImmediate(100*time.Millisecond, informerSyncTimeout, func() (bool, error) {\n-\t\tlogger.V(8).Info(\"Waiting for queue to have 1 item\", \"length\", manager.queue.Len())\n-\t\treturn manager.queue.Len() == 1, nil\n-\t})\n-\tif err != nil {\n-\t\tt.Fatalf(\"initial RS didn't result in new item in the queue: %v\", err)\n+\t// Wait for event delivery.\n+\tsynctest.Wait()\n+\tif manager.queue.Len() != 1 {\n+\t\ttCtx.Fatalf(\"initial RS didn't result in new item in the queue: want len 1, got %d\", manager.queue.Len())\n \t}\n \n \tok := manager.processNextWorkItem(tCtx)\n \tif !ok {\n-\t\tt.Fatal(\"queue is shutting down\")\n+\t\ttCtx.Fatal(\"queue is shutting down\")\n \t}\n \n \tif utilfeature.DefaultFeatureGate.Enabled(features.DeploymentReplicaSetTerminatingReplicas) {\n \t\t// DeploymentReplicaSetTerminatingReplicas feature results in the \"terminatingReplicas nil->0\" update, so we need to do empty sync.\n \t\tok = manager.processNextWorkItem(tCtx)\n \t\tif !ok {\n-\t\t\tt.Fatal(\"queue is shutting down\")\n+\t\t\ttCtx.Fatal(\"queue is shutting down\")\n \t\t}\n \t}\n \n \terr = validateSyncReplicaSet(&fakePodControl, 1, 0, 0)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \tfakePodControl.Clear()\n \n \toldRSKey, err := controller.KeyFunc(oldRS)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \n \trsExp, exists, err := manager.expectations.GetExpectations(oldRSKey)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \tif !exists {\n-\t\tt.Errorf(\"No expectations found for ReplicaSet %q\", oldRSKey)\n+\t\ttCtx.Errorf(\"No expectations found for ReplicaSet %q\", oldRSKey)\n \t}\n \tif rsExp.Fulfilled() {\n-\t\tt.Errorf(\"There should be unfulfilled expectations for creating new pods for ReplicaSet %q\", oldRSKey)\n+\t\ttCtx.Errorf(\"There should be unfulfilled expectations for creating new pods for ReplicaSet %q\", oldRSKey)\n \t}\n \n \tif manager.queue.Len() != 0 {\n-\t\tt.Fatal(\"Unexpected item in the queue\")\n+\t\ttCtx.Fatal(\"Unexpected item in the queue\")\n \t}\n \n \terr = client.AppsV1().ReplicaSets(oldRS.Namespace).Delete(tCtx, oldRS.Name, metav1.DeleteOptions{})\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \n-\terr = wait.PollImmediate(100*time.Millisecond, informerSyncTimeout, func() (bool, error) {\n-\t\tlogger.V(8).Info(\"Waiting for queue to have 1 item\", \"length\", manager.queue.Len())\n-\t\treturn manager.queue.Len() == 1, nil\n-\t})\n-\tif err != nil {\n-\t\tt.Fatalf(\"Deleting RS didn't result in new item in the queue: %v\", err)\n+\t// Wait again for processing.\n+\tsynctest.Wait()\n+\tif manager.queue.Len() != 1 {\n+\t\ttCtx.Fatalf(\"Deleting RS didn't result in new item in the queue: want len 1, got %d\", manager.queue.Len())\n \t}\n \n \t_, exists, err = manager.expectations.GetExpectations(oldRSKey)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \tif exists {\n-\t\tt.Errorf(\"There should be no expectations for ReplicaSet %q after it was deleted\", oldRSKey)\n+\t\ttCtx.Errorf(\"There should be no expectations for ReplicaSet %q after it was deleted\", oldRSKey)\n \t}\n \n \t// skip sync for the delete event so we only see the new RS in sync\n \tkey, quit := manager.queue.Get()\n \tif quit {\n-\t\tt.Fatal(\"Queue is shutting down!\")\n+\t\ttCtx.Fatal(\"Queue is shutting down!\")\n \t}\n \tmanager.queue.Done(key)\n \tif key != oldRSKey {\n-\t\tt.Fatal(\"Keys should be equal!\")\n+\t\ttCtx.Fatal(\"Keys should be equal!\")\n \t}\n \n \tif manager.queue.Len() != 0 {\n-\t\tt.Fatal(\"Unexpected item in the queue\")\n+\t\ttCtx.Fatal(\"Unexpected item in the queue\")\n \t}\n \n \tnewRS := oldRS.DeepCopy()\n \tnewRS.UID = uuid.NewUUID()\n \tnewRS, err = client.AppsV1().ReplicaSets(newRS.Namespace).Create(tCtx, newRS, metav1.CreateOptions{})\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \n \t// Sanity check\n \tif newRS.UID == oldRS.UID {\n-\t\tt.Fatal(\"New RS has the same UID as the old one!\")\n+\t\ttCtx.Fatal(\"New RS has the same UID as the old one!\")\n \t}\n \n-\terr = wait.PollImmediate(100*time.Millisecond, informerSyncTimeout, func() (bool, error) {\n-\t\tlogger.V(8).Info(\"Waiting for queue to have 1 item\", \"length\", manager.queue.Len())\n-\t\treturn manager.queue.Len() == 1, nil\n-\t})\n-\tif err != nil {\n-\t\tt.Fatalf(\"Re-creating RS didn't result in new item in the queue: %v\", err)\n+\t// Wait again for processing.\n+\tsynctest.Wait()\n+\tif manager.queue.Len() != 1 {\n+\t\ttCtx.Fatalf(\"Re-creating RS didn't result in new item in the queue: want len 1, got %d\", manager.queue.Len())\n \t}\n \n \tok = manager.processNextWorkItem(tCtx)\n \tif !ok {\n-\t\tt.Fatal(\"Queue is shutting down!\")\n+\t\ttCtx.Fatal(\"Queue is shutting down!\")\n \t}\n \n \tnewRSKey, err := controller.KeyFunc(newRS)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \trsExp, exists, err = manager.expectations.GetExpectations(newRSKey)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \tif !exists {\n-\t\tt.Errorf(\"No expectations found for ReplicaSet %q\", newRSKey)\n+\t\ttCtx.Errorf(\"No expectations found for ReplicaSet %q\", newRSKey)\n \t}\n \tif rsExp.Fulfilled() {\n-\t\tt.Errorf(\"There should be unfulfilled expectations for creating new pods for ReplicaSet %q\", newRSKey)\n+\t\ttCtx.Errorf(\"There should be unfulfilled expectations for creating new pods for ReplicaSet %q\", newRSKey)\n \t}\n \n \terr = validateSyncReplicaSet(&fakePodControl, 1, 0, 0)\n \tif err != nil {\n-\t\tt.Fatal(err)\n+\t\ttCtx.Fatal(err)\n \t}\n \tfakePodControl.Clear()\n }\n```\n\n/sig apps\n\n### Why is this needed?\n\nMore reliable testing."
  },
  {
    "name": "[Flake] test/integration/servicecidr: TestServiceAllocation fails with \"range is full\"",
    "description": "### Which jobs are flaking?\n\nci-kubernetes-integration-master\n\n### Which tests are flaking?\n\nk8s.io/kubernetes/test/integration/servicecidr TestServiceAllocation/IP_allocator_only\n\n### Since when has it been flaking?\n\nObserved on Dec 28, 2025\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#integration-master\n\n### Reason for failure (if possible)\n\nThe test fails with an IP exhaustion error: \"failed to allocate a serviceIP: range is full\".\n\n### Anything else we need to know?\n\nI attempted to reproduce this locally on macOS M1 using `stress` for ~20 minutes but could not reproduce it, suggesting it is a race condition dependent on the higher concurrency or specific environment of the CI runner.\n\n### Relevant SIG(s)\n\n/sig network"
  },
  {
    "name": "Restructure cacher package to reduce test duration and improve modularity",
    "description": "### What would you like to be added?\n\nI propose restructuring the cacher package to improve modularity and reduce test execution time.\n\nThe restructuring should be done in two phases:\n\n1. Extract Testing Infrastructure Move shared test helpers into a new package, e.g., pkg/storage/cacher/testing. This package should include: dummy, testSetup and their helpers https://github.com/kubernetes/kubernetes/pull/135961\n2. Extract Subpackages Once the testing infrastructure is independent, move the following components into their own subpackages. Initial ideas:\n    * delegator\n    * ready\n    * timebudget\n    * watchcache\n    * store https://github.com/kubernetes/kubernetes/pull/135958 \n\nThe main cacher package should focus on Cacher struct and it's related functions \n\n### Why is this needed?\n\nThe cacher package tests are running for over 2.5 minutes, and on multiple cases adding any new test caused it hit to the 3-minute timeout limit for packages ad. This high duration makes the tests flaky and prone to timeouts in CI environments.\n\nThe cacher package has become monolithic, containing several distinct components (delegator, \nready, time_budget, watch_cache) that are tightly coupled. \n\nWithout direct effort to split the package we cannot easily extract these components because of high coupling of components and testing depending on shared utilities. \n\nEDIT: Also unblock https://github.com/kubernetes/kubernetes/pull/131862\n\n/cc @liggitt @wojtek-t"
  },
  {
    "name": "fake client-go: support ResourceVersion in Watch",
    "description": "### What would you like to be added?\n\nfake.Clientset suffers from a race condition related to informers: it does not implement resource version support in its Watch implementation and instead assumes that watches are set up before further resource changes are made.\n\nIf a test waits for caches to be synced and then immediately adds an object, that new object will never be seen by event handlers if the race goes wrong and the Watch call hadn't completed yet (can be triggered by adding a sleep before https://github.com/kubernetes/kubernetes/blob/b53b9fb5573323484af9a19cf3f5bfe80760abba/staging/src/k8s.io/client-go/tools/cache/reflector.go#L431).\n\nExample test:\n\nhttps://github.com/kubernetes/kubernetes/blob/3c2e339be8a7dbed3f3918c8a33edf41726f22f9/pkg/controller/replicaset/replica_set_test.go#L1214-L1234\n\nThis is becoming more of a problem in https://github.com/kubernetes/kubernetes/pull/135395 because it makes `informerFactory.WaitForCacheSync` return more quickly, thus increasing the risk that the race goes wrong.\n\nThere are two workarounds:\n\n- Explicitly wait for watches:\n\n```Go\n\tvar numWatches atomic.Int32\n\tclientSet.PrependWatchReactor(\"*\", func(action core.Action) (handled bool, ret watch.Interface, err error) {\n\t\tvar opts metav1.ListOptions\n\t\tif watchActcion, ok := action.(core.WatchActionImpl); ok {\n\t\t\topts = watchActcion.ListOptions\n\t\t}\n\t\tgvr := action.GetResource()\n\t\tns := action.GetNamespace()\n\t\twatch, err := clientSet.Tracker().Watch(gvr, ns, opts)\n\t\tif err != nil {\n\t\t\treturn false, nil, err\n\t\t}\n\t\tnumWatches.Add(1)\n\t\treturn true, watch, nil\n\t})\n\n       ktesting.Eventually(tCtx, func(tCtx ktesting.TContext) int32 {\n\t\treturn numWatches.Load()\n\t}).WithTimeout(5*time.Second).Should(gomega.Equal(int32(5)), \"All watches should be registered.\")\n```\n\n- Convert the test to run in a synctest bubble (`tCtx.SyncTest` when using ktesting, `synctest.Test` in plain Go), then add `tCtx.Wait` or `synctest.Wait` after or instead of  `informerFactory.WaitForCacheSync`.\n\nA better solution would be to make fake client-go behave properly.\n\n/sig api-machinery\n\n### Why is this needed?\n\nAvoid test flakes."
  },
  {
    "name": "[Flaky test] TestPriorityAndFairnessWithPanicRecoveryAndTimeoutFilter flakes on APF header assertions",
    "description": "### Which jobs are flaking?\n\n`k8s.io/kubernetes/pkg/apiserver/pkg/server/filters` unit tests. Observed in:\n\n- ci-kubernetes-unit-ppc64le\n- ci-kubernetes-unit-1-34\n- ci-kubernetes-unit-1-32\n\n### Which tests are flaking?\n\n`TestPriorityAndFairnessWithPanicRecoveryAndTimeoutFilter` Specifically sub-tests involving timeout scenarios:\n\n- `priority_level_concurrency_is_set_to_1,_request_times_out_and_inner_handler_hasn't_written_to_the_response_yet`\n- `priority_level_concurrency_is_set_to_1,_inner_handler_panics_after_the_request_times_out`\n\n### Since when has it been flaking?\n\nIntermittently observed\n\n### Testgrid link\n\nhttps://storage.googleapis.com/k8s-triage/index.html?text=TestPriorityAndFairnessWithPanicRecoveryAndTimeoutFilter&test=k8s.io%2Fapiserver%2Fpkg%2Fserver%2Ffilters\n\n### Reason for failure (if possible)\n\nThere is an inherent race condition in the test setup between the TimeoutHandler and the APF (Priority and Fairness) tracking logic.\n\nThe test expects APF headers (like X-Kubernetes-PF-FlowSchema-UID) to be present on all responses. However, when a request times out (Status 504), the outer TimeoutHandler takes over, writes the response, and closes the stream/context.\n\nIf the TimeoutHandler finalizes the response before the inner APF handler has a chance to write its tracking headers, the headers will be missing from the response. The test currently fails an assertion when these headers are missing.\n\nSample Failure Log:\n\n`priority-and-fairness_test.go:788: Expected APF headers to match, but got: expected HTTP header X-Kubernetes-PF-FlowSchema-UID to have value \"test-fs\", but got: \"\"`\n\n### Anything else we need to know?\n\nI have successfully reproduced this flake locally on x86 environments. I have a fix prepared which updates the test helper (`headerMatcher.inspect`) to skip APF header assertions if the request context is already cancelled or timed out.\n\n### Relevant SIG(s)\n\n/sig api-machinery"
  },
  {
    "name": "kube-apiserver accesses the aggregation server, and the TLS cipher suite includes insecure protocols.",
    "description": "kube-apiserver accesses the aggregation server, using the default TLS cipher suite in Go, which includes insecure protocols.This requires the server to ensure that a secure cipher suite is used."
  },
  {
    "name": "globalmount path may be residual while pod creation-deletion",
    "description": "### What happened?\n\n1. created a pod A\n2. pod A starting `mountDevice` , but at this time, the csi-plugin pod was upgrading, kubelet received `rpc error: code = Unavailable error reading from server: EOF`, marked the volume device  `uncertain`, actually the globalmount was mounted.\nhttps://github.com/kubernetes/kubernetes/blob/15673d04e30c711a7bb0f0efe6abf4baead1463b/pkg/volume/util/operationexecutor/operation_generator.go#L690-L694\nhttps://github.com/kubernetes/kubernetes/blob/15673d04e30c711a7bb0f0efe6abf4baead1463b/pkg/volume/csi/csi_client.go#L449-L451\nhttps://github.com/kubernetes/kubernetes/blob/15673d04e30c711a7bb0f0efe6abf4baead1463b/pkg/volume/csi/csi_client.go#L711-L728\n3. kubelet retried to mountDevice for pod A, but at this time. the csi-plugin pod was terminated and had not started again, the kubelet received `rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix xxx/csi.sock: connect: connection refused`, it was not a `UncertainProgressError` and then marked the volume device  `unmounted`. Actually it failed at `csi.NodeSupportsStageUnstage(ctx)` this time, hadn't called `csi.NodeStageVolume` yet.\nhttps://github.com/kubernetes/kubernetes/blob/15673d04e30c711a7bb0f0efe6abf4baead1463b/pkg/volume/util/operationexecutor/operation_generator.go#L674-L683\nhttps://github.com/kubernetes/kubernetes/blob/15673d04e30c711a7bb0f0efe6abf4baead1463b/pkg/volume/csi/csi_client.go#L704-L706\n4. pod A was deleted, but the volume device in asw was unmounted, kubelet didn't call umountDevice, then the globalmount was residual.\n\n\nI think that while a volume was uncertain, kubelet retried to mount it again, althrough kubelet may received a not `UncertainProgressError`, we should not change it's status to unmounted, it should be always uncertain until called unmountDevice successfully\n\n### What did you expect to happen?\n\nglobalmount can be unmounted under any circumstances\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nabove\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Nil pointer panic in getEventKey when Event.Source is nil in event broadcaster",
    "description": "### What happened?\n\nThe OLM operator crashed with a nil pointer dereference panic in client-go/tools/record/events_cache.go:53 within the getEventKey function. The panic occurred when strings.Join was called with a slice containing nil string values, likely due to `event` being nil.\n```go\n2025-12-17T02:12:17.881293848Z E1217 02:12:17.881155 \u00a0 \u00a0 \u00a0 1 panic.go:262] \"Observed a panic\" panic=\"runtime error: invalid memory address or nil pointer dereference\" panicGoValue=\"\\\"invalid memory address or nil pointer dereference\\\"\" stacktrace=<\n2025-12-17T02:12:17.881293848Z\u00a0 goroutine 120 [running]:\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/apimachinery/pkg/util/runtime.logPanic({0x337f8f8, 0xc005bccde0}, {0x29a5ca0, 0x4d76d40})\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:132 +0xbc\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x337f9d8, 0x4e90e40}, {0x29a5ca0, 0x4d76d40}, {0x4e90e40, 0x0, 0x442fb8?})\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0x116\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc000bf3500?})\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:64 +0x105\n2025-12-17T02:12:17.881293848Z\u00a0 panic({0x29a5ca0?, 0x4d76d40?})\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /usr/lib/golang/src/runtime/panic.go:792 +0x132\n2025-12-17T02:12:17.881293848Z\u00a0 strings.(*Builder).WriteString(...)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /usr/lib/golang/src/strings/builder.go:108\n2025-12-17T02:12:17.881293848Z\u00a0 strings.Join({0xc001862b08?, 0x127e2?, 0xb}, {0x0, 0x0})\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /usr/lib/golang/src/strings/strings.go:510 +0x22a\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.getEventKey(0xc001083340?)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/events_cache.go:53 +0x175\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.(*EventAggregator).EventAggregate(0xc0004d7720, 0xc000c78008)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/events_cache.go:242 +0x7e\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.(*EventCorrelator).EventCorrelate(0xc00035c420, 0x10137f901?)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/events_cache.go:510 +0x29\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.(*eventBroadcasterImpl).recordToSink(0xc000bfc300, {0x336e5e0, 0xc000c33710}, 0xc000caea08, 0xc00035c420)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/event.go:293 +0xc5\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartRecordingToSink.func1(0xc001862fa8?)\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/event.go:279 +0x29\n2025-12-17T02:12:17.881293848Z\u00a0 k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher.func1()\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/event.go:418 +0x126\n2025-12-17T02:12:17.881293848Z\u00a0 created by k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher in goroutine 1\n2025-12-17T02:12:17.881293848Z\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /build/vendor/k8s.io/client-go/tools/record/event.go:404 +0xb2\n2025-12-17T02:12:17.881293848Z\u00a0 >\n2025-12-17T02:12:17.884482085Z panic: runtime error: invalid memory address or nil pointer dereference [recovered]\n2025-12-17T02:12:17.884482085Z\u00a0 panic: runtime error: invalid memory address or nil pointer dereference\n2025-12-17T02:12:17.884482085Z [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x485365]\n2025-12-17T02:12:17.884482085Z \n```\n\n### What did you expect to happen?\n\nThe getEventKey function should handle nil fields gracefully instead of panicking. It should either:\n  1. Check for nil before accessing nested fields and use empty strings as defaults\n  2. Return an error that can be handled by the caller\n  3. Skip events with incomplete data\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n  This occurred in an OpenShift environment during operator installation/reconciliation. The logs before the panic show:\n  - Multiple \"resource name may not be empty\" errors from the OperatorCondition controller\n  - Conflict errors when updating Operator status\n  - Rapid CSV reconciliation for servicemeshoperator3.v3.2.0 and kiali-operator.v2.17.2\n\n  The panic happened in the event broadcaster goroutine, which runs independently from the main reconciliation loops.\n\n### Anything else we need to know?\n\n  The getEventKey function in events_cache.go constructs an event key by joining multiple fields:\n```go\n  The `getEventKey` function in `events_cache.go` constructs an event key by joining multiple fields:\n\n  func getEventKey(event *v1.Event) string {\n      return strings.Join([]string{\n          event.Source.Component,      // panics if event is nil\n          event.Source.Host,\n          // ...\n      }, \"\")\n  }\n```\n  If `event` (which is a pointer `*v1.Event`) is nil, accessing `event.Source` causes\n  a nil pointer dereference. The function lacks a nil check for the `event` parameter\n  before accessing its fields.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n1.34\n\n### Cloud provider\n\n<details>\n\n</details>\nazure\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "scheduler integration TestPreFilterPlugin: DATA RACE",
    "description": "### What happened?\n\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/135395/pull-kubernetes-integration-race/2003404146585112576:\n\n```\nWARNING: DATA RACE\nRead at 0x00c001f6ad50 by goroutine 7748:\n  k8s.io/kubernetes/test/integration/scheduler/plugins.TestPreFilterPlugin.func1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/plugins/plugins_test.go:768 +0xaa9\n  testing.tRunner()\n      /usr/local/go/src/testing/testing.go:1934 +0x21c\n  testing.(*T).Run.gowrap1()\n      /usr/local/go/src/testing/testing.go:1997 +0x44\n\nPrevious write at 0x00c001f6ad50 by goroutine 8130:\n  k8s.io/kubernetes/test/integration/scheduler/plugins.(*PreFilterPlugin).PreFilter()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/plugins/plugins_test.go:585 +0x59\n  k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*instrumentedPreFilterPlugin).PreFilter()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/framework/runtime/instrumented_plugins.go:49 +0xb5\n  k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).runPreFilterPlugin()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:927 +0x473\n  k8s.io/kubernetes/pkg/scheduler/framework/runtime.(*frameworkImpl).RunPreFilterPlugins()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/framework/runtime/framework.go:887 +0x7a4\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatFitPod()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:498 +0x2e8\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:438 +0x575\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod-fm()\n      <autogenerated>:1 +0xd0\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulingCycle()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:151 +0x171\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:116 +0x9b9\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x98\n  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed\n  k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108\n  k8s.io/apimachinery/pkg/util/wait.UntilWithContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x59\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:552 +0x17\n\nGoroutine 7748 (running) created at:\n  testing.(*T).Run()\n      /usr/local/go/src/testing/testing.go:1997 +0x9d2\n  k8s.io/kubernetes/test/integration/scheduler/plugins.TestPreFilterPlugin()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/plugins/plugins_test.go:733 +0x519\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:149 +0xc44\n  k8s.io/kubernetes/pkg/registry/resource/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/resource/rest/storage_resource.go:82 +0x150\n  k8s.io/kubernetes/pkg/registry/resource/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/resource/rest/storage_resource.go:51 +0x210\n  k8s.io/kubernetes/pkg/registry/resource/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/admissionregistration/rest/storage_apiserver.go:83 +0x1e6\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/admissionregistration/rest/storage_apiserver.go:50 +0x23a\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xfb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:68 +0x3e7\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:57 +0x146\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:43 +0x1da\n  k8s.io/kubernetes/pkg/registry/apps/rest.(*StorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/flowcontrol/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:98 +0x4fc\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:78 +0x85\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:74 +0x5e\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:49 +0x2c8\n  k8s.io/kubernetes/pkg/registry/storage/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/rbac/rest.RESTStorageProvider.storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/rbac/rest/storage_rbac.go:84 +0x78\n  k8s.io/kubernetes/pkg/registry/rbac/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/rbac/rest/storage_rbac.go:72 +0x244\n  k8s.io/kubernetes/pkg/registry/rbac/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:71 +0x2bc\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:62 +0x146\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:48 +0x2c8\n  k8s.io/kubernetes/pkg/registry/networking/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/batch/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallLegacyAPIGroup()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:858 +0x1da\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:140 +0xab5\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:230 +0x2d84\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:205 +0x584\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:194 +0x407\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:189 +0x30d\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:184 +0x21b\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:179 +0x1ed\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:175 +0x1b6\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:170 +0x190\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:165 +0x16a\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:109 +0xa4b\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:104 +0xa25\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:99 +0x9f8\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:95 +0x9ca\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:90 +0x9a4\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:155 +0xc4\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane.CompletedConfig.New()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/instance.go:341 +0x25a\n  k8s.io/kubernetes/pkg/controlplane.CompletedConfig.New()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/instance.go:336 +0x204\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:220 +0x2fc4\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:220 +0x2faf\n  k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:376 +0x1731\n  k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()\n      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:225 +0x2e4\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:212 +0x2eb4\n  k8s.io/kubernetes/test/integration/util.InitTestAPIServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/util/util.go:511 +0x209\n  k8s.io/kubernetes/test/integration/scheduler/plugins.TestPreFilterPlugin()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/plugins/plugins_test.go:695 +0x4e\n  testing.tRunner()\n      /usr/local/go/src/testing/testing.go:1934 +0x21c\n  testing.(*T).Run.gowrap1()\n      /usr/local/go/src/testing/testing.go:1997 +0x44\n\nGoroutine 8130 (running) created at:\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:552 +0x21e\n  k8s.io/kubernetes/test/integration/scheduler.InitTestSchedulerForFrameworkTest.gowrap1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/util.go:52 +0x4f\n```\n\n### What did you expect to happen?\n\nNo data race.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n`go test -race`\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Before mount the volume, kubelet cannot determine whether CSI is registered.",
    "description": "### What happened?\n\nDuring the process of attaching volumes by kubelet, if the CSI plugin is not registered, it will only be known whether the CSI plugin is registered when the kubelet actually calls the CSI's volume mount method (SetUp). If the CSI plugin is not registered and there are too many volumes to mount, this can cause the kubelet CPU usage to spike in a short period of time.\nShould we check whether the CSI plugin is registered before mounting the volume? This could reduce unnecessary coroutine creation and lower CPU usage.\nThe CSI plugin is only checked for existence when the SetUpAt method is called.\nhttps://github.com/kubernetes/kubernetes/blob/2a3a6605ac9b184711ef212ecbe26c8af4a8bac8/pkg/volume/csi/csi_mounter.go#L102-L110\n### What did you expect to happen?\n\nAfter kubelet knows whether CSI is registered, it proceeds with volume mounting.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nA node has multiple pods, and there are a large number of volumes that need to be mounted by CSI. However, the CSI plugin is not registered.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.31\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Kubelet SyncLoop Remove Error",
    "description": "### What happened?\n\nkubelet remove pod immediately, but containerd kill container after grace period, this make oom when node resource full.\n\nkubelet logs:\n<img width=\"2808\" height=\"670\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/5617c3d6-bf4b-48e3-b9bc-6e199eaea7c2\" />\n\ncontainerd logs:\n<img width=\"2810\" height=\"1346\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/13ca0076-8541-4ec8-811a-c15a22518541\" />\n\n<img width=\"2818\" height=\"1360\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d2247e41-9b48-462e-821a-7e3792d487da\" />\n\n### What did you expect to happen?\n\nkubelet remove pod after containerd kill container really.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\ni can't reproduce it now,  but it happend abiogenetic.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\nk8s v1.24.17\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nk8s\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n5.10.134-13.1.an8.x86_64\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Kubelet cannot handle multiple ResourceClaims if one is already prepared",
    "description": "### What happened?\n\nWhen using Dynamic Resource Allocation (DRA) with ResourceClaims, kubelet fails with:\n\n    Error: internal error: unable to get claim info for ResourceClaim <name>\n\nThe error occurs when:\n1. A first pod is created that references a single ResourceClaim.\n2. A second pod is created that references the same ResourceClaim plus an additional one.\n\nThe failure happens during container creation, after scheduling succeeded.\n\nThe issue is reproducible using the dra-example-driver from kubernetes-sigs.\n\nPotential spots where log may occurs:\nhttps://github.com/kubernetes/kubernetes/blob/release-1.34/pkg/kubelet/cm/dra/manager.go#L417\nhttps://github.com/kubernetes/kubernetes/blob/release-1.34/pkg/kubelet/cm/dra/manager.go#L442\nhttps://github.com/kubernetes/kubernetes/blob/release-1.34/pkg/kubelet/cm/dra/manager.go#L498\n\n### What did you expect to happen?\n\nThe second pod should successfully start with both ResourceClaims prepared.\n\nNotably, the same set of ResourceClaims works correctly when requested in the opposite order:\n- When the first pod uses both ResourceClaims, and the second pod uses only one - Works \u2705 \n- When the first pod uses one ResourceClaim, and the second pod then uses both - Not working \ud83d\udd34 \n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Deploy the dra-example-driver:\n   https://github.com/kubernetes-sigs/dra-example-driver\n\n2. Create two ResourceClaims:\n```yaml\napiVersion: resource.k8s.io/v1\nkind: ResourceClaim\nmetadata:\n  name: first-resource-claim\nspec:\n  devices:\n    requests:\n    - name: request\n      exactly:\n        deviceClassName: gpu.example.com\n        allocationMode: ExactCount\n        count: 1\n---\napiVersion: resource.k8s.io/v1\nkind: ResourceClaim\nmetadata:\n  name: second-resource-claim\nspec:\n  devices:\n    requests:\n    - name: request\n      exactly:\n        deviceClassName: gpu.example.com\n        allocationMode: ExactCount\n        count: 1\n```\n\n3. Create a first pod that references only the first ResourceClaim:\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod-1\nspec:\n  containers:\n  - name: container\n    image: busybox\n    command: [\"sleep\", \"infinity\"]\n    resources:\n      claims:\n      - name: first\n  resourceClaims:\n  - name: first\n    resourceClaimName: first-resource-claim\n  restartPolicy: Never\n```\n\n4. Wait until the pod is running.\n5. Create a second pod that references both ResourceClaims:\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod-2\nspec:\n  containers:\n  - name: container\n    image: busybox\n    command: [\"sleep\", \"infinity\"]\n    resources:\n      claims:\n      - name: first\n      - name: second\n  resourceClaims:\n  - name: first\n    resourceClaimName: first-resource-claim\n  - name: second\n    resourceClaimName: second-resource-claim\n  restartPolicy: Never\n```\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nVersion: v1.34.1+k3s1\n```\n\n</details>\n\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 24.04.3 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04.3 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n$ uname -a\nLinux 6.11.0-17-generic #17~24.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 20 22:48:29 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "SchedulerAsyncAPICalls can lead to performance issues.",
    "description": "The SchedulerAsyncAPICalls feature gate, intended to improve scheduler throughput by making API calls like Pod status updates asynchronous, can inadvertently lead to performance issues in certain scenarios, particularly in large clusters or those with a high rate of unschedulable pods.\n\nProblem:\n\nWhen SchedulerAsyncAPICalls is enabled, the scheduler can cycle through pods more quickly, as it doesn't wait for each API call to complete. However, when dealing with pods that are frequently unschedulable and enter backoff, the asynchronous status update calls can accumulate in the background.\n\nThis accumulation of outstanding asynchronous requests can cause the underlying client-go client to experience significant throttling. The critical issue is that the same client instance, which is now heavily throttled, is also used for essential operations like binding pods to nodes.\n\nConsequently, even when the scheduler successfully finds a node for a pod, the Bind call can be severely delayed due to the client-side throttling, which was primarily caused by the backlog of status updates for other, unrelated unschedulable pods. This effect is pronounced when many pods are coming out of backoff, as they trigger a burst of status updates.\n\nContrast with Synchronous Calls:\n\nWithout SchedulerAsyncAPICalls, API call latency (and any associated throttling) directly impacts the main scheduling loop. While this slows down the overall cycle time, it also naturally limits the rate of new API requests, preventing excessive accumulation and extreme throttling. The throttling effect is immediately \"felt\" by the scheduler's pacing.\n\nWith async calls, the scheduler loses this inherent backpressure. It continues to process pods and initiate new async requests, somewhat oblivious to the growing queue and throttling on the client side. This can lead to a state where the client is overwhelmed, impacting the latency of all operations, including critical ones like Bind.\n\nImpact:\n\nThis can lead to:\n\n- Slower pod startup times, even for pods that are easily schedulable.\n- Reduced effective scheduler throughput for successful placements, despite the async nature of other calls.\n- Potential for pod scheduling to stall or become very slow in clusters under load.\n\nExpected Behavior:\n\nThe scheduler should ideally have mechanisms to be aware of and adapt to client-side throttling, even when using asynchronous API calls, to prevent a backlog of status updates from negatively impacting critical operations like pod binding.\n\nReproduction:\n\nCurrently we see it in GKE internal tests in 1.35. This should also be reproducible in scheduler performance tests with a lowered kube-apiserver performance  https://github.com/kubernetes/kubernetes/issues/134261\n\n/sig scheduling"
  },
  {
    "name": "Feature Request: Support specifying permissions (mode) for the mount directory of ConfigMap/Secret volumes",
    "description": "### What happened?\n\nCurrently, when mounting a ConfigMap or Secret as a volume, we can use the defaultMode field to specify the permissions for the files projected into the volume.\n\nHowever, there is no field to explicitly specify the permissions of the directory (mount point) itself. The directory is created with default permissions (typically governed by the Kubelet or underlying OS defaults), which may not be restrictive enough for certain security requirements.\n\n### What did you expect to happen?\n\nI propose adding a new field, such as directoryMode (or similar), to the ConfigMapVolumeSource and SecretVolumeSource.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nNA\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Kubernetes v1.35 regression: WatchListClient breaks with fake metadata informers",
    "description": "Reproducer:\n\n```go\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n\t\"k8s.io/apimachinery/pkg/watch\"\n\tmetadatafake \"k8s.io/client-go/metadata/fake\"\n\t\"k8s.io/client-go/metadata/metadatainformer\"\n\t\"k8s.io/client-go/tools/cache\"\n)\n\nfunc TestBasic(t *testing.T) {\n\tlog.EnableKlogWithVerbosity(9)\n\tscheme := metadatafake.NewTestScheme()\n\tmetav1.AddMetaToScheme(scheme)\n\tkc := metadatafake.NewSimpleMetadataClient(scheme)\n\tclientStop := make(chan struct{})\n\tg := schema.GroupVersionResource{Group: \"apiextensions.k8s.io\", Version: \"v1\", Resource: \"customresourcedefinitions\"}\n\tinf := cache.NewSharedIndexInformerWithOptions(\n\t\t&cache.ListWatch{\n\t\t\tListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\n\t\t\t\treturn kc.Resource(g).Namespace(\"\").List(context.Background(), options)\n\t\t\t},\n\t\t\tWatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n\t\t\t\treturn kc.Resource(g).Namespace(\"\").Watch(context.Background(), options)\n\t\t\t},\n\t\t},\n\t\t&metav1.PartialObjectMetadata{},\n\t\tcache.SharedIndexInformerOptions{\n\t\t\tResyncPeriod:      0,\n\t\t\tIndexers:          cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc},\n\t\t\tObjectDescription: \"test\",\n\t\t},\n\t)\n\tgo inf.Run(clientStop)\n\tfor range 100 {\n\t\tif inf.HasSynced() {\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(time.Millisecond)\n\t}\n\tt.Fatal(\"failed to sync\")\n}\n```\n\n```\n$ KUBE_FEATURE_WatchListClient=true go test -v . -run ^TestBasic$\n=== RUN   TestBasic\n2025-12-22T21:47:13.697670Z\tinfo\tklog\tFeature gate default state\tfeature=InformerResourceVersion enabled=true\n2025-12-22T21:47:13.697699Z\tinfo\tklog\tFeature gate updated state\tfeature=WatchListClient enabled=true\n2025-12-22T21:47:13.697701Z\tinfo\tklog\tFeature gate default state\tfeature=ClientsAllowCBOR enabled=false\n2025-12-22T21:47:13.697702Z\tinfo\tklog\tFeature gate default state\tfeature=ClientsPreferCBOR enabled=false\n2025-12-22T21:47:13.697704Z\tinfo\tklog\tFeature gate default state\tfeature=InOrderInformers enabled=true\n2025-12-22T21:47:13.697706Z\tinfo\tklog\tFeature gate default state\tfeature=InOrderInformersBatchProcess enabled=true\n2025-12-22T21:47:13.697731Z\tinfo\tklog\tStarting reflector\ttype=test resyncPeriod=0s reflector=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289\n2025-12-22T21:47:13.697735Z\tinfo\tklog\tListing and watching\ttype=test reflector=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289\n    repro_test.go:48: failed to sync\n--- FAIL: TestBasic (0.11s)\nFAIL\nFAIL\n$ KUBE_FEATURE_WatchListClient=false go test -v ./pkg/webhooks/validation/controller -run ^TestBasic$\n=== RUN   TestBasic\n2025-12-22T21:47:18.826968Z\tinfo\tklog\tFeature gate default state\tfeature=InformerResourceVersion enabled=true\n2025-12-22T21:47:18.826998Z\tinfo\tklog\tFeature gate updated state\tfeature=WatchListClient enabled=false\n2025-12-22T21:47:18.827000Z\tinfo\tklog\tFeature gate default state\tfeature=ClientsAllowCBOR enabled=false\n2025-12-22T21:47:18.827001Z\tinfo\tklog\tFeature gate default state\tfeature=ClientsPreferCBOR enabled=false\n2025-12-22T21:47:18.827002Z\tinfo\tklog\tFeature gate default state\tfeature=InOrderInformers enabled=true\n2025-12-22T21:47:18.827003Z\tinfo\tklog\tFeature gate default state\tfeature=InOrderInformersBatchProcess enabled=true\n2025-12-22T21:47:18.827046Z\tinfo\tklog\tStarting reflector\ttype=test resyncPeriod=0s reflector=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289\n2025-12-22T21:47:18.827050Z\tinfo\tklog\tListing and watching\ttype=test reflector=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289\n2025-12-22T21:47:18.827097Z\tinfo\tklog\tCaches populated\ttype=test reflector=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289\n--- PASS: TestBasic (0.00s)\nPASS\n```\n\nIf you bump up the timeout, it will also log\n\n```\nWarning: event bookmark expired\terr=pkg/mod/k8s.io/client-go@v0.35.0/tools/cache/reflector.go:289: awaiting required bookmark event for initial events stream, no events received for 10.000401477s\n```\n\nThis only seems to effect the fake client, the real client appears to work fine"
  },
  {
    "name": "job integration: TestUpdateJobPodResources \"field is immutable\" flake",
    "description": "### Which jobs are flaking?\n\nci-kubernetes-integration-master and similar jobs for other platforms.\n\nhttps://storage.googleapis.com/k8s-triage/index.html?text=field%20is%20immutable&test=k8s.io%2Fkubernetes%2Ftest%2Fintegration\n\n\n\n### Which tests are flaking?\n\n`TestUpdateJobPodResources/suspended_job,_feature_gate_enabled,_update_resources` in test/integration/job.\n\n\n### Since when has it been flaking?\n\nAt least since 2025-12-8.\n\n\n\n### Testgrid link\n\nhttps://testgrid.k8s.io/sig-release-master-blocking#integration-master&include-filter-by-regex=job.job\n\n### Reason for failure (if possible)\n\n```\n=== RUN   TestUpdateJobPodResources/suspended_job,_feature_gate_enabled,_update_resources\n    testserver.go:631: Resolved testserver package path to: \"/home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing\"\n    testserver.go:429: runtime-config=map[]\n    testserver.go:430: Starting kube-apiserver on port 35527...\n    testserver.go:465: Waiting for /healthz to be ok...\n...\n    job_test.go:4428: Failed to update Job: Job.batch \"update-suspend\" is invalid: spec.template: Invalid value: {\"labels\":{\"batch.kubernetes.io/controller-uid\":\"62063ffa-bc98-41f6-af97-fa9df430a6e8\",\"batch.kubernetes.io/job-name\":\"update-suspend\",\"controller-uid\":\"62063ffa-bc98-41f6-af97-fa9df430a6e8\",\"job-name\":\"update-suspend\"},\"Spec\":{\"Volumes\":null,\"InitContainers\":null,\"Containers\":[{\"Name\":\"test-container\",\"Image\":\"busybox\",\"Command\":null,\"Args\":null,\"WorkingDir\":\"\",\"Ports\":null,\"EnvFrom\":null,\"Env\":null,\"Resources\":{\"Limits\":{\"cpu\":\"400m\",\"memory\":\"512Mi\"},\"Requests\":{\"cpu\":\"200m\",\"memory\":\"256Mi\"},\"Claims\":null},\"ResizePolicy\":null,\"RestartPolicy\":null,\"RestartPolicyRules\":null,\"VolumeMounts\":null,\"VolumeDevices\":null,\"LivenessProbe\":null,\"ReadinessProbe\":null,\"StartupProbe\":null,\"Lifecycle\":null,\"TerminationMessagePath\":\"/dev/termination-log\",\"TerminationMessagePolicy\":\"File\",\"ImagePullPolicy\":\"Always\",\"SecurityContext\":null,\"Stdin\":false,\"StdinOnce\":false,\"TTY\":false}],\"EphemeralContainers\":null,\"RestartPolicy\":\"Never\",\"TerminationGracePeriodSeconds\":30,\"ActiveDeadlineSeconds\":null,\"DNSPolicy\":\"ClusterFirst\",\"NodeSelector\":null,\"ServiceAccountName\":\"\",\"AutomountServiceAccountToken\":null,\"NodeName\":\"\",\"SecurityContext\":{\"HostNetwork\":false,\"HostPID\":false,\"HostIPC\":false,\"ShareProcessNamespace\":null,\"HostUsers\":null,\"SELinuxOptions\":null,\"WindowsOptions\":null,\"RunAsUser\":null,\"RunAsGroup\":null,\"RunAsNonRoot\":null,\"SupplementalGroups\":null,\"SupplementalGroupsPolicy\":null,\"FSGroup\":null,\"FSGroupChangePolicy\":null,\"Sysctls\":null,\"SeccompProfile\":null,\"AppArmorProfile\":null,\"SELinuxChangePolicy\":null},\"ImagePullSecrets\":null,\"Hostname\":\"\",\"Subdomain\":\"\",\"SetHostnameAsFQDN\":null,\"Affinity\":null,\"SchedulerName\":\"default-scheduler\",\"Tolerations\":null,\"HostAliases\":null,\"PriorityClassName\":\"\",\"Priority\":null,\"PreemptionPolicy\":null,\"DNSConfig\":null,\"ReadinessGates\":null,\"RuntimeClassName\":null,\"Overhead\":null,\"EnableServiceLinks\":null,\"TopologySpreadConstraints\":null,\"OS\":null,\"SchedulingGates\":null,\"ResourceClaims\":null,\"Resources\":null,\"HostnameOverride\":null,\"WorkloadRef\":null}}: field is immutable\n```\n\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig apps"
  },
  {
    "name": "SAST. Potential bugs",
    "description": "1. In /cmd/kubeadm/app/cmd/reset.go [on line 106 of the newResetData function](https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/cmd/reset.go#L106) an uninitialized initCfg pointer is declared. \nThe initCfg pointer is initialized [on line 139](https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/cmd/reset.go#L139) when calling the FetchInitConfigurationFromCluster function, if the FetchInitConfigurationFromCluster function returned a tuple of nil, err, then the function is not exited. On [on line 170](https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/cmd/reset.go#L170) a potential nil dereference may occur when accessing the ClusterConfiguration field.\n2. In [pkg/kubelet/kuberuntime/kuberuntime_sandbox_linux.go on the line 57](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/kuberuntime_sandbox_linux.go#L57) is the cpuRequest pointer, which is initialized when the condition for equality of the variable cpuRequestExists is true on line 58. When [calling the calculateLinuxResources](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/kuberuntime_sandbox_linux.go#L66) function and passing cpuRequest as an argument, a null pointer may be dereferenced.\n3. In [/staging/src/k8s.io/kubectl/pkg/cmd/cp/cp.go on the line 304](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/cp/cp.go#L304 ) opens a bidirectional read-start channel, in case of exit [on line 316](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/cp/cp.go#L316), the goroutine for closing allocated resources does not close. Resource leakage is possible.\n4. In [/pkg/registry/core/serviceaccount/storage/token.go on the line 98](https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/serviceaccount/storage/token.go#L98 ) the name parameter is used in the error, although the namespace parameter was validated. It might be more correct to use namespace on line 98.\nCould you tell me please, am I right in evaluating these static analyzer triggers as TP?"
  },
  {
    "name": "APIService remains unhealthy due to kube-apiserver reusing connection to incorrect endpoint and not verifying server certificate",
    "description": "### What happened?\n\nWhen kube-apiserver is deployed on a new control-plane node before kube-proxy is running, the AvailableConditionController, which checks the availability of registered APIServices, may connect to the Service\u2019s ClusterIP before it is proxied by kube-proxy. As a result, the connection can be routed to an external server that happens to be listening on the same IP, causing the APIService to be reported as unhealthy due to unexpected responses.\n\nHowever, even after the kube-proxy runs and proxys the ClusterIP, because of transport-layer connection caching and the fact that kube-apiserver is hard-coded to skip server certificate validation (even when the APIService\u2019s `insecureSkipTLSVerify` is set to false), the APIService can remain unhealthy until the kube-apiserver is restarted:\nhttps://github.com/kubernetes/kubernetes/blob/5151f58ef08d9fb26931019d6d844292aeb57637/staging/src/k8s.io/kube-aggregator/pkg/controllers/status/remote/remote_available_controller.go#L179-L184\n\n\n\n### What did you expect to happen?\n\nkube-apiserver should be able to recover from corner cases where it mistakenly establishes a connection to an incorrect server that never returns the expected response during APIService availability checks. It can be addressed either by verifying the server\u2019s certificate when the APIService\u2019s caBundle is provided, or by avoiding reuse of cached connections when the server returns an unexpected response.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n1. Install an APIService with a caBundle configured and backed by a ClusterIP Service.\n2. Run an HTTPS API server outside the cluster that listens to the ClusterIP and is routable from the cluster and returns 404 for unknown paths.\n3. Add a new control-plane node where kube-apiserver starts before kube-proxy is scheduled on the node.\n4. Check the kube-apiserver logs: the APIService continues to be reported as unhealthy due to 404 status code even after kube-proxy is running on the node.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "kube-apiserver problem",
    "description": "### What happened?\n\nK8S 1.34.3 version kube apiserver reports the following error after running for a period of time. Is this a bug\n{\"level\":\"warn\",\"ts\":\"2025-12-17T21:02:19.407Z\",\"caller\":\"clientv3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\" endpoint://client-06db3ae2-580c-423b-b38f-4d066ebc168d/x.x.x.x:2379 \",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"\n\n### What did you expect to happen?\n\nFix this issue\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nno\n\n### Anything else we need to know?\n\nno\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Why kubelet CRI streaming server bind port 0 if use dockershim",
    "description": "As we see, when use dockershim, kubelet startup a streaming server as cri server.\nThe streaming server will bind port 0 as cri server port. \nWhy bind port 0 not a fixed port as stream server port? In my case,  random port may conflict with node port, and make more uncertainly."
  },
  {
    "name": "v1.35.0 Prefilght Check logic for cgroup version vs kubelet version",
    "description": "### What would you like to be added?\n\nI did try the upgrade, but failed on \nhttps://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md#no-really-you-must-read-this-before-you-upgrade because of `the SystemVerification preflight check throws an error if cgroups v1 is detected and the detected kubelet version is v1.35 or newer. For older versions of kubelet, a preflight warning is displayed.`\nI'm used to set `--ignore-preflight-errors=KubeletVersion` because my kubelet is containerized so 'kubelet --version' can't be run and make the whole `SystemVerification` check to fail\n\nMy suggestion would be to check cgroups version, and KubeletVersion only if cgroup=V1 because setting `--ignore-preflight-errors=SystemVerification` will certainly work but I could miss some basic checks\n\n\n### Why is this needed?\n\nThe `if cgroup version is 2 then it's ok` logic would prevent the full `SystemVerification` preflight check to fail if kubelet version can't be checked"
  },
  {
    "name": "ci-kubernetes-unit-dependencies failing due to otelgrpc v0.64.0 nil pointer panic",
    "description": "### What happened?\n\nThe `ci-kubernetes-unit-dependencies` CI job is failing after updating `go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc` from v0.60.0 to v0.64.0.\n\n**Failed job**: https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-unit-dependencies/2002001097211777024\n\n### Root Cause\n\nCommit [07504e453](https://github.com/open-telemetry/opentelemetry-go-contrib/commit/07504e45307f41b8b4e609324a3b62374ad6008f) in otelgrpc removed nil checks from `WithTracerProvider` and `WithMeterProvider` options. When Kubernetes passes a nil `TracerProvider` in `staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:326`:\n\n```go\notelgrpc.WithTracerProvider(c.TracerProvider),  // c.TracerProvider can be nil\n```\n\nThe nil now overwrites the default global provider, causing a panic in `NewClientHandler()` when it calls `c.TracerProvider.Tracer(...)`.\n\n/kind bug\n/priority important-soon"
  },
  {
    "name": "Future dependencies CI jobs failing on new github.com/godbus/dbus/v5 tag",
    "description": "### Which jobs are failing?\n\n\n```\n+ hack/verify-typecheck.sh\ntype-checking windows/arm64\ntype-checking windows/386\nERROR(windows/arm64): /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/godbus/dbus/v5/auth_sha1_windows.go:10:2: \"os/user\" imported and not used\ntype-checking linux/amd64\nERROR(windows/386): /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/godbus/dbus/v5/auth_sha1_windows.go:10:2: \"os/user\" imported and not used\ntype-checking linux/arm64\ntype-checking windows/amd64\ntype-checking linux/ppc64le\ntype-checking linux/s390x\nERROR(windows/amd64): /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/godbus/dbus/v5/auth_sha1_windows.go:10:2: \"os/user\" imported and not used\ntype-checking darwin/arm64\ntype-checking darwin/amd64\ntype-checking linux/arm\ntype-checking linux/386\nexit status 1\n!!! Typecheck has failed. This may cause cross platform build failures.\n!!! Please see https://git.k8s.io/kubernetes/test/typecheck for more information.\n+ EXIT_VALUE=1\n+ set +o xtrace\n```\n\n### Which tests are failing?\n\nPlease see: \n- https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-unit-dependencies/2002303465103036416\n- https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-dependencies/2002337439577804800\n\n\n### Since when has it been failing?\n\nSince [v5.2.1](https://github.com/godbus/dbus/releases/tag/v5.2.1) \n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\n_No response_\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig architecture\n/label code-organization"
  },
  {
    "name": "test/integration/scheduler/gang: DATA RACE",
    "description": "### What happened?\n\nhttps://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-integration-race-master/2002170085040459776\n\n```\n==================\nWARNING: DATA RACE\nRead at 0x00c0041808a0 by goroutine 6502:\n  runtime.mapIterStart()\n      /usr/local/go/src/runtime/map_swiss.go:160 +0x0\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).UnschedulablePods()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1306 +0xa4\n  k8s.io/kubernetes/test/integration/scheduler/gang.podInUnschedulablePods()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/gang/gang_test.go:41 +0x57\n  k8s.io/kubernetes/test/integration/scheduler/gang.TestGangScheduling.func1.1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/gang/gang_test.go:208 +0x8e\n  k8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext.func2()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/loop.go:87 +0xc5\n  k8s.io/apimachinery/pkg/util/wait.loopConditionUntilContext()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/loop.go:88 +0x32f\n  k8s.io/apimachinery/pkg/util/wait.PollUntilContextTimeout()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/poll.go:48 +0xd7\n  k8s.io/kubernetes/test/integration/scheduler/gang.TestGangScheduling.func1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/gang/gang_test.go:206 +0x1088\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallAPIGroups()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:897 +0x4e9\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:149 +0xc44\n  k8s.io/kubernetes/pkg/registry/resource/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/resource/rest/storage_resource.go:82 +0x150\n  k8s.io/kubernetes/pkg/registry/resource/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/resource/rest/storage_resource.go:51 +0x210\n  k8s.io/kubernetes/pkg/registry/resource/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/admissionregistration/rest/storage_apiserver.go:83 +0x1e6\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/admissionregistration/rest/storage_apiserver.go:50 +0x23a\n  k8s.io/kubernetes/pkg/registry/admissionregistration/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xfb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:68 +0x3e7\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:57 +0x146\n  k8s.io/kubernetes/pkg/registry/apps/rest.StorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/apps/rest/storage_apps.go:43 +0x1da\n  k8s.io/kubernetes/pkg/registry/apps/rest.(*StorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/flowcontrol/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:98 +0x4fc\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:78 +0x85\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:74 +0x5e\n  k8s.io/kubernetes/pkg/registry/storage/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/storage/rest/storage_storage.go:49 +0x2c8\n  k8s.io/kubernetes/pkg/registry/storage/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/scheduling/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/rbac/rest.RESTStorageProvider.storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/rbac/rest/storage_rbac.go:84 +0x78\n  k8s.io/kubernetes/pkg/registry/rbac/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/rbac/rest/storage_rbac.go:72 +0x244\n  k8s.io/kubernetes/pkg/registry/rbac/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xeb\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:71 +0x2bc\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.v1Storage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:62 +0x146\n  k8s.io/kubernetes/pkg/registry/networking/rest.RESTStorageProvider.NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/networking/rest/storage_settings.go:48 +0x2c8\n  k8s.io/kubernetes/pkg/registry/networking/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/registry/batch/rest.(*RESTStorageProvider).NewRESTStorage()\n      <autogenerated>:1 +0xd7\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).Install()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:210 +0x3ce\n  k8s.io/apiserver/pkg/endpoints.(*APIGroupVersion).InstallREST()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/groupversion.go:114 +0x245\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).installAPIResources()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:801 +0x504\n  k8s.io/apiserver/pkg/server.(*GenericAPIServer).InstallLegacyAPIGroup()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:858 +0x1da\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:140 +0xab5\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:230 +0x2d84\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:205 +0x584\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:194 +0x407\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:189 +0x30d\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:184 +0x21b\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:179 +0x1ed\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:175 +0x1b6\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:170 +0x190\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:165 +0x16a\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:109 +0xa4b\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:104 +0xa25\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:99 +0x9f8\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:95 +0x9ca\n  k8s.io/kubernetes/pkg/registry/core/rest.(*GenericConfig).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core_generic.go:90 +0x9a4\n  k8s.io/kubernetes/pkg/registry/core/rest.(*legacyProvider).NewRESTStorage()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:155 +0xc4\n  k8s.io/kubernetes/pkg/controlplane/apiserver.(*Server).InstallAPIs()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/apis.go:105 +0x38a\n  k8s.io/kubernetes/pkg/controlplane.CompletedConfig.New()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/instance.go:341 +0x25a\n  k8s.io/kubernetes/pkg/controlplane.CompletedConfig.New()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/instance.go:336 +0x204\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:220 +0x2fc4\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:220 +0x2faf\n  k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:376 +0x1731\n  k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()\n      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:225 +0x2e4\n  k8s.io/kubernetes/test/integration/framework.StartTestServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/framework/test_server.go:212 +0x2eb4\n  k8s.io/kubernetes/test/integration/util.InitTestAPIServer()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/util/util.go:511 +0x209\n  k8s.io/kubernetes/test/integration/util.InitTestSchedulerWithNS()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/util/util.go:708 +0x5c\n  k8s.io/kubernetes/test/integration/scheduler/gang.TestGangScheduling.func1()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/gang/gang_test.go:169 +0x2c8\n  testing.tRunner()\n      /usr/local/go/src/testing/testing.go:1934 +0x21c\n  testing.(*T).Run.gowrap1()\n      /usr/local/go/src/testing/testing.go:1997 +0x44\n\nPrevious write at 0x00c0041808a0 by goroutine 12310:\n  runtime.mapassign_faststr()\n      /usr/local/go/src/internal/runtime/maps/runtime_faststr_swiss.go:263 +0x0\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*unschedulablePods).addOrUpdate()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/unschedulable_pods.go:86 +0x2cf\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).moveToActiveQ.func1()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:662 +0x4bc\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*activeQueue).underLock()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/active_queue.go:249 +0xa1\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).moveToActiveQ()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:649 +0x2a1\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).requeuePodWithQueueingStrategy()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1215 +0x144\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).movePodsToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1245 +0x82f\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).moveAllToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1181 +0x4f8\n  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).MoveAllToActiveOrBackoffQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1191 +0x1a9\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).addPodToSchedulingQueue()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:236 +0x5fe\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).addPod()\n      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:145 +0x198\n  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).addPod-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnAdd()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:286 +0x63\n  k8s.io/client-go/tools/cache.(*ResourceEventHandlerFuncs).OnAdd()\n      <autogenerated>:1 +0x1b\n  k8s.io/client-go/tools/cache.(*processorListener).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1074 +0x1e4\n  k8s.io/client-go/tools/cache.(*processorListener).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1084 +0x5e\n  k8s.io/client-go/tools/cache.(*processorListener).run-fm()\n      <autogenerated>:1 +0x33\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x82\n\nGoroutine 6502 (running) created at:\n  testing.(*T).Run()\n      /usr/local/go/src/testing/testing.go:1997 +0x9d2\n  k8s.io/kubernetes/test/integration/scheduler/gang.TestGangScheduling()\n      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/gang/gang_test.go:161 +0x1fa4\n  testing.tRunner()\n      /usr/local/go/src/testing/testing.go:1934 +0x21c\n  testing.(*T).Run.gowrap1()\n      /usr/local/go/src/testing/testing.go:1997 +0x44\n\nGoroutine 12310 (running) created at:\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0xdc\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:886 +0x1e7\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:890 +0x4b\n  k8s.io/client-go/tools/cache.(*sharedProcessor).run-fm()\n      <autogenerated>:1 +0x47\n  k8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x46\n  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()\n      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x82\n==================\n```\n\n### What did you expect to happen?\n\nNo DATA RACE.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n`go test -race`\n\n### Anything else we need to know?\n\nStarted to occur between 92d5eb117 and 779565541 on 2025-12-17 (code thaw, so lots of things merged). Not a flake. New in 1.36.\n\ncc @macsko @dom4ha \n\n/sig scheduling\n\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Can we build kubelet as static binary (CGO_ENABLED=0)",
    "description": "We have had a lot of discussions over time, but i do want to circle back to why exactly do we need kubelet build with CGO_ENABLED=1? Does anyone remember? Is it still true?\n\n- https://github.com/kubernetes/kubernetes/pull/120893#discussion_r1337563675\n- https://github.com/kubernetes/release/issues/3246\n- https://github.com/kubernetes/kubernetes/pull/114225\n- https://github.com/kubernetes/kubernetes/pull/82342 (we needed cgo for nvml?)\n\ncc @thockin @BenTheElder @mrunalp @derekwaynecarr ?\n\nPS: I totally get why kubectl needs CGO_ENABLED=1 under darwin and we have codified that in:\nhttps://github.com/kubernetes/kubernetes/blob/7d0b8f979c92253bbc3aade4b3f6c9161e3a839f/hack/lib/golang.sh#L357-L358"
  },
  {
    "name": "Unable to verify cosign signature for kubectl 1.35.0 on amd64",
    "description": "### What happened?\n\nAfter downloading kubectl 1.35.0, I attempted to verify the cosign keyless signature. This failed for amd64 but succeeds for arm64.\n\nI used the following command:\n\n```bash\n# amd64\ncosign verify-blob https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl \\\n    --signature https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl.sig \\\n    --certificate https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl.cert \\\n    --certificate-oidc-issuer https://accounts.google.com \\\n    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com\n\n# arm64\ncosign verify-blob https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl \\\n    --signature https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl.sig \\\n    --certificate https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl.cert \\\n    --certificate-oidc-issuer https://accounts.google.com \\\n    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com\n```\n\n### What did you expect to happen?\n\nThe command should have printed:\n\n```plaintext\nVerified OK\n```\n\nBut the error message was:\n\n```plaintext\nError: searching log query: [POST /api/v1/log/entries/retrieve][400] searchLogQueryBadRequest {\"code\":400,\"message\":\"verifying signature: invalid signature when validating ASN.1 encoded signature\"}\nerror during command execution: searching log query: [POST /api/v1/log/entries/retrieve][400] searchLogQueryBadRequest {\"code\":400,\"message\":\"verifying signature: invalid signature when validating ASN.1 encoded signature\"}\n```\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n```bash\n# amd64\ncosign verify-blob https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl \\\n    --signature https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl.sig \\\n    --certificate https://dl.k8s.io/release/v1.35.0/bin/linux/amd64/kubectl.cert \\\n    --certificate-oidc-issuer https://accounts.google.com \\\n    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com\n\n# arm64\ncosign verify-blob https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl \\\n    --signature https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl.sig \\\n    --certificate https://dl.k8s.io/release/v1.35.0/bin/linux/arm64/kubectl.cert \\\n    --certificate-oidc-issuer https://accounts.google.com \\\n    --certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com\n```\n\n### Anything else we need to know?\n\nThe SHA256 of kubectl 1.35.0 for amd64 is a2e984a18a0c063279d692533031c1eff93a262afcc0afdc517375432d060989.\n\nThe corresponding Rekor UUID is https://search.sigstore.dev/?uuid=108e9186e8c5677a0d75f1d86fbb64dd10a3e796fe62fa95e6b4707ceec7e91cfe741ba456a23bc0\n\nThe Rekor entry shows that for amd64 the signature matches but the certificate does not.\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.35.0\nKustomize Version: v5.7.1\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nnone\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n$ cat /etc/os-release\nPRETTY_NAME=\"Zorin OS 17.3\"\nNAME=\"Zorin OS\"\nVERSION_ID=\"17\"\nVERSION=\"17.3\"\nVERSION_CODENAME=jammy\nID=zorin\nID_LIKE=\"ubuntu debian\"\nHOME_URL=\"https://zorin.com/os/\"\nSUPPORT_URL=\"https://help.zorin.com/\"\nBUG_REPORT_URL=\"https://zorin.com/os/feedback/\"\nPRIVACY_POLICY_URL=\"https://zorin.com/legal/privacy/\"\nUBUNTU_CODENAME=jammy\n$ uname -a\nLinux visar 6.8.0-90-generic #91~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 20 15:20:45 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Preemption logic does not consider pod criticality",
    "description": "### What happened?\n\nPreemption logic `evictPodsToFreeRequests` and its helpers do not consider priority of the preemptable pods between them, when deciding which one to preempt.\n\nThe function `getPodsToPreempt` buckets pods by QoS (BestEffort, Burstable, Guaranteed), but fails to sort or prioritize them by `PodPriority` within those buckets. Please refer to:\nhttps://github.com/kubernetes/kubernetes/blob/4cf195304caa519be476b367267f6c656bce19f7/pkg/kubelet/preemption/preemption.go#L94C1-L191C2\n\nWhen having a preemptor pod with the highest priority, a `BestEffort` critical pod (with lower priority than the preemptor) will be preempted first, than a `Burstable` pod with low priority.\n\n\n### What did you expect to happen?\n\n* Priority should be considered to determine which pods to preempt first, not only the quality of service, thus making possible to preempt first a `Burstable` pod with low priority, than a `BestEffort` critical pod.\n* The `admitPod` causing the preemption should be logged to communicate which pod's admission is causing the mechanism to trigger.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nDeploy any pod with `system-node-critical` priority that requires more resources than the current available, while having a running `BestEffort` pod with `system-cluster-critical` priority and a `Burstable` pod with low priority, to trigger the preemption mechanism.\n\n### Anything else we need to know?\n\nN/A\n\n### Kubernetes version\n\nN/A\n\n### Cloud provider\n\nN/A\n\n### OS version\n\nN/A\n\n### Install tools\n\nN/A\n\n### Container runtime (CRI) and version (if applicable)\n\nN/A\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nN/A"
  },
  {
    "name": "genfeaturegates: Add linkCode and linkKEPs to json output for consistency with markdown",
    "description": "### What would you like to be added?\n\nThe recently added [genfeaturegates tool](https://github.com/kubernetes/kubernetes/blob/master/cmd/genfeaturegates/genfeaturegates.go) outputs feature gate information in both Markdown and json formats. However, I noticed that the json output is missing `linkCode` and `linkKEPs` fields that are present in the markdown output.\n\nMarkdown output includes:\n- `[code](https://cs.k8s.io/?q=...&repos=kubernetes/kubernetes)`\n- `[KEPs](https://cs.k8s.io/?q=...&repos=kubernetes/enhancements)`\n\nJSON output currently lacks these links.\n\n### Why is this needed?\n\nI'm working on k/docs (kubernetes/website) and discovered this inconsistency.\nHaving consistent output between formats would help tools and documentation that consume the json output.\n\n### Proposal\n\nUnless there is a specific reason for omitting these fields from json, I would like to add `linkCode` and `linkKEPs` fields to match what's already in the Markdown output."
  },
  {
    "name": "ServerSideApply: empty object {} converted to null and causing failure in schema validation",
    "description": "### What happened?\n\nWe are using ArgoCD Application CRD, in their [documentation](https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/), to set automatic sync with selfHeal we need to specify\n\n```yaml\nspec:\n  syncPolicy:\n    automated:\n      selfHeal: true\n```\n\nLater, we want to disable selfHeal, the issue we found is that we cannot change the existing manifest to \n\n```yaml\nspec:\n  syncPolicy:\n    automated: {}\n```\n\nusing server-side apply,\n\nIt will give an error saying the `{}` converted to `null` and thus become invalid according to the schema\n\nAlthough ArgoCD provide an alternative solution by updating the manifest to\n\n```yaml\nspec:\n  syncPolicy:\n    automated:\n      enabled: true\n```\n\nWe still think that ServerSideApply should work for `{}` too\n\n### What did you expect to happen?\n\nThe apply should be successfully\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nWe can reproduce this by creating the following simple CRD\n\n```yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: hellos.example.com\nspec:\n  group: example.com\n  names:\n    kind: Hello\n    plural: hellos\n    singular: hello\n  scope: Namespaced\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                automated:\n                  type: object\n                  properties:\n                    selfHeal: { type: boolean }\n\n```\n\nApplying the following manifest is successful\n\n```bash\nbash$ kubectl apply --server-side -f- <<EOF\napiVersion: example.com/v1alpha1\nkind: Hello\nmetadata:\n  name: example\nspec:\n  automated:\n    selfHeal: true\n---\napiVersion: example.com/v1alpha1\nkind: Hello\nmetadata:\n  name: example2\nspec:\n  automated: {}\nEOF\n```\n\nthis is to prove that apiserver can store empty object\n\n```bash\nbash$ kubectl get hello -o json | jq '.items[].spec'\n{\n  \"automated\": {\n    \"selfHeal\": true\n  }\n}\n{\n  \"automated\": {}\n}\n```\n\nbut change the `example` to have empty object will fail\n\n```bash\nbash$ kubectl apply --server-side -f- <<EOF\napiVersion: example.com/v1alpha1\nkind: Hello\nmetadata:\n  name: example\nspec:\n  automated: {}\nEOF\nThe Hello \"example\" is invalid: spec.automated: Invalid value: \"null\": spec.automated in body must be of type object: \"null\"\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.35.0\nKustomize Version: v5.7.1\nServer Version: v1.35.0\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nNone, I tested it using [Kind](https://kind.sigs.k8s.io/)\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 25.10\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"25.10\"\nVERSION=\"25.10 (Questing Quokka)\"\nVERSION_CODENAME=questing\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=questing\nLOGO=ubuntu-logo\n$ uname -a\nLinux colima 6.17.0-6-generic #6-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct  7 14:22:06 UTC 2025 aarch64 GNU/Linux\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Slow pod scheduling when ResourceClaimTemplate is created after pod creation due to exponential backoff",
    "description": "### What happened?\n\nWhen a Pod references a `ResourceClaimTemplate` (RCT) that doesn't exist at pod creation time, and the RCT is created later, there can be significant delays before the pod gets scheduled.\n\n**Root Cause:**\n\nThe ResourceClaim controller in kube-controller-manager uses `AddRateLimited()` when the referenced ResourceClaimTemplate is not found. This applies dual rate limiting:\n\n1. **Exponential backoff (per-item):** `5ms \u00d7 2^failures`, capped at 1000 seconds\n2. **Token bucket (global):** 10 QPS with 100-token burst\n\nThe final delay is `MAX(exponential_delay, token_bucket_delay)`.\n\n**The Problem:**\n\nWhen the ResourceClaimTemplate is eventually created, there is no mechanism to:\n1. Notify the ResourceClaim controller to immediately reprocess waiting pods\n2. Move affected pods from `unschedulablePods` queue back to `activeQ` in the scheduler\n\nThe pod must wait for its current backoff period to expire before the controller retries, which can be up to 1000 seconds if the pod has already failed multiple times.\n\n### What did you expect to happen?\n\nWhen a new `ResourceClaimTemplate` is created, pods that were waiting due to the missing template should be processed promptly (within seconds), not after potentially long backoff delays.\n\n### How to reproduce it (as minimally and precisely as possible)?\n\n1. Create a Pod that references a non-existent ResourceClaimTemplate:\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: test\n    image: nginx\n    resources:\n      claims:\n      - name: my-claim\n  resourceClaims:\n  - name: my-claim\n    resourceClaimTemplateName: my-template  # Does not exist yet\n```\n2. Wait for several retry cycles (observe increasing backoff delays in controller logs)\n3. Create the ResourceClaimTemplate:\n```yaml\napiVersion: resource.k8s.io/v1\nkind: ResourceClaimTemplate\nmetadata:\n  name: my-template\nspec:\n  spec:\n    devices:\n      requests:\n      - name: device\n        deviceClassName: example-class\n```"
  },
  {
    "name": "`ServiceAccount` existence check is missing when creating deploy/sts/ds/job/cj workloads",
    "description": "### What would you like to be added?\n\nWhen creating deploy/sts/ds/job/cj workloads, there is no `ServiceAccount` existence check.\nExample:\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      serviceAccountName: not-exist # not-exist serviceAccount\n      containers:\n      - name: my-app\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n```\nAfter appling to the cluster, no pod created.\nCheck rs event:\n`k describe rs my-app-deployment-xxx | less`\n```\nEvents:\n  Type     Reason        Age                From                   Message\n  ----     ------        ----               ----                   -------\n  Warning  FailedCreate  9s (x13 over 29s)  replicaset-controller  Error creating: pods \"my-app-deployment-6ddd76cf44-\" is forbidden: error looking up service account default/not-exist: serviceaccount \"not-exist\" not found\n```\nGot error `error looking up service account default/not-exist: serviceaccount \"not-exist\" not found`.\n\n### Why is this needed?\n\nShoud check `ServiceAccount` existence when creating and updating deploy/sts/ds/job/cj workloads."
  },
  {
    "name": "Feature Request: Native Deployment strategy to trigger rollout on referenced ConfigMap/Secret updates (Reviving #22368)",
    "description": "### What would you like to be added?\n\nI would like to propose a native mechanism within the `Deployment` (and `StatefulSet`/`DaemonSet`) spec to trigger a rollout when a referenced `ConfigMap` or `Secret` is modified.\n\nThis is a request to revive the discussion from **Issue #22368** with a focus on modern Platform Engineering requirements.\n\n**Proposed Spec:**\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        envFrom:\n        - configMapRef:\n            name: app-config\n            # Proposed new field to handle the rollout logic natively\n            policy:\n              onUpdate: RollingUpdate # Options: Ignore, RollingUpdate, Recreate\n\n```\n\n**Justification for Core Inclusion:**\n\n* **Safety:** The `kube-controller-manager` is the only component that can guarantee a safe rollout (respecting `maxUnavailable`/`maxSurge`) while ensuring the new config is actually available on the node before the pod starts.\n* **Standardization:** This is a fundamental operational requirement for 90% of stateful and stateless applications. It should not require a third-party controller.\n\n**Related Issues:**\n\n* Relates to #22368 (Facilitate ConfigMap rollouts)\n\n**Note for KEP**\n\nI understand this is a significant change to the API and controller logic that would require a Kubernetes Enhancement Proposal (KEP). I am opening this issue to gauge community consensus on whether this logic belongs in upstream Kubernetes before attempting to draft a KEP.\n\n### Why is this needed?\n\nThis is one of the most common operational pain points for Kubernetes users. While separating Configuration from Code is a best practice (12-Factor App), Kubernetes currently lacks a native way to reconcile the lifecycle of the two.\n\n**Current Workarounds and their downsides:**\n\n1. **Templating Hacks (Helm/Kustomize):** Users manually inject a checksum annotation (e.g., `checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}`) into the Pod template. This forces a rollout but requires complex templating logic and couples the CI/CD tool to the runtime behavior.\n2. **External Controllers:** Users install third-party operators (like `Reloader` or `Wave`) solely to watch resources and delete Pods. This adds maintenance overhead, security surface area, and extra resource consumption to the cluster.\n3. **Manual Toil:** Operators manually run `kubectl rollout restart` after applying config changes.\n\n**Benefits of Native Support:**\n\n* **Standardization:** Removes the need for every organization to \"reinvent the wheel\" for config reloads.\n* **Safety:** The Kube Controller Manager is best positioned to handle this rollout safely, respecting `maxUnavailable` and `maxSurge` constraints better than external \"pod killer\" scripts.\n* **GitOps Friendly:** Simplifies the reconciliation loop for tools like ArgoCD and Flux, ensuring that the running state always matches the configuration state defined in Git without extra glue code."
  },
  {
    "name": "Kubelet SyncTerminatedPod times out after 30s due to race with cleanupOrphanedPodCgroups",
    "description": "### What happened?\n\nIn this [OpenShift CI run](https://prow.ci.openshift.org/view/gs/test-platform-results/logs/periodic-ci-openshift-multiarch-master-nightly-4.21-ocp-e2e-ovn-remote-s2s-libvirt-ppc64le/1999572809340162048), the upstream `[sig-node] Lifecycle sleep action zero value when create a pod with lifecycle hook using sleep action with a duration of zero seconds prestop hook using sleep action with zero duration` test failed because SyncTerminatedPod timed out. The kubelet logs shows:\n\n```\nDec 12 21:19:08.719197 compute-1 kubenswrapper[7200]: E1212 21:19:08.719175    7200 pod_workers.go:1324] \"Error syncing pod, skipping\" err=\"failed to delete cgroup paths for [kubepods besteffort poda4418b44-5c5a-4bd5-b7b3-42860ff00483] : unable to destroy cgroup paths for cgroup [kubepods besteffort poda4418b44-5c5a-4bd5-b7b3-42860ff00483] : Timed out while waiting for systemd to remove kubepods-besteffort-poda4418b44_5c5a_4bd5_b7b3_42860ff00483.slice\" pod=\"e2e-pod-lifecycle-sleep-action-allow-zero-761/pod-with-prestop-sleep-hook-zero-duration\" podUID=\"a4418b44-5c5a-4bd5-b7b3-42860ff00483\"\n```\n\nThe systemd journal reveals that the slice was actually removed successfully ~30 seconds before the kubelet timeout:\n\n```\nDec 12 21:18:39.127974 compute-1 systemd[1]: Removed slice libcontainer container kubepods-besteffort-poda4418b44_5c5a_4bd5_b7b3_42860ff00483.slice.\n```\n\nThis shows that systemd completed the slice removal at 21:18:39, but kubelet still timed out at 21:19:08 (exactly ~30 seconds later). The timeout occurred because the kubelet goroutine that initiated the removal never received the D-Bus completion signal.\n\n### What did you expect to happen?\n\nPod deletion should complete promptly after containers terminate, without a 30-second timeout.\n\n### How can we reproduce it? (as minimally and precisely as possible)\n\n1. Create a cluster with cgroupsPerQOS enabled (default)\n2. Run a workload that creates and deletes many pods rapidly\n3. Observe that some pods take 30+ seconds to fully terminate\n\nThe race is more likely to occur when:\n- Multiple pods are being terminated simultaneously\n- The orphaned pod cgroup cleanup housekeeping runs during pod termination\n\n### Anything else we need to know?\n\n#### Root Cause\n\nThere is a race condition between two code paths that call `pcm.Destroy()` on the same pod cgroup:\n\n1. **`SyncTerminatedPod`** - [pkg/kubelet/kubelet.go](https://github.com/kubernetes/kubernetes/blob/c34c5a5426aeb48c55c40af5c4b016a71c24d2d1/pkg/kubelet/kubelet.go#L2398-L2402):\n```go\npcm := kl.containerManager.NewPodContainerManager()\nif err := pcm.Destroy(name); err != nil {\n    return err\n}\n```\n\n2. **`cleanupOrphanedPodCgroups`** - [pkg/kubelet/kubelet_pods.go](https://github.com/kubernetes/kubernetes/blob/c34c5a5426aeb48c55c40af5c4b016a71c24d2d1/pkg/kubelet/kubelet_pods.go#L2835-L2836):\n```go\ngo pcm.Destroy(val)\n```\n\nBoth paths can run concurrently for the same pod cgroup. The `Destroy()` method calls systemd's `StopUnit` via D-Bus using `go-systemd`.\n\n#### Why the 30-second timeout occurs\n\nWhen `StopUnitContext` is called, the `go-systemd` library:\n\n1. Sends a D-Bus method call to systemd's `StopUnit`\n2. Systemd returns a **job path** (e.g., `/org/freedesktop/systemd1/job/12345`)\n3. The library registers the caller's `statusChan` in an internal map keyed by that job path\n4. When systemd emits a `JobRemoved` D-Bus signal with that job path, the library sends the result to the registered channel\n\n**The problem:** When two goroutines call `StopUnit` on the same unit concurrently, systemd may return the **same job path** for both requests (since it's already processing a stop for that unit). The `go-systemd` library overwrites the first channel registration:\n\n[go-systemd/dbus/methods.go](https://github.com/coreos/go-systemd/blob/f3c9410fa503128ff5a025265768843eb09815ca/dbus/methods.go#L55-L75\n)\n```go\n// In go-systemd's dbus/methods.go (simplified)\nfunc (c *Conn) startJob(ch chan<- string, job dbus.ObjectPath) {\n    c.jobListener.Lock()\n    c.jobListener.jobs[job] = ch  // OVERWRITES any existing channel for this job path!\n    c.jobListener.Unlock()\n}\n```\n\n**Timeline of the race:**\n\n1. **T=0ms:** Goroutine A calls `StopUnit`, gets job `/job/100`, registers `chanA`\n2. **T=1ms:** Goroutine B calls `StopUnit`, gets job `/job/100`, registers `chanB` (overwrites `chanA`!)\n3. **T=50ms:** Systemd finishes, emits `JobRemoved` for `/job/100`\n4. **T=50ms:** go-systemd sends result to `chanB` (the only one in the map now)\n5. **T=50ms:** Goroutine B completes successfully\n6. **T=30s:** Goroutine A times out (`chanA` was orphaned, never receives signal)\n\n#### Relevant code\n\nThe 30-second timeout is in [opencontainers/cgroups/systemd/common.go](https://github.com/opencontainers/cgroups/blob/main/systemd/common.go#L184-L210):\n\n```go\nfunc stopUnit(cm *dbusConnManager, unitName string) error {\n    statusChan := make(chan string, 1)\n    err := cm.retryOnDisconnect(func(c *systemdDbus.Conn) error {\n        _, err := c.StopUnitContext(context.TODO(), unitName, \"replace\", statusChan)\n        return err\n    })\n    if err == nil {\n        timeout := time.NewTimer(30 * time.Second)\n        defer timeout.Stop()\n\n        select {\n        case s := <-statusChan:\n            // ...\n        case <-timeout.C:\n            return errors.New(\"Timed out while waiting for systemd to remove \" + unitName)\n        }\n    }\n    // ...\n}\n```\n\n#### Proposed Fix\n\nUse `singleflight.Group` in `podContainerManagerImpl.Destroy()` to deduplicate concurrent calls for the same cgroup. This ensures only one D-Bus `StopUnit` call proceeds per cgroup while other callers wait and receive the same result, avoiding the channel overwrite race entirely.\n\n### Environment\n\n- Kubernetes version: v1.34+\n- CRI: CRI-O (crun/runc)\n- cgroup driver: systemd\n\n/kind bug\n/sig node\n/area kubelet"
  },
  {
    "name": "TestReplaceEvents is flaky",
    "description": "### Which jobs are flaking?\n\nTestReplaceEvents in client-go https://github.com/kubernetes/kubernetes/blame/bb52ae5e24a4eb118460e8feadee86b9f41b4244/staging/src/k8s.io/client-go/tools/cache/controller_test.go#L901 is flaking and failed in the smoke tests run as part of kubernetes/publishing-bot\n\n### Which tests are flaking?\n\nTestReplaceEvents/Delete_all_objects\n\nRef: https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-3669611965\n\n\n\n### Since when has it been flaking?\n\nFirst noticed in publishing-bot run https://github.com/kubernetes/kubernetes/issues/56876#issuecomment-3669611965\n\nThe changes were introduced in https://github.com/kubernetes/kubernetes/pull/135665\n\n### Testgrid link\n\n_No response_\n\n### Reason for failure (if possible)\n\nI think the ordering in the slice comparison is causing the flake\n\n```\nexpected: []cache.eventRecord{cache.eventRecord{Action:\"delete\", Key:\"default/pod-1\", EventRV:\"1\", StoreRV:\"\"}, cache.eventRecord{Action:\"delete\", Key:\"default/pod-3\", EventRV:\"2\", StoreRV:\"\"}}\nactual  : []cache.eventRecord{cache.eventRecord{Action:\"delete\", Key:\"default/pod-3\", EventRV:\"2\", StoreRV:\"\"}, cache.eventRecord{Action:\"delete\", Key:\"default/pod-1\", EventRV:\"1\", StoreRV:\"\"}}\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Relevant SIG(s)\n\n/sig api-machinery"
  },
  {
    "name": "It is confusing to have more than one default `StorageClass`",
    "description": "### What would you like to be added?\n\nCurrently, `StorageClass` allows to set many default StorageClasses, by add annotation `storageclass.kubernetes.io/is-default-class: \"true\"` on the StorageClass object.\n\nExample:\n```\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: sc-test1\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"  # default StorageClass\nprovisioner: example.com\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: sc-test2\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"  # default StorageClass\nprovisioner: example.com\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n```\nAfter appling to the cluster, the sc list shows more than 1 `default` sc:\n```\ncontrolplane:~$ k get sc\nNAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  30d\nsc-test1 (default)     example.com             Delete          WaitForFirstConsumer   true                   2s\nsc-test2 (default)     example.com             Delete          WaitForFirstConsumer   true                   2s\n```\n\nIt is confusing to use which default StorageClass when pvc not specify StorageClassName.\n\n### Why is this needed?\n\nNeed to restrict only one default `StorageClass` by add verification, to clarify the default `StorageClass` usage."
  },
  {
    "name": "[code-generator] `validation-gen` fails to parse default `ref(...)` values",
    "description": "### What happened?\n\nThe `defaulter-gen` tool knows how to generate defaults from referenced symbols when using the `+default=ref(SomeSymbol)`.\n\nFor example:\n\n``` golang\nconst (\n\tDefaultFooValue = 42\n\tDefaultBarValue = \"some-value\"\n)\n\n// TestSpec provides the configuration spec\ntype TestSpec struct {\n\t// +k8s:optional\n\t// +default=ref(DefaultFooValue)\n\tFoo int `json:\"foo,omitzero\"`\n\n\t// +k8s:optional\n\t// +default=ref(DefaultBarValue)\n\tBar string `json:\"bar,omitzero\"`\n}\n```\n\nThis is how the `defaulter-gen` tool handles such symbol references.\n\nhttps://github.com/kubernetes/kubernetes/blob/9d8404426fc33f6fbb2e5180b2e80f685b16bbd6/staging/src/k8s.io/code-generator/cmd/defaulter-gen/generators/defaulter.go#L551-L572\n\nHowever, the `validation-gen` tool does not understand `+default=ref(SomeSymbol)` and results in errors during generation.\n\nPlease see the details below for more details on how to reproduce the issue.\n\n### What did you expect to happen?\n\n`validation-gen` should succeed and understand how to parse markers supported by `defaulter-gen`.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nCreate a new project.\n\n```shell\nmkdir example-project\ncd example-project\ngo mod init example-project\n```\n\nInstall the tools.\n\n``` shell\ngo get -v -tool k8s.io/code-generator/cmd/defaulter-gen\ngo get -v -tool k8s.io/code-generator/cmd/validation-gen\n```\n\nCreate API package.\n\n``` shell\nmkdir -p pkg/apis/v1\n```\n\nCreate `pkg/apis/v1/doc.go` so that the generator tools can find the respective markers/tags.\n\n``` golang\n// pkg/apis/v1/doc.go\n\n// +k8s:deepcopy-gen=package\n// +k8s:defaulter-gen=TypeMeta\n// +k8s:validation-gen=TypeMeta\n// +groupName=my.group.example.org\n\npackage v1\n```\n\nCreate `pkg/apis/v1/types.go` with our API types.\n\n``` golang\npackage v1\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\nconst (\n\tDefaultFooValue = 42\n\tDefaultBarValue = \"some-value\"\n)\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// TestType is a top-level type. A client is created for it.\ntype TestType struct {\n\t// +k8s:opaqueType\n\tmetav1.TypeMeta `json:\",inline\"`\n\n\t// TestSpec provides the configuration spec.\n\tSpec TestSpec `json:\"spec,omitzero\"`\n}\n\n// TestSpec provides the configuration spec\ntype TestSpec struct {\n\t// +k8s:optional\n\t// +default=ref(DefaultFooValue)\n\tFoo int `json:\"foo,omitzero\"`\n\n\t// +k8s:optional\n\t// +default=ref(DefaultBarValue)\n\tBar string `json:\"bar,omitzero\"`\n}\n```\n\nRun the tools to generate the defaults.\n\n``` shell\ngo tool defaulter-gen ./pkg/apis/v1/...\n```\n\nThe command above would generate the following code in the `pkg/apis/v1/generated.defaults.go`.\n\n``` golang\n//go:build !ignore_autogenerated\n// +build !ignore_autogenerated\n\n// Code generated by defaulter-gen. DO NOT EDIT.\n\npackage v1\n\nimport (\n\truntime \"k8s.io/apimachinery/pkg/runtime\"\n)\n\n// RegisterDefaults adds defaulters functions to the given scheme.\n// Public to allow building arbitrary schemes.\n// All generated defaulters are covering - they call all nested defaulters.\nfunc RegisterDefaults(scheme *runtime.Scheme) error {\n\tscheme.AddTypeDefaultingFunc(&TestType{}, func(obj interface{}) { SetObjectDefaults_TestType(obj.(*TestType)) })\n\treturn nil\n}\n\nfunc SetObjectDefaults_TestType(in *TestType) {\n\tif in.Spec.Foo == 0 {\n\t\tin.Spec.Foo = int(DefaultFooValue)\n\t}\n\tif in.Spec.Bar == \"\" {\n\t\tin.Spec.Bar = string(DefaultBarValue)\n\t}\n}\n```\n\nWe can see that the default values are properly referencing the `DefaultFooValue` and `DefaultBarValue` symbols.\n\nIf we attempt to run the `validation-gen` tool now it will fail, because it doesn't understand the `+default=ref(...)` notation.\n\n```shell\ngo tool validation-gen --readonly-pkg=k8s.io/apimachinery/pkg/apis/meta/v1 ./pkg/apis/v1/...\n```\n\nResults in the following error.\n\n``` shell\nF1218 10:20:30.056153   23352 targets.go:361] failed to generate validations: field example-project/pkg/apis/v1.TestSpec.foo: tag \"k8s:optional\": failed to parse default value \"ref(DefaultFooValue)\": invalid character 'r' looking for beginning of value\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "1.35 release error: Package kubernetes-cni is not available",
    "description": "### What happened?\n\ntried to deploy 1.35 release and got error:\n```\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.35/deb/Release.key | gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg\necho 'deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.35/deb/ /'\napt-get update\n....\napt-get install -y kubelet=1.35.0-1.1 kubeadm=1.35.0-1.1 kubectl=1.35.0-1.1 kubernetes-cni\nReading package lists...\nBuilding dependency tree...\nReading state information...\nPackage kubernetes-cni is not available, but is referred to by another package.\nThis may mean that the package is missing, has been obsoleted, or\nis only available from another source\n\nE: Package 'kubernetes-cni' has no installation candidate\n```\n\n### What did you expect to happen?\n\nno error.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nrepro steps:\n```\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.35/deb/Release.key | gpg --dearmor -o /usr/share/keyrings/kubernetes-archive-keyring.gpg\necho 'deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.35/deb/ /'\napt-get update\napt-get install -y kubelet=1.35.0-1.1 kubeadm=1.35.0-1.1 kubectl=1.35.0-1.1 kubernetes-cni\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n1.35.0-1.1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\ncapi\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\nPRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"22.04\"\nVERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\nVERSION_CODENAME=jammy\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=jammy\n$ uname -a\nLinux host-a 5.15.0-161-generic #171-Ubuntu SMP Sat Oct 11 08:17:01 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "HNS loadbalancers for services are not recreated",
    "description": "### What happened?\n\nHNS loadbalancers appear to persist in cache, if removed these are never created throughout the lifecycle of kube-proxy. This out-of-band cleanup of loadbalancers could occur during service restarts of kubelet a CNI or race condition with another service.\n\nSimilar to https://github.com/kubernetes/kubernetes/issues/133928 however this issue applies to all service types. Reproduction has occurred using the default `internalTrafficPolicy: Cluster`\n\n\n\n### What did you expect to happen?\n\nkube-proxy invalidates cache and recreates missing HNS loadbalancers to avoid service disruption\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis has been known to occur during rke2 service restarts, the simplest reproduction is a manual removal of a loadbalancer:\n\nOn a Windows worker node:\n- List the load balancers: `hnsdiag list loadbalancers`\n  - Example:\n  ```\n  Load Balancer    : 3599612c-54b4-4b9d-9258-4fc5e2100549\n    Virtual IPs      : 172.30.0.1\n    Direct IP IDs    : 78c19c4d-7edf-43bd-ab2a-5ccf97f214fe\n  ```\n\n- Select an LB to remove: `hnsdiag delete loadbalancers <ID>`\n  - Example:\n  ```\n  > hnsdiag delete loadbalancers 3599612c-54b4-4b9d-9258-4fc5e2100549\n  ```\n- Monitor the LB list, and kube-proxy logs:\n  - `hnsdiag list loadbalancers`\n  - In kube-proxy logs the LB is persisted in a cache:\n  ```\n  I1217 23:36:36.438104    7712 proxier.go:1225] \"Policy already applied\" serviceInfo=\"172.30.0.1:443/TCP\"\n  ```\n\nThe kube-proxy sync interval continues to log that the policy is applied, yet the loadbalancers list has no matching LB. Restarting kube-proxy will rebuild the loadbalancers.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nRKE2 v1.31.14 (however other versions have been tested, v1.33.6 and v1.34.2)\n\n```console\n# kubectl version\nClient Version: v1.31.14+rke2r1\nKustomize Version: v5.4.2\nServer Version: v1.31.14+rke2r1\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n# cat /etc/os-release\nNAME=\"SLES\"\nVERSION=\"15-SP6\"\nVERSION_ID=\"15.6\"\nPRETTY_NAME=\"SUSE Linux Enterprise Server 15 SP6\"\nID=\"sles\"\nID_LIKE=\"suse\"\nANSI_COLOR=\"0;32\"\nCPE_NAME=\"cpe:/o:suse:sles:15:sp6\"\nDOCUMENTATION_URL=\"https://documentation.suse.com/\"\n# uname -a\nLinux ip-172-31-7-247 6.4.0-150600.23.73-default #1 SMP PREEMPT_DYNAMIC Tue Oct  7 08:43:02 UTC 2025 (46f6a23) x86_64 x86_64 x86_64 GNU/Linux\n\n# On Windows:\nC:\\>  Get-CimInstance Win32_OperatingSystem | Select-Object Caption, Version, BuildNumber, OSArchitecture\n\nCaption                                  Version    BuildNumber OSArchitecture\n-------                                  -------    ----------- --------------\nMicrosoft Windows Server 2022 Datacenter 10.0.20348 20348       64-bit\n```\n\n</details>\n\n\n### Install tools\n\n<details>\nInstalled using a custom cluster in Rancher v2.11.2. Configured with `cni: calico` and BGP\n  * https://docs.rke2.io/install/quickstart#windows-agent-worker-node-installation\n  * https://docs.rke2.io/networking/windows_bgp\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCalico with BGP enabled\n\n```yaml\n    chartValues:\n      rke2-calico:\n        installation:\n          calicoNetwork:\n            bgp: Enabled\n            ipPools:\n              - blockSize: 24\n                cidr: 172.29.0.0/16\n                encapsulation: None\n                natOutgoing: Enabled\n```\n</details>"
  },
  {
    "name": "replace `registry.k8s.io/etcd` image with official images from etcd",
    "description": "xref: https://github.com/etcd-io/etcd/issues/19798\n\nSlack conversation:\nhttps://kubernetes.slack.com/archives/C3HD8ARJ5/p1749575265549799\n\nQuote from @ivanvc :\n> I believe we're tracking that in this ticket: https://github.com/etcd-io/etcd/issues/19798\nIt links to a Google Doc, we've been discussing this for a while. I thought we had another k/k issue re: this, but I couldn't find it\n\nQuote from @ahrtr :\n> [@dims](https://kubernetes.slack.com/team/U0Y7A2MME) YES, eventually we expect all users (Kubeadm, KOps, k/k workflow) to use the etcd officially released image instead of building etcd image again from https://github.com/kubernetes/kubernetes/tree/master/cluster/images/etcd. Once that's done, we can deprecate [cluster/images/etcd](https://github.com/kubernetes/kubernetes/tree/master/cluster/images/etcd) , and eventually remove it from k/k repo.\nWe'll drive the effort, but It might take time to get it done (edited)\n\nGoogle Doc in question:\nhttps://docs.google.com/document/d/1B0C391PJ2zwHnmIwOWpzq-tjzj9-Gci-Ph2k9mwC5R4/edit?tab=t.0#heading=h.xblvj5c0ffhf\n\n/sig architecture\n/sig etcd\n/area code-organization\n/kind feature"
  },
  {
    "name": "CVE-2025-14269: Credential caching in Headlamp with Helm enabled",
    "description": "Original tracking issue: https://github.com/kubernetes-sigs/headlamp/issues/4282\n\nCVSS Rating: High (8.8) [CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3-1#CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H)\n\n_Description of vulnerability_\n\nA security issue was discovered in the in-cluster version of Headlamp where unauthenticated users may be able to reuse cached credentials to access Helm functionality through the Headlamp UI. Kubernetes clusters are only affected if Headlamp is installed, is configured with config.enableHelm: true, and an authorized user has previously accessed the Helm functionality.\n\n### Am I vulnerable?\n\nKubernetes clusters with an in-cluster installation of Headlamp <= v0.38.0 and config.enableHelm set to true are affected. The Headlamp desktop version is not affected.\n\n#### Affected Versions\n\n- Headlamp <= v0.38.0\n\n### How do I mitigate this vulnerability?\n\nUpgrade to the fixed version. Prior to upgrading, this vulnerability can be mitigated by ensuring Headlamp is not publicly exposed with an ingress server to limit exposure.\n\n\n#### Fixed Versions\n\n- Headlamp v0.39.0 https://github.com/kubernetes-sigs/headlamp/releases/tag/v0.39.0 \n\nTo upgrade, refer to the documentation: https://headlamp.dev/docs/latest/ \n\n### Detection\n\nReview logs for unexpected access to clusters/main/helm/releases/list and other Helm related endpoints.\n\nIf you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io\n\n#### Acknowledgements\n\nThis vulnerability was reported by [brndstrp](https://hackerone.com/brndstrp).\n\n/area security\n/kind bug\n/committee security-response\n/label official-cve-feed"
  },
  {
    "name": "TopologySpreadConstraint should fail if labelSelector not provided",
    "description": "So I've just recently realized, and folks can confirm - that if you create a TopologySpreadConstraint but do not list a `labelSelector`, it will be accepted - but will effectively do nothing since it matches nothing.\n\nAt least my observation was it was not respected until I added the `labelSelector`.\n\ne.g. something like:\n```\ntopologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: topology.kubernetes.io/hostname\n    whenUnsatisfiable: DoNotSchedule\n```\n\nactually needs to be:\n```\ntopologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: topology.kubernetes.io/hostname\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: my-app\n```\n\nUnless I'm missing something, it seems clear to me that no one would want to specify a constraint without also having a `labelSelector` - and seems easy to miss.\n\nI already missed it for my own application, and another application I use (cockroachDB), [also seems to have missed it](https://github.com/cockroachdb/helm-charts/blob/65fa4c6822b18d3cd8922cca60393199b7d95edb/cockroachdb-parent/charts/cockroachdb/values.yaml#L439) ([fix](https://github.com/cockroachdb/helm-charts/pull/584)).\n\nWhile we are here - a related matter I find interesting, is that you can not specify `matchLabelKeys` without `labelSelector` (as per the [docs](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.34/#topologyspreadconstraint-v1-core)). Assuming I understand how it works, it seems like a pretty standard default to just want to match against key \"app\", whatever it is for my pod - and that's it.\n\nPerhaps more generally, errors could be reported if labelSelector / matchLabelKeys ends up finding nothing (including itself!)."
  },
  {
    "name": "resource_quota_controller doesn't run properly in edge case with calico",
    "description": "### What happened?\n\nWe installed calico in our EKS cluster, and we observe that when kube-apiserver restart with new client-ca-file value, it updates configmap kube-system/extension-apiserver-authentication, then calico-apiserver restart as its design. kube-controller-manager also restarts during that timing.\n\nThe problem is rq.Run() is called, but never call to start worker(), it got stuck at first [WaitForNamedCacheSync()](https://github.com/kubernetes/kubernetes/blob/5ed8e8aa89e4ecab8073d2e6644f0ad59030a9d5/pkg/controller/resourcequota/resource_quota_controller.go#L307-L308)\nI believe that there is no worker started in that case. The cluster behavior is:\n- resourcequota status is not reflected when spec changed\n- Only pod increment is added into resourcequota, decrement is not (it possible as resourcequota also be updated via kube-apiserver resourcequota addmision)\n- We try to force delete resourcequota.status.used (set {}), but it not recalcuated\n- After we disable APIService for calico-apiserver, everything back, you can see double `Caches are synced for resource quota` (1 for Run(), other for Sync()) at same time on the log.\n\nRegarding calico-apiserver, as it restarted if configmap changed, so I believe it may takes time to startup. During that period, APIService calico is unavailable and discovery failed\n\n\n\n\n\n### What did you expect to happen?\n\nresource_quota_controller can Run() properly\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nI'm not sure whether you can reproduce it in your setup, but I can provide log for execution\n\n### Anything else we need to know?\n\n**CloudWatch Logs Insights**    \nregion: ap-northeast-1    \nlog-group-names: /eks\ndata-sources:     \nfacets: @data_source_type in [\"scheduler\", \"controller_manager\", \"authenticator\", \"audit\", \"api_server\"]    \nstart-time: 2025-12-15T04:00:00.000Z    \nend-time: 2025-12-16T06:59:59.000Z    \nquery-string:\n  ```\n  fields @data_source_type, @message\n| sort @timestamp asc\n| filter @message like /Starting resource quota controller/\nor @message like /Golang settings/\nor @message like /QuotaMonitor running/\nor @message like /Waiting for caches to sync for resource quota/ \nor @message like /unable to sync caches for resource quota/ \nor @message like /Caches are synced for resource quota/\nor @message like /failed to discover resources/\nor @message like /failed to sync resource monitors/\nor @message like /timed out waiting for quota monitor sync/\n| limit 10000\n  ```\n---\n| @data_source_type | @message |\n| --- | --- |\n| api_server | I1215 04:37:17.387352      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| api_server | I1215 04:37:19.587482      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 04:37:22.933375      10 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 04:37:28.435673      11 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 04:37:40.535459      10 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 04:37:42.211092      12 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 04:42:57.733815      12 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 04:43:01.747163      10 resource_quota_controller.go:300] &quot;Starting resource quota controller&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 04:43:01.747177      10 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | I1215 04:43:01.747347      10 resource_quota_monitor.go:308] &quot;QuotaMonitor running&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 04:43:02.149327      10 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| api_server | I1215 04:43:14.356008      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 04:43:16.812866      11 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 04:43:24.173397      11 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | E1215 04:43:32.151080      10 shared_informer.go:316] &quot;Unhandled Error&quot; err=&quot;unable to sync caches for resource quota&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1215 04:43:32.152649      10 resource_quota_controller.go:498] &quot;Unhandled Error&quot; err=&quot;timed out waiting for quota monitor sync&quot; logger=&quot;UnhandledError&quot; |\n| scheduler | I1215 04:46:56.544854      12 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| api_server | I1215 21:08:19.898600      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 21:08:19.494749      12 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 21:08:25.573153      12 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| api_server | I1215 21:08:26.088071      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 21:08:27.108287      11 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 21:08:32.938447      12 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 21:14:30.305757      11 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 21:14:36.995717      12 resource_quota_controller.go:300] &quot;Starting resource quota controller&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 21:14:36.995736      12 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | I1215 21:14:36.995752      12 resource_quota_monitor.go:308] &quot;QuotaMonitor running&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 21:14:37.132932      12 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| api_server | I1215 21:14:46.771550      12 server.go:149] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| scheduler | I1215 21:14:46.464244      12 server.go:168] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | I1215 21:14:54.091528      13 controllermanager.go:187] &quot;Golang settings&quot; GOGC=&quot;&quot; GOMAXPROCS=&quot;&quot; GOTRACEBACK=&quot;&quot; |\n| controller_manager | E1215 21:15:07.133490      12 shared_informer.go:316] &quot;Unhandled Error&quot; err=&quot;unable to sync caches for resource quota&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1215 21:15:07.134588      12 resource_quota_controller.go:498] &quot;Unhandled Error&quot; err=&quot;timed out waiting for quota monitor sync&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1215 21:18:37.215129      12 resource_quota_controller.go:446] &quot;Unhandled Error&quot; err=&quot;failed to discover resources: Get \\&quot;https:&#x2f;&#x2f;172.16.43.49:443&#x2f;api\\&quot;: dial tcp 172.16.43.49:443: connect: connection refused&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1215 21:18:43.814012      12 shared_informer.go:316] &quot;Unhandled Error&quot; err=&quot;unable to sync caches for resource quota&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | I1215 21:18:49.214463      13 resource_quota_controller.go:300] &quot;Starting resource quota controller&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 21:18:49.214475      13 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | I1215 21:18:49.214494      13 resource_quota_monitor.go:308] &quot;QuotaMonitor running&quot; logger=&quot;resourcequota-controller&quot; |\n| controller_manager | I1215 21:18:49.441625      13 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | E1215 21:19:19.441717      13 shared_informer.go:316] &quot;Unhandled Error&quot; err=&quot;unable to sync caches for resource quota&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1215 21:19:19.442855      13 resource_quota_controller.go:498] &quot;Unhandled Error&quot; err=&quot;timed out waiting for quota monitor sync&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | I1216 02:14:27.734872      13 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | I1216 02:14:27.734916      13 shared_informer.go:320] Caches are synced for resource quota |\n| controller_manager | I1216 02:14:27.814835      13 shared_informer.go:320] Caches are synced for resource quota |\n| controller_manager | I1216 02:28:28.087950      13 shared_informer.go:313] Waiting for caches to sync for resource quota |\n| controller_manager | E1216 02:28:58.088023      13 shared_informer.go:316] &quot;Unhandled Error&quot; err=&quot;unable to sync caches for resource quota&quot; logger=&quot;UnhandledError&quot; |\n| controller_manager | E1216 02:28:58.089226      13 resource_quota_controller.go:498] &quot;Unhandled Error&quot; err=&quot;timed out waiting for quota monitor sync&quot; logger=&quot;UnhandledError&quot; |\n---\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\nClient Version: v1.32.7\nKustomize Version: v5.5.0\nServer Version: v1.32.9-eks-3025e55\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nAWS EKS\n</details>\n\n\n### OS version\n\n<details>\nAmazon Linux arm64\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\ncontainerd\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\nCalico installed by tigeraoperator v3.30.0\namazon-k8s-cni:v1.20.4\n</details>"
  },
  {
    "name": "Kubelet restart causes NotReady pod to be briefly marked Ready before probe workers exist",
    "description": "### What happened?\n\nWhen kubelet restarts, there is a race condition in `prober_manager.go` that causes pods that were NOT Ready before the restart to be briefly marked as Ready. This happens because:\n\n1. During `SyncPod`, `UpdatePodStatus` is called before prober workers are created\n2. The code assumes \"no worker exists = no readiness probe configured\" and marks the container Ready\n3. The pod is updated to Ready in the API server\n4. Prober workers are then created and the first probe runs\n5. The probe fails and the pod is marked NotReady again\n\nThis brief Ready window (we observed ~4 seconds) is enough for PodDisruptionBudget checks to succeed, allowing evictions that should have been blocked.\n\n### Code Location\n\nIn `pkg/kubelet/prober/prober_manager.go`, the `UpdatePodStatus` function (around line 350):\n\n```go\n} else {\n    // The check whether there is a probe which hasn't run yet.\n    w, exists := m.getWorker(pod.UID, c.Name, readiness)\n    ready = !exists // no readinessProbe -> always ready\n```\n\nThe comment says \"no readinessProbe -> always ready\" but `exists` is false in TWO cases:\n1. No readiness probe configured (intended)\n2. Prober worker hasn't been created yet after kubelet restart (potential bug)\n\n### Timeline from Production Incident\n\n| Time (UTC) | Event | Source |\n|------------|-------|--------|\n| 05:13:53 | Pod marked NOT Ready | Kubelet logs |\n| 05:14:25 | Kubelet stops and restarts | systemd logs |\n| 05:14:26 | Pod marked **Ready** | API server audit logs |\n| 05:14:29 | Another pod eviction **succeeds** | API server audit logs |\n| 05:14:30 | Pod marked NOT Ready | API server audit logs |\n\nThe pod was Ready for only 4 seconds, but this was enough for Karpenter (retrying evictions) to successfully evict another pod, causing two pods to be down simultaneously despite PDB `maxUnavailable: 1`.\n\n### Why KEP-4781 Doesn't Fix This\n\n[KEP-4781](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/4781-kublet-restart-pod-status) addresses the opposite problem (Ready\u2192NotReady after restart). \n\nLooking at [scenario-1.png](https://github.com/kubernetes/enhancements/raw/master/keps/sig-node/4781-kublet-restart-pod-status/scenario-1.png) from the KEP:\n\n1. During \"First SyncPod Execution\", the pod is marked Ready=true and API server is updated\n2. The fix's precondition for \"First Probe Execution\" is \"container Ready status is true\"\n3. But for pods that were NOT Ready before restart, the damage is already done - the API server was updated with Ready=true during SyncPod\n\nThe KEP fix preserves Ready status for pods that WERE Ready. It doesn't prevent the incorrect Ready=true update during SyncPod for pods that were NOT Ready.\n\n### What did you expect to happen?\n\nPods that were NOT Ready before kubelet restart should remain NOT Ready until probes pass. The kubelet should not assume \"no worker = no probe configured\" immediately after restart.\n\n### How can we reproduce it?\n\n1. Create a pod with a readiness probe that initially fails (e.g., checks for a file that doesn't exist)\n2. Wait for pod to be marked NotReady\n3. Restart kubelet on that node\n4. Observe the pod briefly becoming Ready before the first probe runs and fails\n\n### Anything else we need to know?\n\n**Suggested Fix:**\nDuring `UpdatePodStatus` after kubelet restart, if:\n- No probe result exists in readinessManager\n- No probe worker exists yet\n- But the pod spec HAS a readiness probe configured\n\nThen the container should be marked NOT Ready (or preserve the previous Ready state from API server) rather than assuming Ready.\n\n### Kubernetes version\n\nObserved in production on EKS 1.33. Code analysis shows the issue exists in current master.\n\n### Cloud provider\n\nAWS (EKS)\n\n### OS version\n\nAmazon Linux 2023\n\n### Related Issues\n\n- #100277 - Pod temporarily reports containerNotReady after restart (opposite direction)\n- #78733 - Pod with readinessProbe becomes not ready on restart (opposite direction)\n- KEP-4781 - Kubelet Restart Pod Status (addresses opposite problem)"
  },
  {
    "name": "Go 1.24.11 and 1.25.5 Cannot Build Projects Using Kubernetes Due to gogo/protobuf",
    "description": "### What happened?\n\nProjects using Kubernetes client libraries cannot upgrade to Go 1.24.11 or 1.25.5, which include fixes for **CVE-2025-61729** (crypto/x509), because these versions fail during `go mod tidy`/`go mod vendor` with gogo/protobuf errors.\n\n\n## Affected Versions\n\n**Go versions with CVE-2025-61729 fix (FAIL with Kubernetes):**\n- Go 1.24.11\n- Go 1.25.5\n\n**Last working Go versions (VULNERABLE to CVE-2025-61729):**\n- Go 1.24.10\n- Go 1.25.4\n\n**Kubernetes versions tested:**\n- k8s.io/client-go v0.33.1\n- k8s.io/client-go v0.34.1\n\nBoth Kubernetes versions fail with Go 1.24.11 and Go 1.25.5 due to stricter module validation that correctly identifies gogo/protobuf's broken module structure.\n\n## Root Cause\n\n- `gogo/protobuf` has broken module structure\n- Go 1.24.11 and 1.25.5 include stricter module validation (correct behavior)\n- Kubernetes still depends on `gogo/protobuf` (via `openshift/api` and others)\n\n## Questions\n\n1. **When will Kubernetes complete the gogo/protobuf removal?**\n2. **Which Kubernetes version will be compatible with Go 1.24.11 and 1.25.5?**\n3. **What should projects do about CVE-2025-61729 in the meantime?**\n\n## Impact\n\n- **CVE-2025-61729:** crypto/x509 vulnerability (Certificate.Verify and Certificate.VerifyHostname)\n- **Fixed in:** Go 1.24.11 and Go 1.25.5 (December 2, 2025)\n- **Problem:** Cannot apply these fixes while using Kubernetes v0.33.x or v0.34.x\n- **Compliance:** Security scanners flag CVE-2025-61729, blocking compliance audits\n- **Scope:** Affects all projects using k8s.io/client-go, k8s.io/api, k8s.io/apimachinery\n\n## Request\n\nThis is a critical security blocker. Please provide:\n1. Timeline for gogo/protobuf removal\n2. Compatible Kubernetes version\n3. Guidance for projects that need both current K8s and CVE fixes\n\n## Related\n\n- CVE-2025-61729 - https://pkg.go.dev/vuln/GO-2025-3468\n\n### What did you expect to happen?\n\nI expected `go mod tidy` and `go mod vendor` to succeed when using Go 1.24.11 \nor Go 1.25.5 with Kubernetes v0.33.1 and v0.34.1, just as they work with \nearlier Go versions (1.24.10 and 1.25.4).\n\nProjects should be able to upgrade to the latest Go patch releases to receive \ncritical security fixes (like CVE-2025-61729) without breaking their Kubernetes \ndependencies.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\n## Reproduction\n\n```bash\nmkdir /tmp/k8s-go-test && cd /tmp/k8s-go-test\n\ncat > go.mod << 'EOF'\nmodule example.com/k8s-test\ngo 1.24.11\nrequire (\n    k8s.io/api v0.33.1\n    k8s.io/apimachinery v0.33.1\n    k8s.io/client-go v0.33.1\n)\nEOF\n\ncat > main.go << 'EOF'\npackage main\nimport (\n    corev1 \"k8s.io/api/core/v1\"\n    \"fmt\"\n)\nfunc main() {\n    fmt.Println(&corev1.Pod{})\n}\nEOF\n\ngo mod tidy  # Fails with Go 1.24.11 or 1.25.5\n```\n\n**Error:**\n```\ngo: module github.com/gogo/protobuf@latest found (v1.3.2), \nbut does not contain package github.com/gogo/protobuf/proto\n```\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "test/e2e should compile identically for all platforms",
    "description": "See: https://github.com/kubernetes/kubernetes/pull/132791/changes#r2615407057\n\nWe currently have test code that only compiles for linux.\n\nEven if you are on windows or mac, you should be able to run the test logic locally against a remote cluster that is on Linux (or perhaps Linux + Windows).\n\nTest logic has no business being OS specific because it should not be interacting with the local OS, and logic to interact with the remote hosts should be protable.\n\nI'm not sure if we have other instances of this, and what we might do to prevent it (linting for build tags under test/e2e?)\n\ncc @kubernetes/sig-testing-leads \n\n/sig node testing\n\nAttempt at fixing known instance:\nhttps://github.com/kubernetes/kubernetes/pull/135739"
  },
  {
    "name": "Possible null pointer dereference",
    "description": "Below is a snippet of code where we check the pointer `options` for null, and then dereference it if it is null. This is a theoretical dereference of a null pointer.\nhttps://github.com/kubernetes/kubernetes/blob/c180d6762d7ac5059d9b50457cafb0d7f4cf74a9/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L1429-L1433"
  },
  {
    "name": "NominatedNodeName is cleared on Bind failure leading to unschedulable pods",
    "description": "### What happened?\n\n/sig scheduling\n\nI was verifying a scenario where two pods with topology spread and PVCs with WaitForFirstConsumer storage got their PVCs assigned to the same node, resulting in one of the pods becoming unschedulable due to conflicting rules (topology spread prevents using the same node, volume binding enforces using that node).\n\nThe suspected root cause was a pod binding failure or scheduler restart, something like:\n\n1. Pod#1's PVC is bound to Node#1\n2. Pod#1 fails to bind\n3. Pod#2's PVC is bound to Node#1\n4. Pod#2 is bound to Node#1\n5. Pod#1 is unschedulable because it has to use Node#1 due to the PVC binding but can't use it due to pod topology spread\n\nI wrote an integration test to repro, and it's still reproducible even with the `NominatedNodeNameForExpectation` feature enabled.\n\nUpon further investigation, it seems that the NominatedNodeName is cleared on binding error, so scheduler isn't aware of Pod#1 initial assignment when scheduling Pod#2. https://github.com/kubernetes/kubernetes/blob/c180d6762d7ac5059d9b50457cafb0d7f4cf74a9/pkg/scheduler/schedule_one.go#L387\n\n### What did you expect to happen?\n\nI was expecting Pod#2 to be scheduled on another node because the scheduler should be aware of the previous, partially failed, retriable binding of Pod#1.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSee https://github.com/kubernetes/kubernetes/pull/135854 (2 attempts case)\n\nA test can follow these steps:\n\n1. Create 2 nodes of same sizes\n2. Create storage with WaitForFirstConsumer binding mode\n3. Create pvc using dynamic binding (pvc#1), and a pod using the pvc (pod#1). The pod has preferred affinity for node#1 and takes up all of its resources.\n3. Set up Bind plugin that will fail to bind pod#1 in the next cycle\n4. Run scheduler\n5. Expect: pvc#1 has selected-node annotation equal to node#1 (OK), pod#1 fails to bind (OK), pod#1 has nominated node name equal to node#1 (FAIL, nominated node name seems to be cleared)\n\nTo fully validate the scenario in the bug, add the following steps afterwards:\n\n1. Tear down the scheduler\n2. Add another pvc using dynamic binding (pvc#2) and a pod using that pvc (pod#2). The pod also has preferred affinity for node#1\n3. Set up QueueSort plugin that will force pod#2 to be scheduled before pod#1\n4. Run scheduler\n5. Expect: pod#2 is scheduled on node#2 (FAIL, actual node#1), pod#1 is scheduled on node#1 (FAIL, unschedulable)\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\nAt least up to 1.35 inclusive\n\n### Cloud provider\n\nIndependent\n\n### OS version\n\n_No response_\n\n### Install tools\n\n\n\n\n### Container runtime (CRI) and version (if applicable)\n\n_No response_\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n_No response_"
  },
  {
    "name": "The kubelet's business container is already up and running; currently, the init container is being created.",
    "description": "### What happened?\n\n\n1. Time Synchronization Causing Metadata Conflict\nAfter the node rebooted, NTP synchronization adjusted the system time forward by 10 seconds.\nThe Kubelet, after the node restart, attempted to recreate the Pod's sandbox and init containers. However, due to the time rollback, the CreationTimestamp of the newly created init container could be earlier than the previously uncompleted init container from before the reboot.\nKubernetes uses the CreationTimestamp, container name, and attempt number to uniquely identify containers. When the CreationTimestamp and attempt of the new and old containers conflict (e.g., the new one appears older), the container runtime (e.g., containerd or CRI-O) rejects creating duplicate containers.\n2. Abnormal Kubelet Behavior\nThe Kubelet, during Pod reconciliation, mistakenly identifies the stale (stopped) init container as still belonging to the current Pod. This causes it to repeatedly attempt to recreate the same-named container.\nDue to the time rollback, the metadata (e.g., timestamps and attempts) of the new and old containers clash. This results in an infinite loop where the Pod remains stuck in the Init:0/1 state.\n\n```\nfunc findContainerStatusByName(status *v1.PodStatus, name string) (*v1.ContainerStatus, error) {\n\tfor _, containerStatus := range append(status.InitContainerStatuses, status.ContainerStatuses...) {\n\t\tif containerStatus.Name == name {\n\t\t\treturn &containerStatus, nil\n\t\t}\n\t}\n\treturn nil, fmt.Errorf(\"unable to find status for container with name %v in pod status (it may not be running)\", name)\n}\n```\nThis is the process of retrieving the init container itself. It retrieves containers returned from runtime, sorted by time, and returns the one with the same name. However, it seems that the selection of which init container to use for the current pod shouldn't be based on time, but rather on an attempt mechanism.\n\n### What did you expect to happen?\n\nPod is running not init,The pod is in normal working order.\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nsame as 'what happened'\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\nv1.34\n```console\n$ kubectl version\n# paste output here\n```\n\n</details>\n\n\n### Cloud provider\n\n<details>\nNA\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "Analysis of Kubelet Init Container Restart Count Issue",
    "description": "### What happened?\n\nThe user created a pod with an init container. After the init container was created but before it started, the system rebooted.\n\nAfter the system rebooted, clock synchronization was performed, causing the time to jump forward by several tens of seconds. At this point, the kubelet recreated the sandbox and init container and started it. The start time of the newly launched init container was earlier than that of the init container in the \"created\" state that existed before the reboot.\n\nDuring subsequent operations, the status of the init container was retrieved, but it corresponded to the container created before the reboot. When attempting to create a new one, the restart count of the container was actually undercounted by 1. This caused the newly created container to have the same sequence number as the one after the reboot, preventing it from starting at all.\n\nI am not sure whether this qualifies as an issue.\n\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/container/runtime.go#L424-L433\n```golang\n// FindContainerStatusByName returns container status in the pod status with the given name.\n// When there are multiple containers' statuses with the same name, the first match will be returned.\nfunc (podStatus *PodStatus) FindContainerStatusByName(containerName string) *Status {\n\tfor _, containerStatus := range podStatus.ContainerStatuses {\n\t\tif containerStatus.Name == containerName {\n\t\t\treturn containerStatus\n\t\t}\n\t}\n\treturn nil\n}\n```\n\n### What did you expect to happen?\n\npod can be run normal\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nThis is an intermittent issue, perhaps it can be reproduced through mock testing.\n\n### Anything else we need to know?\n\n_No response_\n\n### Kubernetes version\n\n<details>\n\n```console\n$ kubectl version\n# paste output here\n```\n1.34.0\n</details>\n\n\n### Cloud provider\n\n<details>\n\n</details>\n\n\n### OS version\n\n<details>\n\n```console\n# On Linux:\n$ cat /etc/os-release\n# paste output here\n$ uname -a\n# paste output here\n\n# On Windows:\nC:\\> wmic os get Caption, Version, BuildNumber, OSArchitecture\n# paste output here\n```\n\n</details>\n\n\n### Install tools\n\n<details>\n\n</details>\n\n\n### Container runtime (CRI) and version (if applicable)\n\n<details>\n\n</details>\n\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\n<details>\n\n</details>"
  },
  {
    "name": "kube-apiserver crash: concurrent map read and map write in CEL ValidatingAdmissionPolicy evaluation",
    "description": "### What happened?\n\nThe kube-apiserver crashed with a fatal error `concurrent map read and map write` during CEL expression evaluation for ValidatingAdmissionPolicy.\n\nThe crash occurs in `k8s.io/apiserver/pkg/cel/types.go:463` inside `FindStructFieldType` when reading from a `DeclType.Fields` map that is being concurrently written to.\n\n**Stack trace (reader goroutine):**\n```\nfatal error: concurrent map read and map write\n\ngoroutine 11296835574 [running]:\ninternal/runtime/maps.fatal({0x3a59a3a?, 0xc633d97608?})\n  runtime/panic.go:1058 +0x18\nk8s.io/apiserver/pkg/cel.(*DeclTypeProvider).FindStructFieldType(0xc026d9dce0, {0x3a32069, 0x14}, {0xc07763b2e0, 0xc})\n  k8s.io/apiserver/pkg/cel/types.go:463 +0x74\nk8s.io/apiserver/pkg/cel.(*DeclTypeProvider).FindStructFieldType(0xc045262180, {0x3a32069, 0x14}, {0xc07763b2e0, 0xc})\n  k8s.io/apiserver/pkg/cel/types.go:460 +0x21f\ngithub.com/google/cel-go/interpreter.(*attrFactory).NewQualifier(...)\n  github.com/google/cel-go@v0.23.2/interpreter/attributes.go:226 +0x9f\n...\nk8s.io/apiserver/pkg/admission/plugin/cel.(*CompositedConditionEvaluator).ForInput(...)\n  k8s.io/apiserver/pkg/admission/plugin/cel/composition.go:188 +0x189\nk8s.io/apiserver/pkg/admission/plugin/policy/validating.(*validator).Validate(...)\n  k8s.io/apiserver/pkg/admission/plugin/policy/validating/validator.go:122 +0x772\n...\nk8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func2()\n  k8s.io/apiserver/pkg/endpoints/handlers/patch.go:704 +0xa7\n```\n\nI haven't managed to capture the writer, but I suspect it is the `AddField` method since it writes to `c.MapType.Fields[name]`\n\n**Root Cause guess:**\n\nI'm guessing there is a race condition in `staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/composition.go`. The `NewCompositedCompilerFromTemplate` function shares the same `MapType` (and its `Fields` map) across all `CompositedCompiler` instances:\n\n```go\n// composition.go:72-77\nfunc NewCompositedCompilerFromTemplate(context *CompositionEnv) *CompositedCompiler {\n  context = &CompositionEnv{\n    MapType:           context.MapType,  // SHARED - same pointer!\n    EnvSet:            context.EnvSet,\n    CompiledVariables: map[string]CompilationResult{},\n  }\n```\n\nThe singleton template is created via `getCompositionEnvTemplateWithStrictCost()` in `plugin.go:49-55`.\n\nWhen multiple goroutines process admission requests concurrently:\n- **Reader**: `FindStructFieldType` at `types.go:463` reads from `st.Fields[fieldName]`\n- **Writer**: `AddField` at `composition.go:127` writes to `c.MapType.Fields[name]`\n\n### What did you expect to happen?\n\nNo fatal errors\n\n### How can we reproduce it (as minimally and precisely as possible)?\n\nSorry to say I don't have a great answer here. My best guess at the root cause points towards:\n\n1. Create a ValidatingAdmissionPolicy with CEL expressions that use variables\n2. Generate high concurrent load to the API server (multiple PATCH/UPDATE requests) that trigger that policy while also updating VAPs (triggering further compilation)\n\n### Anything else we need to know?\n\nThe crash was observed in a production environment under normal operational load. The error message preceding the crash was:\n```\nE1212 20:54:45.514642 12 finisher.go:175] \"Unhandled Error\" err=\"FinishRequest: post-timeout activity - time-elapsed: 1.691904645s, panicked: false, err: context canceled, panic-reason: <nil>\" logger=\"UnhandledError\"\n```\n\n\n### Kubernetes version\n\n```\n$ kubectl version\nClient Version: v1.33.5\nServer Version: v1.33.5\n```\n\n### Cloud provider\n\nAWS EKS\n\n### OS version\n\nnot sure (whatever EKS uses, presumably Amazon Linux 2023)\n\n### Install tools\n\nn/a\n\n### Container runtime (CRI) and version (if applicable)\n\nn/a\n\n### Related plugins (CNI, CSI, ...) and versions (if applicable)\n\nn/a"
  },
  {
    "name": "Migrate \"Good First Migration\" Required Fields to Declarative Validation",
    "description": "### What would you like to be added?\n\nThis issue tracks a number of identified \"good first migration\" cases from handwritten Go code to Declarative Validation tags (+k8s:required) as part of KEP-5073.  This issue aims to be a more narrowed down list with specific fields and validation tags than the list of types tracked in https://github.com/kubernetes/kubernetes/issues/134280. The goals is to have users more easily able to aid in the migration by first having a set of \"Good First Migration\" cases with minimal migration friction and build from there.\n\nFor more information on the tasks here, the Declarative Validation work group (guiding this and similar issues) has open discussion in kubernetes Slack channel: #sig-api-machinery-dev-tools\n\nWe are targeting fields that meet the \"Good First Migration\" criteria:\n1.  **Validation Logic:** Simple `Required` checks (e.g., `if len(field) == 0 { return Required }` or `if ptr == nil { return Required }`).\n2.  **Versioning:** The field exists at the same path across all API versions (typically v1).\n3.  **Nesting:** The field is close to the root of the struct or part of a struct that is **always** validated (not a pointer that might be nil). This avoids having to check field path parents for short-circuiting logic.\n4.  **Not Optional:** The field is not marked `+optional` in `types.go`.\n5.  **Status:** The field has not yet been migrated (does not currently have a `+k8s:` tag).\n\n### PR Structure\nThe ideal PR structure for a PR contributing a DV tag migration is:\n\nCommit # 1: IF NOT ALREADY WIRED UP FOR DV - Wire up the apigroup, add empty test and empty generated file. \n\nCommit # 2: Use a single DV tag on a single field. E.g. Add `+k8s:required` to required field. Includes generated code and robust test cases. Emphasis on test-cases -- we want enough to prove that a field is correct, but we don't want to duplicate all the per-tag tests (e.g. a k8s-short-name field has dozens of corner cases, but we really only need a few). We want each case to be as short and specific as possible, which is why the \"tweak\" pattern and the use of Origin for Invalid() errors is so important. Specify as little as possible to be sufficiently precise. Newcomers need guidance on this.\n\nCommits # 3...N: Use DV tags on the same field, one per commit; repeat until that field is done or all that is left is not handled by DV yet. Includes generated code and tests for each field. (not the case for the specific examples below as they are for single tag migrations)\n\nCommits N+1...M: Repeat the above for more fields. (not the case for the specific examples below as they are for single field migrations)\n\nThe emphasis is on lots of small commits. Maintaining commit discipline is hard but worthwhile.\n\nIf DV exposes bugs in the existing testing or the hand-written validation itself (!!), those should be fixed as a PR of its own or as commits at the front of the PR.\n\n### Resources\n\n*   **KEP:** [KEP-5073: Declarative Validation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/5073-declarative-validation)\n*   **Migration Guide:** [Review the \"Migrating Handwritten Validation\" section in the README](https://github.com/jpbetz/validation-gen/blob/main/staging/src/k8s.io/code-generator/cmd/validation-gen/README.md#2-migrating-handwritten-validation-to-declarative-validation)\n\n### Task List\n\nThe following fields have been identified as good candidates for the initial migration.\n\n\n#### Storage (`storage.k8s.io`)\n\n*   [ ] **VolumeAttachment.Spec.Attacher**\n    *   **Field:** `Spec.Attacher` (string)\n    *   **Location:** `staging/src/k8s.io/api/storage/v1/types.go`\n    *   **Current Validation:** `pkg/apis/storage/validation/validation.go` (`validateAttacher`)\n    *   **Analysis:** `Attacher` is a direct string field on `VolumeAttachmentSpec`. It is not marked optional. Validation checks `len(attacher) == 0`. `VolumeAttachment` is a top-level resource. Safe to migrate.\n\n#### AdmissionRegistration (`admissionregistration.k8s.io`)\n\n*   [ ] **ValidatingAdmissionPolicyBinding.Spec.PolicyName**\n    *   **Field:** `Spec.PolicyName` (string)\n    *   **Location:** `staging/src/k8s.io/api/admissionregistration/v1/types.go`\n    *   **Current Validation:** `pkg/apis/admissionregistration/validation/validation.go` (`validateValidatingAdmissionPolicyBindingSpec`)\n    *   **Analysis:** `PolicyName` is a direct string field. It is not marked optional. Validation checks `len(spec.PolicyName) == 0`. Safe to migrate.\n\n*   [ ] **ValidatingAdmissionPolicyBinding.Spec.ValidationActions**\n    *   **Field:** `Spec.ValidationActions` ([]ValidationAction)\n    *   **Location:** `staging/src/k8s.io/api/admissionregistration/v1/types.go`\n    *   **Current Validation:** `pkg/apis/admissionregistration/validation/validation.go` (`validateValidationActions`)\n    *   **Analysis:** This is a slice field. It is not marked optional. Validation checks `len(actions) == 0`. Safe to migrate.\n\n#### FlowControl (`flowcontrol.apiserver.k8s.io`)\n\n*   [ ] **FlowSchema.Spec.PriorityLevelConfiguration.Name**\n    *   **Field:** `Spec.PriorityLevelConfiguration.Name` (string)\n    *   **Location:** `staging/src/k8s.io/api/flowcontrol/v1/types.go`\n    *   **Current Validation:** `pkg/apis/flowcontrol/validation/validation.go` (`ValidateFlowSchemaSpec`)\n    *   **Analysis:** `PriorityLevelConfiguration` is a struct type (not a pointer) embedded in `Spec`, so it is always present. `Name` inside it is a string and not optional. Validation checks for empty name. The struct `PriorityLevelConfigurationReference` appears to only be used in this context. Safe to migrate.\n\n#### Autoscaling (`autoscaling/v1`)\n\n*   [ ] **HorizontalPodAutoscaler.Spec.ScaleTargetRef.Kind**\n    *   **Field:** `Spec.ScaleTargetRef.Kind` (string)\n    *   **Location:** `staging/src/k8s.io/api/autoscaling/v1/types.go`\n    *   **Current Validation:** `pkg/apis/autoscaling/validation/validation.go` (`ValidateCrossVersionObjectReference`)\n    *   **Analysis:** `ScaleTargetRef` is a struct (not a pointer) in `HorizontalPodAutoscalerSpec`, so it is always traversed. `Kind` is a string field in `CrossVersionObjectReference`. It is not marked optional. Validation checks `len(ref.Kind) == 0`.\n    *   **Hierarchy Note:** `CrossVersionObjectReference` is also used in v2 `MetricSpec` (`ObjectMetricSource`). In both cases, `Kind` is required. Adding the tag to the v1 struct definition is safe and correct.\n\n*   [ ] **HorizontalPodAutoscaler.Spec.ScaleTargetRef.Name**\n    *   **Field:** `Spec.ScaleTargetRef.Name` (string)\n    *   **Location:** `staging/src/k8s.io/api/autoscaling/v1/types.go`\n    *   **Current Validation:** `pkg/apis/autoscaling/validation/validation.go` (`ValidateCrossVersionObjectReference`)\n    *   **Analysis:** Same as `Kind`. `Name` is required in all usages of this struct.\n\n#### Certificates (`certificates.k8s.io`)\n\n*   [ ] **CertificateSigningRequest.Spec.Usages**\n    *   **Field:** `Spec.Usages` ([]KeyUsage)\n    *   **Location:** `staging/src/k8s.io/api/certificates/v1/types.go`\n    *   **Current Validation:** `pkg/apis/certificates/validation/validation.go` (`validateCertificateSigningRequest`)\n    *   **Analysis:** `Usages` is a slice. It is not marked optional. Validation checks `len(csr.Spec.Usages) == 0`. Safe to migrate.\n\n\n### How to Migrate a Field\n\n1.  **Locate the Versioned Type:** Find the `types.go` file for the versioned API (e.g., `staging/src/k8s.io/api/storage/v1/types.go`).\n2.  **Add the Tag:** Add `// +k8s:required` above the field definition.\n3.  **Generate:** Run `hack/update-codegen.sh validation`.\n4.  **Update Strategy:** In the resource's `strategy.go` (usually in `pkg/registry/<group>/<resource>/`), update the `Validate` function to use `rest.ValidateDeclarativelyWithMigrationChecks`.\n5.  **Verify:**\n    *   Ensure unit tests pass.\n    *   Run `declarative_validation_test.go` (add one if missing) to prove equivalence.\n6.  **Mark Covered:** In the original `validation.go`, find the manual check and append `.MarkCoveredByDeclarative()`.\n\nExample of marking hand-written validation code:\n```go\nif len(t.Provisioner) == 0 {\n    allErrs = append(allErrs, field.Required(fldPath.Child(\"provisioner\"), \"\")).MarkCoveredByDeclarative()\n}\n```\n\n7. **If new API group being plumbed for DV** - wire up API group and all associated versions for fuzz testing in [pkg/api/testing/validation_test.go](https://github.com/kubernetes/kubernetes/blob/9720186a466cc627b0417433fb1f66cd1dd96f94/pkg/api/testing/validation_test.go#L32-L50)\nhttps://github.com/kubernetes/kubernetes/blob/9720186a466cc627b0417433fb1f66cd1dd96f94/pkg/api/testing/validation_test.go#L32-L50\n\n### Why is this needed?\n\nThis helps onboard the community to DV, improves the APIs touched, and helps in the process of getting Declarative Validation to it's stated goal of migrating ~50% of existing hand-written code to DV over 5 releases -> improved API readability and more declarative APIs"
  },
  {
    "name": "[DevTools Bug]:",
    "description": "### Website or app\n\nhttp://localhost:3000/en/p2p/purchaseOrders/purchase-order\n\n### Repro steps\n\nhttp://localhost:3000/en/p2p/purchaseOrders/purchase-order\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```"
  },
  {
    "name": "Bug: ViewTransition can't be isolated",
    "description": "React version:\n    \"react\": \"19.2.3\",\n    \"react-dom\": \"19.2.3\",\n\n## Steps To Reproduce\n\nI have a Dialog wrapped with ViewTransition. Inside the dialog I\u2019m using the Streamdown library.\n\nStreamdown updates its blocks using startTransition. As a result, view transitions keep triggering while Streamdown is streaming.\n\nIt seems like there\u2019s no way to isolate part of the tree so that startTransition inside the dialog doesn\u2019t trigger transitions outside of it.\n\nI can\u2019t disable ViewTransition on the dialog during streaming, because the dialog needs the close animation.\n\nI think view transitions must be executed somehow explicitly and not via startTransition itself.\nOr I need a way like \n```\n<Isolate>\n  ...startTransitions here would not cause view transitions to start\n</Isolate>\n```\n\nPS: `addTransitionType`\nAnd \n```\n::view-transition-group(*) {\n  animation-duration: 0ms;\n  animation: none;\n}\n...\n:root:active-view-transition-type(video-grid-dialog) {\n...\n}\n```\ndoesn't help, chrome flickers anyway"
  },
  {
    "name": "Bug: React 19 does not attach custom element event listeners during hydration",
    "description": "When hydrating server-rendered markup, React does not attach event listeners for custom element events that have been configured on the custom element using the React's `on<custom event name>` prop syntax. The event handler is only attached after the first client render, not during hydration. This issue affects all React SSR frameworks like Next.js (linking a related issue from Next.js: https://github.com/vercel/next.js/issues/84091 - note that the issue is on React side).\n\nReact version: 19.2.3\n\n## Steps To Reproduce\n\n1. Open the CodeSandbox example.\n2. Click the first \"Emit custom event\" button.\n2. Click the second \"Emit custom event\" button.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example: https://codesandbox.io/p/devbox/thirsty-leaf-4n6r2t\n\n### Description of the demo\n\nThe CodeSandbox simulates SSR + hydration using only React:\n\n- Server HTML is generated via `renderToString`\n- The markup is injected into the DOM\n- `hydrateRoot` is used to hydrate the content\n\nA custom element `<my-element>`:\n\n- Uses Shadow DOM\n- Dispatches a CustomEvent(\"my-event\") when clicking the \"Emit custom event\" button\n\nEach React component (NotWorking & WorksWithForcedRerender) renders:\n\n```js\n<my-element\n  onmy-event={() => {\n    console.log(\n      \"React custom event handler fired from <component name> component\"\n    );\n    alert(\"React custom event handler fired from <component name> component\");\n  }}\n/>\n```\n\n## The current behavior\n\nThe `onmy-event` prop on `<my-element>` in `<NotWorking>` React component does not call the configured callback function and does not cause a native alert to pop up because React does properly attach event listener for the `onmy-event` prop during hydration.\n\nHowever, the `onmy-event` prop on `<my-element>` in `<WorksWithForcedRerender>` React component properly calls the configured callback function and causes a native alert to pop up by using a \"forced re-render on mount\" workaround.\n\n## The expected behavior\n\nThe `onmy-event` prop on `<my-element>` in `<NotWorking>` React component should call the configured callback function and cause a native alert to pop up when clicking the \"Emit custom event\" button inside the `<NotWorking>` component ."
  },
  {
    "name": "Leave me alone",
    "description": "https://github.com/facebook/react/pull/35409\n\nThere are junior developers: @josephsavona, @ryancavanaugh, @facebook, @microsoft, @tiktok harassing me. \n\n@josephsavona renamed my pull request to Spam? Delete the pull request."
  },
  {
    "name": "Bug:",
    "description": "Title: Consider built-in routing support in React core\n\nReact is an amazing library and has become the backbone of modern web applications.\n\nOne common requirement in almost every React web app is routing. Currently, developers must rely on third-party libraries such as react-router-dom. While these libraries are excellent, routing is such a fundamental need that it might be worth considering some level of built-in routing support in React itself.\n\nHaving an official, minimal router maintained alongside React could:\n\t\u2022\tReduce dependency on third-party packages\n\t\u2022\tImprove beginner experience\n\t\u2022\tEnsure tighter integration with React features and future updates\n\nThank you for building such an awesome ecosystem and for continuously improving React."
  },
  {
    "name": "react-reconciler: flushSync was renamed to flushSyncFromReconciler without deprecation notice",
    "description": "In react-reconciler, the `flushSync` export was renamed to `flushSyncFromReconciler` in PR #28500 (merged April 8, 2024). This is an undocumented breaking change for custom renderers.\n\n**Commit:** https://github.com/facebook/react/pull/28500\n\nAccording to that PR, custom renderers should now use:\n- `reconciler.updateContainerSync()` - to update synchronously\n- `reconciler.flushSyncWork()` - to flush pending sync work\n\nHowever, this migration path is not documented anywhere for custom renderer authors. The change silently causes `flushSync` to be `undefined` with no warning.\n\n**0.29.x API:**\n```js\nconst reconciler = ReactReconciler(hostConfig)\nconst { flushSync } = reconciler  // works\n\n// Usage\nflushSync(() => {\n  reconciler.updateContainer(element, container, null, callback)\n})\n```\n\n**0.31.0+ API:**\n```js\nconst reconciler = ReactReconciler(hostConfig)\nconst { flushSync } = reconciler  // undefined!\n\n// New intended usage (undocumented):\nreconciler.updateContainerSync(element, container, null, callback)\nreconciler.flushSyncWork()\n```\n\nCould we please:\n1. Document the new API for custom renderers in the react-reconciler README\n2. Or add a deprecation warning when accessing the old `flushSync` property\n\nThis affects custom renderers like @opentui/react that were following the previous patterns."
  },
  {
    "name": "[DevTools Bug] Cannot reorder children for suspense node \"333\" because no matching node was found in the Store.",
    "description": "### Website or app\n\nhttp://localhost:3000/auth/default-prompt\n\n### Repro steps\n\nStart local project nextjs 15.2.8\nF12 => Component tabs => error\n\n### How often does this bug happen?\n\nSometimes\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n7.0.1-3cde211b0c\n\n### Error message (automated)\n\nCannot reorder children for suspense node \"333\" because no matching node was found in the Store.\n\n### Error call stack (automated)\n\n```text\nat chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:730986\n    at p.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:680330)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:682241\n    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1189368)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Cannot reorder children for suspense node  because no matching node was found in the Store. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```"
  },
  {
    "name": "[DevTools Bug] Commit tree already contains fiber \"536\". This is a bug in React DevTools.",
    "description": "### Website or app\n\nnone\n\n### Repro steps\n\n1\uff1aClicking the right side to turn pages results in an error\n\n<img width=\"179\" height=\"32\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f30325ef-665c-437b-b8c0-9f6903d91d1a\" />\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-extensions\n\n### DevTools version (automated)\n\n7.0.1-3cde211b0c\n\n### Error message (automated)\n\nCommit tree already contains fiber \"536\". This is a bug in React DevTools.\n\n### Error call stack (automated)\n\n```text\nat updateTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:699600)\n    at getCommitTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:698832)\n    at $e.getCommitTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:703384)\n    at CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1005870)\n    at renderWithHooks (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:66940)\n    at updateFunctionComponent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:97513)\n    at beginWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:111594)\n    at performUnitOfWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:184246)\n    at workLoopSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:184102)\n    at renderRootSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:183886)\n```\n\n### Error component stack (automated)\n\n```text\nat CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1005671)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:879909)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1144384\n    at ao (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:898411)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:901116)\n    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:901313\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at div (<anonymous>)\n    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:901116)\n    at SuspenseTreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1147064)\n    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:915935)\n    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:994422)\n    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:986233)\n    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:786019)\n    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:815755)\n    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:972480)\n    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1175879)\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Commit tree already contains fiber . This is a bug in React DevTools. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```"
  },
  {
    "name": "Bug: resolveLazy catching promises causes race condition with short-lived Suspense-wrapped components",
    "description": "## Steps To Reproduce\n\n1. Create a React Server Component with a short async delay (~5ms) wrapped in Suspense\n2. Trigger a revalidation (e.g., via revalidatePath() in Next.js)\n3. The server sends correct updated data, but the client UI does not update\n\nLink to code example: https://github.com/vercel/next.js/issues/87529\n\n## The current behavior\n\nAfter calling revalidatePath(), the browser receives the correct Flight data with the updated counter value, but the UI remains stuck showing the old value. This only happens:\n- In production mode\n- With short async delays (~5ms)\n- When the component is wrapped in Suspense\n\nhttps://github.com/user-attachments/assets/85e3ccaa-75af-4bed-859e-972c30e50904\n\nThe issue was introduced in commit that upgraded React from `eaee5308-20250728` to `9be531cd-20250729`.\n\n## Root cause\n\nThe new resolveLazy function in ReactChildFiber.js catches promises and throws SuspenseException:\n\n```ts\nexport function resolveLazy<T>(lazyType: LazyComponentType<T, any>): T {\n  try {\n    if (__DEV__) {\n      return callLazyInitInDEV(lazyType);\n    }\n    const payload = lazyType._payload;\n    const init = lazyType._init;\n    return init(payload);\n  } catch (x) {\n    if (x !== null && typeof x === 'object' && typeof x.then === 'function') {\n      // This lazy Suspended. Treat this as if we called use() to unwrap it.\n      suspendedThenable = x;\n      if (__DEV__) {\n        needsToResetSuspendedThenableDEV = true;\n      }\n      throw SuspenseException;\n    }\n    throw x;\n  }\n}\n```\n\nWhen a lazy component's init() throws a short-lived promise (~5ms), this creates a race condition:\n1. resolveLazy catches the promise and stores it in suspendedThenable\n2. It throws SuspenseException to signal suspension\n3. The promise resolves before React has finished setting up its subscription\n4. React remains suspended, waiting for a signal that already passed\n5. The UI never updates\n\n## Previous working version\n\n```ts\nfunction resolveLazy(lazyType: any) {\n  if (__DEV__) {\n    return callLazyInitInDEV(lazyType);\n  }\n  const payload = lazyType._payload;\n  const init = lazyType._init;\n  return init(payload);\n}\n```\n\nWithout the try-catch, promises propagate naturally through React's existing Suspense machinery, which handles them correctly.\n\nThe expected behavior\n\nAfter revalidatePath() is called:\n1. The server sends updated Flight data\n2. React reconciles the new data\n3. The UI updates to show the new counter value"
  },
  {
    "name": "Bug: Nested providers for the same context will lead to unnecessary render calls when only the outside context value changed  (React 19 only)",
    "description": "Since React 19, nested providers for the same context will lead to unnecessary render calls for some scenarios. I wrote a unit test and confirmed that React 18.3.1 behaves as expected.\n\nE.g. for the following pseudo-code React component structure:\n```tsx\n<SomeContextProvider>\n  <SomeContextConsumer>\n    <SomeContextProvider>\n      <SomeContextConsumer />\n    </SomeContextProvider>\n  </SomeContextConsumer>\n</SomeContextProvider>\n```\nIf the value provided by the outer SomeContextProvider changes, the innermost SomeContextConsumer will also be rendered. (Actually it will only \"semi-render\", because even though the function is called, the result is ignored.)\n\nIt is probably only an unnecessary detail for most users, but for me it broke some tests which track every single render for performance reasons and it actually causes >1k unnecessary component function calls in some situations because I utilize some nested contexts for some widely used information where it is important that elements are only rerendered when the context value changes. So if the top-level context provider is updated, but another provider between the element and it is not updated, the element should not be rendered.  \nAlso, it is very concerning that it executes the component function but ignores the result.\n\nReact version: 19.2.3 (and 19.0.0)\nWorks in 18.3.1\n\n## Steps To Reproduce\n\n1. Open code example below. Execute `vitest` in terminal.\n  - The test will fail, because the component was kind of rendered, even though it was unnecessary.\n2. Downgrade to React 18.3.1 by updating package.json and running `npm install`\n3. Execute `vitest` in terminal. The test will pass.\n\nLink to code example:\n\n[Repro code in unit test on Stackblitz](https://stackblitz.com/edit/vitest-tests-browser-examples-akwsn1xm?file=tests%2Freact19ContextsBugMinimalReprodWithUnitTest.test.tsx)\n\n## The current behavior\nThe inner component consuming the React context is called, even though the context value has not changed for it.\n\n## The expected behavior\nLike in React 18.3.1: Only component functions where the context value has changed will be called."
  },
  {
    "name": "[DevTools Bug]:",
    "description": "### Website or app\n\nhttps://legacy.reactjs.org/versions/\n\n### Repro steps\n\n![Image](https://github.com/user-attachments/assets/a7e10b3c-cba9-4355-9b6e-99cc306a596a)\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```"
  },
  {
    "name": "Perf: react-hooks ESLint plugin (eslint-plugin-react-hooks) rules are extremely slow, dominating lint time",
    "description": "# react-hooks ESLint plugin rules are extremely slow, dominating lint time\n\n- React version: `react@19.2.3`\n- `eslint-plugin-react-hooks` version: `eslint-plugin-react-hooks@7.0.1`\n- `eslint` version: `eslint@9.39.1`\n\n## Steps To Reproduce\n\n1. Install `eslint-plugin-react-hooks` (the React Compiler ESLint plugin)\n2. Enable the recommended plugin rules: e.g. `reactHooksPlugin.configs.flat.recommended, jstsFiles`\n3. Run ESLint with `TIMING=50` on a large codebase (~15k+ files)\n4. Observe that `react-hooks/static-components` takes 42-56% of total lint time\n\n## Link to code example\n\nUnfortunately, we cannot share a minimal reproduction as this occurs on a large proprietary codebase. However, the performance data should be reproducible on any sufficiently large React codebase.\n\n## The current behavior\n\nWhen running ESLint with the `TIMING` flag on our codebase, the `react-hooks` plugin rules dominate lint time:\n\n```\nRule                                       |  Time (ms) | Relative\n:------------------------------------------|-----------:|--------:\nreact-hooks/static-components              | 393394.236 |    56.2%\nunused-imports/no-unused-imports           |  62579.162 |     8.9%\nredos-detector/no-unsafe-regex             |  35114.790 |     5.0%\nimport/named                               |  24641.617 |     3.5%\nreact-hooks/rules-of-hooks                 |  16596.903 |     2.4%\n...\n```\n\n**Key findings:**\n- `react-hooks/static-components` takes **~393 seconds** (6.5 minutes) \u2014 more than **5x longer** than the next slowest rule\n- The react-hooks plugin nearly **doubles total lint time**: ~3 minutes without the plugin vs ~6 minutes with it enabled\n- With 4x parallelism (`--concurrency=4`), total ESLint time goes from ~190 seconds to ~370 seconds when the plugin is enabled\n- Even if we disable `react-hooks/static-components`, another rule jumps up to take the top spot. It seems to trigger from the plugin itself, not any particular rule.\n\n## The expected behavior\n\nThe `react-hooks` rules should have comparable performance to other ESLint rules. A single rule taking 42-56% of total lint time and being 5x slower than any other rule suggests there may be optimization opportunities in the rule implementation.\n\n---\n\n**Impact:** This performance issue significantly affects CI pipeline times and local development experience. We're considering running these rules as a separate parallel step or disabling them entirely, which isn't ideal."
  },
  {
    "name": "Bug: `eslint-plugin-react-hooks` hides all errors on multiple findings",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nWhat kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\n1. Add `eslint-plugin-react-hooks` to your project\n2. Open a tsx file\n3. Intentionally add a violation of the `react-hooks/set-state-in-effect` rule\n4. In the same file, intentionally add a violation of `react-hooks/static-components` rule\n5. Uncomment 1 of the 2 commented lines of code below\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n```tsx\nexport default function App() {\n  const [foo, setFoo] = useState(false);\n\n  useEffect(() => {\n    setFoo(!foo);\n  }, []);\n\n  const ViolateStaticComponents = () => {\n    // const [counter, setCounter] = useState(0);\n    return (\n      <h1>Hello, World!</h1>\n    );\n  };\n\n  // const date = Intl.DateTimeFormat().format(new Date());\n\n  return (\n    <>\n      <ViolateStaticComponents />\n      ...\n    </>\n  );\n}\n```\n\n## The current behavior\nIf you uncomment the code on either line 9 or 15, the linter no longer picks up either set-state-in-effect or static-components rules.\n\n## The expected behavior\nBoth lint rule violations are correctly identified.\n\n---\n\nNot sure why the linter breaks on the 2nd `useState` reference, but I think it's breaking on the `Intl.DateTimeFormat` code because `Intl` isn't marked as a global."
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[Compiler Bug]: useEffectEvent bypasses react-hooks/set-state-in-effect lint rule",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBUYCAogGaUJx4A0JZVNd5AbggHYNMIDKeAIZ4EBAL4FKMCBgIByGAiF15AbgA6XLQgAeOfATgQuYQgGFZOLt0IBeAgAoAlATsA+IloJGTZggDajGR4giIIALpufGGijvLyzpraXD6kFNS0eI4ubp7A3j4EAPTFBBhCMADWCAAmBEJgBEoqeAC0ABYQEFVgxSFtZuFtaFxtCJl0BAgwMjCFPiGxCPGiZonJPuKMARFJWoXGpoQm-AihwqLR6SxZHLY5rh5eqUVLlyvya3gbheL7KTSzEm2VyzwKrx8pQIABtRoQsI0yGAFgRTudli5NhIAYUlHhYKkuFAYTDkuJkiB6CAjpQ0ABzFDobC4Qh4ACeWDExAACjCoPTRgB5LB4NB+CRSGRyeQAIyEsoQMLaWH5grGLTobWM2DQMJmxVqaHWyS0jghUOKOqwepE4q4AFkILUEMgCBoQEJSR6tJIwHawHSEE0+QLhaL7WAklTwF0AO4ASR4My4XrAKEoaYQ4iAA\n\n### Repro steps\n\nCalling setState in `useEffect` is marked as an error per `react-hooks/set-state-in-effect` lint:\n\n```typescript\n  const [, setState] = useState('');\n\n  useEffect(() => {\n    // marked as react-hooks/set-state-in-effect error\n    setState('test');\n  }, []);\n\n```\n\nHowever there is no lint error if the `setState` is wrapped in an `useEffectEvent`:\n\n```typescript\n  const onSetState = useEffectEvent(() => {\n    setState('test');\n  });\n\n  useEffect(() => {\n    // lint passes\n    onSetState();\n  });\n```\n\nThis provides an easy escape hatch for developers who want to bypass the lint rule.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2\n\n### What version of React Compiler are you using?\n\nNo compiler needed. eslint-plugin-react-hooks is 7.0.1"
  },
  {
    "name": "Bug: react-devtools 7.0.1 - Electron failed to install correctly",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: Latest\n\n## Steps To Reproduce\n\n1. Install `react-devtools` as global package.\n2. Try running it.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n```\n\u276f react-devtools         \n~/.local/share/mise/installs/npm-react-devtools/7.0.1/5/.pnpm/electron@23.3.13/node_modules/electron/index.js:17\n    throw new Error('Electron failed to install correctly, please delete node_modules/electron and try installing again');\n    ^\n\nError: Electron failed to install correctly, please delete node_modules/electron and try installing again\n    at getElectronPath (~/.local/share/mise/installs/npm-react-devtools/7.0.1/5/.pnpm/electron@23.3.13/node_modules/electron/index.js:17:11)\n    at Object.<anonymous> (~/.local/share/mise/installs/npm-react-devtools/7.0.1/5/.pnpm/electron@23.3.13/node_modules/electron/index.js:21:18)\n    at Module._compile (node:internal/modules/cjs/loader:1761:14)\n    at Object..js (node:internal/modules/cjs/loader:1893:10)\n    at Module.load (node:internal/modules/cjs/loader:1481:32)\n    at Module._load (node:internal/modules/cjs/loader:1300:12)\n    at TracingChannel.traceSync (node:diagnostics_channel:328:14)\n    at wrapModuleLoad (node:internal/modules/cjs/loader:245:24)\n    at Module.require (node:internal/modules/cjs/loader:1504:12)\n    at require (node:internal/modules/helpers:152:16)\n\nNode.js v24.12.0\n```\n\n## The expected behavior\nTo open the devtools window.\n\n## Additional Info\n\nI know that older nodejs versions are not the problem, didnt work on 22 nor 18.\n\n```\n\u276f ll ~/.local/share/mise/installs/npm-react-devtools/7.0.1/5/.pnpm/electron@23.3.13/node_modules/electron \nOctal Permissions Links Size User    Group   Date Modified    Name\n0755  drwxr-xr-x      1    - user user 2025-12-13 08:30 \ue5fa node_modules/\n0644  .rw-r--r--      1 8,2k user user 2025-12-13 08:30 \ue60b checksums.json\n0755  .rwxr-xr-x      1  612 user user 2025-12-13 08:30 \ue74e cli.js*\n0644  .rw-r--r--      1 761k user user 2025-12-13 08:30 \ue628 electron.d.ts\n0644  .rw-r--r--      1  659 user user 2025-12-13 08:30 \ue74e index.js\n0644  .rw-r--r--      1 3,1k user user 2025-12-13 08:30 \ue74e install.js\n0644  .rw-r--r--      1 1,1k user user 2025-12-13 08:30 \uf02d LICENSE\n0644  .rw-r--r--      1  589 user user 2025-12-13 08:30 \ue71e package.json\n0644  .rw-r--r--      1 5,4k user user 2025-12-13 08:30 \udb80\udcba README.md\n```\n\nI tried to set `ELECTRON_OVERRIDE_DIST_PATH` to the `electron` installed locally.\n\n`ELECTRON_OVERRIDE_DIST_PATH=\"/usr/bin/electron\"` \n\nBut then, no error occur, but also doesnt work.\n\n```\n\u276f react-devtools                                                                         \n\n~\n```\n\nPerhaps if electron were upgraded to latest. Latest being `extra/electron39 39.2.7-1 [installed]`, yet it is still using version `23`.\n\n---\n\nEven if one argued to install older electron version, it's no longer available.\n\n```\nextra/electron 1:39-1 [installed]\n    Meta package providing the latest available stable Electron build\nextra/electron27 27.3.11-10\n    Build cross platform desktop apps with web technologies\nextra/electron31 31.7.7-4\n    Build cross platform desktop apps with web technologies\nextra/electron34 34.5.8-1\n    Build cross platform desktop apps with web technologies\nextra/electron35 35.7.5-1\n    Build cross platform desktop apps with web technologies\nextra/electron36 36.9.3-1 [installed: 36.9.3-1.1]\n    Build cross platform desktop apps with web technologies\nextra/electron37 37.5.1-1\n    Build cross platform desktop apps with web technologies\nextra/electron38 38.7.2-1 [installed]\n    Build cross platform desktop apps with web technologies\nextra/electron39 39.2.7-1 [installed]\n    Build cross platform desktop apps with web technologies\n```\n\n---\n\nI tried with v27, same result, no error but didnt do anything.\n\n```\n\u276f ELECTRON_OVERRIDE_DIST_PATH=\"/usr/bin/electron27\" pnpx react-devtools\n...\n\n~ took 4s\n\u276f pacman -Ss electron27 \nextra/electron27 27.3.11-10 [installed]\n    Build cross platform desktop apps with web technologies\n\n~\n\u276f whereis electron27\nelectron27: /usr/bin/electron27 /usr/lib/electron27\n```\n\nEven v23, full path or not.\n\n```\n ELECTRON_OVERRIDE_DIST_PATH=\"~/Downloads/electron-v23.3.13-linux-x64/electron\" pnpx react-devtools\n```"
  },
  {
    "name": "Bug: Transition never resolves when nesting Suspense",
    "description": "React version: 19.2.3\n\n## Steps To Reproduce\n\n1. Open https://codesandbox.io/p/sandbox/kng288?file=%2Fsrc%2FApp.js\n2. Press \"resolve content of 0\"\n3. Press \"next session\"\n\n## The current behavior\n\nnothing happens\n\n## The expected behavior\n\n\"session 0\" changes to \"session 1\"\n\n## Full Code\n\n```js\nimport { useState, use, Suspense, startTransition } from \"react\";\n\nlet nextId = 0;\n\nclass Session {\n  id = nextId++;\n  contentPromise;\n  resolveContent;\n\n  constructor() {\n    this.contentPromise = new Promise((r) => (this.resolveContent = r));\n  }\n\n  static async create() {\n    await Promise.resolve();\n    return new Session();\n  }\n}\n\nlet initialSessionPromise = Session.create();\n\nexport default function App() {\n  const [sessionPromise, setSessionPromise] = useState(initialSessionPromise);\n\n  return (\n    <Suspense fallback={<p>loading session...</p>}>\n      <SessionView\n        sessionPromise={sessionPromise}\n        onNew={() => startTransition(() => setSessionPromise(Session.create()))}\n      />\n    </Suspense>\n  );\n}\n\nfunction SessionView({ sessionPromise, onNew }) {\n  const session = use(sessionPromise);\n\n  return (\n    <>\n      <p>session {session.id}</p>\n      <button onClick={onNew}>new session</button>\n      <button onClick={session.resolveContent}>\n        resolve content of {session.id}\n      </button>\n      <Suspense fallback={<p>loading content for {session.id}...</p>}>\n        <Content session={session} />\n      </Suspense>\n    </>\n  );\n}\n\nfunction Content({ session }) {\n  use(session.contentPromise);\n  return <p>content ready for {session.id}</p>;\n}\n```"
  },
  {
    "name": "[Compiler Bug]: .map() callback extracted to module scope breaks closure over locally-scoped variables",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAejQAgLIEsB2uAtgIYA2mMCADjBACZRwAuuE+mEAZpgEoIkWmAMIQi1XGQQxMAIygBzADr4MmAIKYuUfCzYd6CLgQT1MBTlGbTMYOBGoI5Ce0QRhMOw8fynz+MFxDTAA6UmoACgBKTDhyMllBAGsVNQB3AAsEDlkIZgzMEip-QOCSWLIIMFgnKmZYXzMuOiJCrUFmCBgATy0dPXYVFWJqLuZMACpCj34OrRbMAHIqDsWh3wAPUZhx7V1WdliV6wAhAnoCBTBozGAVTEw1AEl8XxlXUd98ce8TMws4JVqlR7rF2GBxi83qJxOxssxkHwBCwQgAxYQAHmAmAAbuQoAhERCYJdMABfAB8mAAvJgIti8WQCeSYtSqXcOA9KAh6jAOBiLjiKcBGQSyRi0IKKaCyesHmoAPJWGwfOHfTD5EjjKBgdyYKHSGGfeH+NphEiRKKg+wBcZK6wwI1qhFIjpozHY3DWIhgInMEn4BQAbQAuuSqbT6eZvR4yaz2aCHnUGnTE1yBbghWmubcvQgfebIhE80R47dsznHlgTgBVADiiINjrExvVuA8XiMf0wWWKmWyYPEklMFZzNsgUhClQUEUWTadXwRiwANBruo5uPrXoaW86rZzK1zk3zMBj57vF5gkghutTgCWybj8Qg7w-HtKD5W47LPw8JVK033B4f1BY8OGxe0d1hS8fxA1QsBrMASAUBAVHHcYIOVZtoJNR9aTgY4EDOfALkDa59xUBAtjGTBvBIKAyF2foDg4dRqEtctOTA09IOw1txhLMA7yDRYSBXJZZHExY4EWENHzQD8fxAZcQBtYwFBQEARho5h1ycbEAAUmQUAgFWoFjY3mMQJJIWQEDIABaahjIIByVhYByPmHGBJXbZhFgAbnWelQQwLyyC1fRsAYQlMCUEB4nilRHyQ1gwGMPUjMUUzzP0MAogC5TwAyCA0heB18HIMAUC4KqEDJIA\n\n### Repro steps\n\n1. Open the React Compiler Playground with this code: [LINK](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAejQAgLIEsB2uAtgIYA2mMCADjBACZRwAuuE+mEAZpgEoIkWmAMIQi1XGQQxMAIygBzADr4MmAIKYuUfCzYd6CLgQT1MBTlGbTMYOBGoI5Ce0QRhMOw8fynz+MFxDTAA6UmoACgBKTDhyMllBAGsVNQB3AAsEDlkIZgzMEip-QOCSWLIIMFgnKmZYXzMuOiJCrUFmCBgATy0dPXYVFWJqLuZMACpCj34OrRbMAHIqDsWh3wAPUZhx7V1WdliV6wAhAnoCBTBozGAVTEw1AEl8XxlXUd98ce8TMws4JVqlR7rF2GBxi83qJxOxssxkHwBCwQgAxYQAHmAmAAbuQoAhERCYJdMABfAB8mAAvJgIti8WQCeSYtSqXcOA9KAh6jAOBiLjiKcBGQSyRi0IKKaCyesHmoAPJWGwfOHfTD5EjjKBgdyYKHSGGfeH+NphEiRKKg+wBcZK6wwI1qhFIjpozHY3DWIhgInMEn4BQAbQAuuSqbT6eZvR4yaz2aCHnUGnTE1yBbghWmubcvQgfebIhE80R47dsznHlgTgBVADiiINjrExvVuA8XiMf0wWWKmWyYPEklMFZzNsgUhClQUEUWTadXwRiwANBruo5uPrXoaW86rZzK1zk3zMBj57vF5gkghutTgCWybj8Qg7w-HtKD5W47LPw8JVK033B4f1BY8OGxe0d1hS8fxA1QsBrMASAUBAVHHcYIOVZtoJNR9aTgY4EDOfALkDa59xUBAtjGTBvBIKAyF2foDg4dRqEtctOTA09IOw1txhLMA7yDRYSBXJZZHExY4EWENHzQD8fxAZcQBtYwFBQEARho5h1ycbEAAUmQUAgFWoFjY3mMQJJIWQEDIABaahjIIByVhYByPmHGBJXbZhFgAbnWelQQwLyyC1fRsAYQlMCUEB4nilRHyQ1gwGMPUjMUUzzP0MAogC5TwAyCA0heB18HIMAUC4KqEDJIA)\n  2. The code defines a factory function createBindings() that returns React components. Inside the factory:\n    - InnerComponent is defined as a simple functional component\n    - OuterComponent uses InnerComponent inside a .map() callback\n  3. Observe the compiled output - the .map() callback is extracted to a module-scope function _temp:\n```ts\n  function _temp(item) {\n    console.log(\"InnerComponent:\", typeof InnerComponent);\n    return <InnerComponent key={item} value={item} />;\n  }\n```\n  4. _temp references InnerComponent, but InnerComponent is defined inside createBindings() - not at module scope.\n  The closure is broken.\n  5. At runtime, InnerComponent is undefined inside _temp, causing: Error: InnerComponent is not defined\n\n  Expected behavior:\n\n  The compiler should either:\n  - Keep the callback inline to preserve the closure\n  - Hoist captured variables along with the extracted function\n  - Skip compilation for callbacks that close over locally-scoped values\n\n  Workaround:\n\n  Adding 'use no memo' to OuterComponent prevents the callback extraction and preserves the closure.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: I have found one issue on your blog.",
    "description": "I found your blog in issue: \nBlog URL: https://react.dev/blog/2025/12/11/denial-of-service-and-source-code-exposure-in-react-server-components#additional-fix-published\nBlog Title: Denial of Service and Source Code Exposure in React Server Components\n-> In this blog you are added multiple react version. \nThis includes versions 19.0.0, 19.0.1, 19.0.2, 19.1.0, 19.1.1, 19.1.2, 19.1.2, 19.2.0, 19.2.1 and 19.2.2\n-> Issue is your are added multiple time added this version 19.1.2."
  },
  {
    "name": "React Compiler generates erroneous code when an app contains a component named `Symbol`",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAejQAgMIwQQwBcFN9M4IBbABwgDsE7DM79KEATTAZQE9KARhAA2AHToAzKHTiEAlvR78hwgBQBKTMHGZMeQrDqYAPBzkA3AHwBJUpVJLBI8lVoMmxtGasBucQF9xcQxMAFUwEjlmOSMAQWpqcQQAD1oYZg4ECXwoYWYpGXlFeOoNLR09BAMYI1UK3VMLS3rdEwALAEZLACUCWRxXOWEEGEwAISgAc0xe6hgIDihZBTpPTuajVoa+J2FMNA2tz29DzHU-On8QABoQCkk5SZQQORoIdMxCXmoSYEwABWEUxiAHlqEU6GBMP5MBJ5vYAOQCfACBDCAC01CBkxi6Lw+Fk6IoNCGIy8cjAhARF3Eqm0mwwxOoQyIKwAsgsEMhMKIQPhhGIQAFMGBWWAJHIEFDAcC6GCIWBzjdwG0IAB3axMEasYRgFDZXUIfxAA\n\n### Repro steps\n\n1. Configure a React app to run React Compiler.\n2. Create a component named `Symbol`.\n3. Run the app.\n\nThe issue relates to the fact that React Compiler generates output with a call to `Symbol.for`. As shown in the playground link, if a component is in scope with the name `Symbol`, the compiler-generated code tries to call the `for` method on the React component rather than `globalThis.Symbol`. This results in the runtime error \"Symbol.for is not a function\".\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.3\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: `react-hooks/set-state-in-effect` doesn't work in `React.useState` and `React.useEffect`",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nWhen calling `setState` from inside `React.useEffect`, as opposed to directly `useEffect`, the `set-state-in-effect` eslint rule does not detect a violation. This rule should detect violations regardless of how `useState` and `useEffect` are imported.\n\nReact version: 19.2.3\n`eslint-plugin-react-hooks` version: 7.0.1\n\n## Steps To Reproduce\n\n1. Enable `set-state-in-effect` in your eslint config\n2. Import React with `import * as React from 'react'`\n3. Create a component that calls `setState` from inside `React.useEffect`\n4. Run eslint and observe that eslint does not fail\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example: https://github.com/appellation/eslint-react-repro\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n`set-state-in-effect` only detects violations when directly importing `useState` and `useEffect`. It does not detect violations when using `React.useState` and `React.useEffect` from `import * as React from 'react'`.\n\n## The expected behavior\n\n`set-state-in-effect` should detect violations when using `React.useState` and `React.useEffect` from `import * as React from 'react'`."
  },
  {
    "name": "Bug: Fiber Scheduler Overhead Under Heavy Suspense Trees",
    "description": "Large Suspense boundaries create nested work units that increase reconciliation time due to excessive yielding.\nA micro-optimization in beginWork to short\u2011circuit noop boundaries would reduce UI stalls in concurrent mode."
  },
  {
    "name": "Improve cyclic thenable detection in ReactFlightReplyServer",
    "description": "React version:\n19.0.3\n\n## Steps To Reproduce\n\n1. In `ReactFlightReplyServer.js`, construct a chain of `ReactPromise` instances where each `.value` points to the next, and the last one points back to the first (e.g. `A -> B -> C -> A`), creating a multi\u2011node cycle.\n2. Trigger the cycle detection logic by resolving a chunk whose `value` is the head of this cycle and observing how the `while (inspectedValue instanceof ReactPromise)` loop behaves.\n\n## The current behavior\n\nThe cycle detection added in commit `bd4289b` only checks for a direct\nself\u2011reference:\n\n```js\nlet cycleProtection = 0;\nwhile (inspectedValue instanceof ReactPromise) {\n  cycleProtection++;\n  if (inspectedValue === chunk || cycleProtection > 1000) {\n    if (typeof reject === 'function') {\n      reject(new Error('Cannot have cyclic thenables.'));\n    }\n  }\n}\n```\n\n`inspectedValue === chunk` catches a simple self\u2011loop, but a longer cycle like `A -> B -> C -> A` will continue iterating until the `cycleProtection` counter hits the $1000$ limit instead of detecting the cycle when a node is revisited.\n\n## The expected behavior\n\nAny cyclic thenable chain, including multi\u2011node cycles (not just direct\nself\u2011references), should be rejected as soon as a `ReactPromise` is encountered more than once in the traversal, rather than relying on the $1000$ `cycleProtection` cap to eventually bail out."
  },
  {
    "name": "Improve cyclic thenable detection in ReactFlightReplyServer",
    "description": "Closing this PR for now; I want to rethink the approach and will open a new one if needed."
  },
  {
    "name": "[DevTools Bug] Children cannot be added or removed during a reorder operation.",
    "description": "### Website or app\n\nnpx react-devtools\n\n### Repro steps\n\nI just run `npx react-devtools`, it recognise my app because does not appear the first page about how to connect it, but I always receive this error even have already checked my FlatList components (I am running React Native Expo development build). And if I close the error, it never loads my components. I receive a \"Loading React Element Tree...\" in looping.\n\nI have already tried `NODE_OPTIONS=\"--max-old-space-size=4096\" react-devtools` but still the same error\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\nreact-devtools-core\n\n### DevTools version (automated)\n\n7.0.1-3cde211b0c\n\n### Error message (automated)\n\nChildren cannot be added or removed during a reorder operation.\n\n### Error call stack (automated)\n\n```text\nat /Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:1124896\n    at f.emit (/Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:1055367)\n    at /Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:1057098\n    at /Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:1539281\n    at Array.forEach (<anonymous>)\n    at KS.e.onmessage (/Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:1539264)\n    at T.s (/Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:454936)\n    at T.emit (node:events:513:28)\n    at e.exports.B (/Users/mariaferreira/.nvm/versions/node/v24.6.0/lib/node_modules/react-devtools/node_modules/react-devtools-core/dist/standalone.js:1:489080)\n    at e.exports.emit (node:events:513:28)\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\nhttps://api.github.com/search/issues?q=Children cannot be added or removed during a reorder operation. in:title is:issue is:open is:public label:\"Component: Developer Tools\" repo:facebook/react\n```"
  },
  {
    "name": "[Compiler Bug]:Compiler doesn't memoize if const is between hooks",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4lgzg9grgTgxgUxALhAejQAgLIILYQCWAXggCaYBGAnpgEolCGc\n\n### Repro steps\n\n1. Open the React Compiler Playground link: https://playground.react.dev/#N4lgzg9grgTgxgUxALhAejQAgLIILYQCWAXggCaYBGAnpgEolCGc\n2. Observe the code that has a `const` declaration between React hooks\n3. Check the compiled output - you'll see that memoization is not working correctly\n4. The compiler should memoize values even when const is between hooks, but it currently doesn't\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n Latest (React 19 or main branch)\n\n### What version of React Compiler are you using?\n\nLatest babel-plugin-react-compiler from main branch"
  },
  {
    "name": "[Compiler Bug]: Function hoisting ignored in eslint rule react-hooks/immutability",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAsgJ4DCEAtgA6EKY4AUtMEtYAlEcADrEiUMAgCiaNAlzNmPALwA+XgKKqiaCBFkBuFUQC+XXYID0J9VlwFiAEykAbAIYxH+QmCLOERBBKn4ANwR7Mk8cIhwAC28cDiIINAjoojA4DgQ9DGw3Yg0tHmADIj0YBBxYYkwoe3tjfQEQABoQNMw0PABzFBA8OggYcJwyWm9CgAV7KA68TAB5WhyPfXV2aiIAcgAjR03ggFpaSenMPdLHXD20ujx7BBgTGzwwHHXjAWZ+UxMr2hvXaxIEDsyGKIEcNT4IAEyzA-zA7QQHgmUxm80WRia4EiEAA7gBJJh3TDgsAoNAkhD6IA\n\n### Repro steps\n\neslint rule `react-hooks/immutability` ignores [function hoisting](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Functions#function_hoisting). This\n```\nfunction MyComponent(props) {\n  useEffect(() => {\n    foo();\n  });\n  // function declarations are effectively at the top of the scope\n  function foo() { } \n  return null;\n}\n```\nshould not cause error\n```\nError: Cannot access variable before it is declared\n\n`foo` is accessed before it is declared, which prevents the earlier access from updating when this value changes over time.\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2\n\n### What version of React Compiler are you using?\n\n1.0"
  },
  {
    "name": "[Compiler Bug]: 'use no memo' does not apply recursively",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwGEJKoBbOgGTIAjBJTAAKYPhIQI+AL4BKfMAA6bfAHIoYBPjpyeCHhE0BudfnxxWYAp259BIygCUEdYjAQx8AXnxJfCEyXyV-AD4VSyt8b1xYNgAeQiYANwjgEJh5JIB6VIyLDXliq3jEji5eAWFRd08fH2L5EAAaEBs6EiYAcxQQJh5sPHxcAE9MPSkABW5epjoAeUxmWwVpGAgeLRCXAFpMecX97zJGfZthpkofAqY7c3V1SRi8vKvMG7I1ugBZCDEZD4VQgMiUSig9TyfBgH4PHoIMD4OZQBbLVYsOhgRRmdrgAAWEAA7gBJOi4Hx0cFgFDkMQIeRAA\n\n### Repro steps\n\n1. Write a render prop function which happens to contain nested functions with component-like names\n2. Enable react compiler\n3. App crashes due to hook errors because the render prop function got compiled. (This is expected behavior)\n4. Try to fix the problem by applying 'use no memo' to the toplevel function\n5. Nested function continues to be compiled; 'use no memo' needs to be applied to *all* nested component-like function names\n\nAs a side note, the [component-hook-factories](https://react.dev/reference/eslint-plugin-react-hooks/lints/component-hook-factories) rule does not detect this error: https://eslint-online-playground.netlify.app/#eNp9U01P4zAQ/Ssjn4rUJvcuVCuxu9rDSou4Yg5uMikJjh38gYqi/HfGHykpVFziyczzzHtvkpFZU5V4FP0gsegs27LGq8q1WsGdMKjc6gpGrgBO6dunVtZzFqAsoSiKEE9chcOg80bBdcRBufvBVay0/aCNgxEqqRX+lthTd5igMboHzgyKynEWkHiMyBob4aVbTNbS9+qf2KO0qxEarWHKPCqtrFsC7lHVaNDADRB0L0yA3uxm1jPJun3djVSdrssQElfSEZ8ZcaFlEMTWDK1slStoctMeknUniUS9VXgbSx8S040y3eCM+uQLnZ0hPzOms8t69Oav1s/2TvpDq84bboaY3ETU5inAgo+fXFxSWgWBD8mJzmYJtjBY6Z62UmO9TsXsFrnhJdrtxzvAi9cupB6IhjHacLYmRrX2e4mcPeYG5GaO5vOzltP0Rgp3gQJ3NJQ77qRQBy8O+H8IH0MiE/LcDcJYNF/z3GHViz8oaJOZ/VzgrrPHLTjjkebMucDxPEpn4v7I1RXthFY/iOqZmNDStaK1R1c4c28DKd9S1Oua/CJHUqHG1184BFGqapGWczLyfOHhKpmAlv6DWX3e8bfFix/AEp//zolN7yFnXC0=\n\nWe ran into this while compiling a project that uses react-virtualized. react-virtualized takes a [render prop](https://github.com/bvaughn/react-virtualized/blob/c737715486f724586aee8870ebea1e9efb7b0bfe/docs/Table.md#headerrowrenderer) which gets called as a function rather than rendered as a component. Our render prop function crashed when it got compiled and passed into this prop. \n\n\n\n\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n18.3.1\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: Can't really access component stack `errorInfo.componentStack` on server",
    "description": "React version: 19.2.2\n\nOn the server-side, upon a boundary error, the component stack can be accessed via `onError(err, errorInfo) => errorInfo.componentStack` but this has limited (no?) value because the general recommendation is to swallow boundary errors and let the client-side throw the error instead (if it fails again on the client-side).\n\nThe real value AFAICT would be to access the component stack upon shell errors, but I don't see a way to access `error.componentStack`:\n- The `onShellError()` callback of [`renderToPipeableStream()`](https://react.dev/reference/react-dom/server/renderToPipeableStream) doesn't receive `errorInfo.componentStack`.\n- Shell errors thrown while [`renderToReadableStream()`](https://react.dev/reference/react-dom/server/renderToReadableStream) also seem to be missing `errorInfo.componentStack`.\n\n## Context\n\nI'm the author of [`react-streaming`](https://github.com/brillout/react-streaming) which powers [`vike-react`](https://vike.dev/vike-react) amongst others."
  },
  {
    "name": "[Compiler Bug]: React Compiler incorrectly hoists inner function outside closure, breaking variable reference",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEwBUYCAygJ4B2cAogB54IzUCGANuXrggQL4EAZjAgYCAchgI2cPBIA61JQgY58BOBGphCcaW2YBxDhABGnbrwIBeAgB4AKgD4AFGmpo8aTgDVOUAjIBI4AlLbOREoEBBwIhLq8wY62BB5ePhz+HIFK0ZraurFougjULGCp5QDuBOTx9q7hNpEAbhBoACZuoXnUMVo6CVBmYPpoZnx2rhwlzOUwwU0RBO1dzZHA+TGzpQtgAHRsnZ0zc2Usvf0xBNJ4sP3LLVHXN8V7FQedCHHMZx8wK5vfj5EHKa6DIpkPBWaSpVxQLCdQwsJauLDSVrBNjUSgbAg4vEEAA+BNx+K2rzSQgIrjwlCwCAgNMRyOYMFsNjsEiEUFo3m0EnClLeMUScLsrJRMFc4oQQJugh+ZBeooIctSUvZ2wEfTeu3mnyEuDosgAFq5XHAzPjrU0FWD8pDCABzeKwqa0ik6u4PdU8aSgvUFIYkMge+HeqnO-0ozXhmj0JgsdhcAMIJxuMAjMYwCYIAA0BDdMPTVx1Meh3Dj0y1qNp6MxyXxYRJISjaqr6YRSOlCpijqpvtYBAA2roUUWuyiALoEyrOoPgmLD-rEUgUdNFkseqfu9O66hgkAFkCDIRoF0odDYXCEemMogEAAKORdHgA8lgBToBMJROIEgWJMHAALRYG+HigQYcigVo2BoHEMAAPSdHMEgANx9K4IoEMhyHwVgiGGGg2gALIQN8wQKCAnAcDRSiCGAJFgBeCCVK+UDvtQX4-mAoQYSe4BmhA1QAJLUOyqZgCgQicGQ-BAA\n\n### Repro steps\n\nHi,\n\n  I found an issue while adopting React Compiler and wanted to report it.\n\n  ### Original Source Code\n  ```typescript\n  import { useSyncExternalStore } from 'react'\n\n  export const createGlobalStore = <T>(initialValue: T) => {\n    let store: T = initialValue\n\n    const listeners = new Set<() => void>()\n\n    const subscribe = (listener: () => void) => {\n      listeners.add(listener)\n      return () => {\n        listeners.delete(listener)\n      }\n    }\n\n    const setStore = (updater: ((prev: any) => any) | any) => {\n      if (typeof updater === 'function') {\n        store = updater(store)\n      } else {\n        store = updater\n      }\n\n      listeners.forEach((cb) => cb())\n    }\n\n    const getStore = () => {\n      return store\n    }\n\n    const useStore = () => {\n      const state = useSyncExternalStore<T>(subscribe, getStore)\n\n      const setState = (updater: ((prev: T) => T) | T) => {\n        setStore(updater)\n      }\n\n      return [state, setState] as const\n    }\n\n    return { useStore, getStore, setStore }\n  }\n\n  Compiled Output\n\n  import __vite__cjsImport0_react_compilerRuntime from\n  \"/node_modules/.vite-bizprofile-webview/deps/react_compiler-runtime.js?v=e0557d73\";\n  const _c = __vite__cjsImport0_react_compilerRuntime[\"c\"];\n  import __vite__cjsImport1_react from \"/node_modules/.vite-bizprofile-webview/deps/react.js?v=e0557d73\";\n  const useSyncExternalStore = __vite__cjsImport1_react[\"useSyncExternalStore\"];\n  export const createGlobalStore = (initialValue) => {\n      let store = initialValue;\n      const listeners = /* @__PURE__ */\n      new Set();\n      const subscribe = (listener) => {\n          listeners.add(listener);\n          return () => {\n              listeners.delete(listener);\n          }\n          ;\n      }\n      ;\n      const setStore2 = (updater) => {\n          if (typeof updater === \"function\") {\n              store = updater(store);\n          } else {\n              store = updater;\n          }\n          listeners.forEach( (cb) => cb());\n      }\n      ;\n      const getStore = () => {\n          return store;\n      }\n      ;\n      const useStore = () => {\n          const $ = _c(3);\n          if ($[0] !== \"fa62d2894ba91e5b72b8903a58ed08db2e221dc337d30b892bb5c3f5e1dd73b5\") {\n              for (let $i = 0; $i < 3; $i += 1) {\n                  $[$i] = Symbol.for(\"react.memo_cache_sentinel\");\n              }\n              $[0] = \"fa62d2894ba91e5b72b8903a58ed08db2e221dc337d30b892bb5c3f5e1dd73b5\";\n          }\n          const state = useSyncExternalStore(subscribe, getStore);\n          const setState = _temp;\n          let t0;\n          if ($[1] !== state) {\n              t0 = [state, setState];\n              $[1] = state;\n              $[2] = t0;\n          } else {\n              t0 = $[2];\n          }\n          return t0;\n      }\n      ;\n      return {\n          useStore,\n          getStore,\n          setStore: setStore2\n      };\n  }\n  ;\n  function _temp(updater) {\n      setStore(updater);\n  }\n\n  Problem\n\n  The compiler hoists the setState function to module scope as _temp, but:\n\n  1. The original setStore variable inside the closure is renamed to setStore2\n  2. The hoisted _temp function still references setStore (the original name)\n  3. Since _temp is outside the closure, it cannot access setStore2\n  4. This results in ReferenceError: setStore is not defined at runtime\n\n  This error occurs in both local development and production builds.\n\n  Environment\n\n  - vite: 7.2.2\n  - react: 19.2.0\n  - react-dom: 19.2.0\n  - babel-plugin-react-compiler: 1.0.0\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n19.2.0"
  },
  {
    "name": "Bug: Server Components error when directly rendering a Context",
    "description": "A Server Component that renders `<Context>` directly will result in a runtime error:\n\n> Element type is invalid. Received a promise that resolves to: Context. Lazy element type must resolve to a class or function.\n\n[Here's the minimum repro](https://github.com/samselikoff/2025-12-09-render-context-from-rsc):\n\n```tsx\nimport { Context } from './context';\n\nexport default function Home() {\n  return <Context value={2}>hello</Context>;\n}\n```\n\n```tsx\n// context.tsx\n'use client';\n\nimport { createContext } from 'react';\n\nexport const Context = createContext(1);\n```\n\n\nReact version: 19.3.0-canary-378973b3-20251205\n\nLink to code example:\n\nhttps://github.com/samselikoff/2025-12-09-render-context-from-rsc\n\n## The current behavior\n\nRenders without error\n\n## The expected behavior\n\nErrors with\n\n> Element type is invalid. Received a promise that resolves to: Context. Lazy element type must resolve to a class or function."
  },
  {
    "name": "Bug: ViewTransition with enter/exit hard-crashes iOS Safari",
    "description": "I'm experiencing an iOS-only crash (iOS 26.1) when there's `ViewTransition` with `enter` and another with `exit`:\n\n```js\nimport { Suspense, use } from \"react\";\nimport { ViewTransition } from \"react\";\nimport \"./App.css\";\n\nconst promise = new Promise((r) => setTimeout(() => r(\"Done\"), 3000));\n\nfunction Content() {\n  return <div>{use(promise)}</div>;\n}\n\nexport default function App() {\n  return (\n    <Suspense\n      fallback={\n        <ViewTransition exit=\"vt-reveal\">\n          <div>Loading...</div>\n        </ViewTransition>\n      }\n    >\n      <ViewTransition enter=\"vt-reveal\">\n        <Content />\n      </ViewTransition>\n    </Suspense>\n  );\n}\n```\n\n```css\n::view-transition-old(.vt-reveal) {\n  animation: 300ms ease-out both vt-out;\n}\n::view-transition-new(.vt-reveal) {\n  animation: 300ms ease-out both vt-in;\n}\n@keyframes vt-out {\n  to {\n    opacity: 0;\n  }\n}\n@keyframes vt-in {\n  from {\n    opacity: 0;\n  }\n}\n```\n\nThis is a hard crash, meaning the browser tab literally dies.\n\nSandbox: https://codesandbox.io/p/sandbox/gns365?file=%2Fsrc%2FApp.js"
  },
  {
    "name": "Bug: Suspense fallback streaming not working without html/body/head components",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\nSuspense fallback is not rendered during SSR when the root component lacks top-level `<html>`, `<head>`, or `<body>` tags. The server buffers the entire response until all suspended promises resolve, preventing streaming and the rendering of the initial fallback content.\n\nThis behavior impacts a use case where the React component is rendered as a sub-tree to be integrated into a larger application that uses a different framework.\n\nThis [commit](https://github.com/facebook/react/commit/b25bcd460f98a0b89e5a7199a6c88112163d961f) introduced this behavior change in the streaming logic where it worked in 19.0.0.\n\nReact version: 19.1-19.2\n\n## Steps To Reproduce\n\n1. Access https://3c4mqn-8080.csb.app/no-streaming\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\nThe server renders the component's final output without showing the fallback. The generated HTML for the suspended component only contains the resolved content wrapped in the standard suspense boundary markers:\n\n```html\n<!--$--><h1>Hello</h1><!--/$-->\n```\n\nNo fallback content (i.e., the \"Loading...\" message) is rendered to the client.\n\n## The expected behavior\n\nThe`Loading...` fallback should be rendered, mirroring the correct behavior observed in the full-document rendering path:\n\n```html\n<!--$-->\n<!--$?-->\n<template id=\"B:0\"></template>\nLoading...\n<!--/$-->\n<!--/$-->\n<script id=\"_R_\">\n  ...\n</script>\n<div hidden id=\"S:0\">\n  <h1>Hello</h1>\n</div>\n<script>\n  ...\n</script>\n```\nComparison Path: https://3c4mqn-8080.csb.app/"
  },
  {
    "name": "[Compiler Bug]: False positive on \"Hooks may not be referenced as normal values, they must be called\"",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABFMBANQEMAbKBXAXlwAoBKGgPn0IFkEBbCOxl3ABYATABpcAbQC6DANwAdDOix4AFhAgBrMDVzA2xcpVwBfBRkXKcuAGIkAlmR21+1VsEW5cVvPoKGKKhNddS0wcy8fXAA3Iypaf1JAxgjcGARsWAxcAB4wAAcSDGZgWMCTHIB6AqLmRRNFS0xrAHUIGG1dV3dPb2bfAyTKZBjTEI1tVKiy41polN70zJhsvMLi0riK6vW6jAaMEFE0TAAzewBzFBB7Lnz2vGwAT3yqfQAFCgv7DAB5fOw9maY1OMAgXFwAHIAEYkaEIMgAWnyXx+iPSJDg2ER6DujgQMEqABN7DhIeZFHQPPtcGASICwOcEDpPlBvn8AUCsHIjuB1AB3ACSGGwBIw5DAKFOEoQJiAA\n\n### Repro steps\n\nThe React Compiler doesn't correctly flag improper hook usage, when using a \"hooks provider\".\nAlthough the hook is always called, React Compiler fails with the following message:\n```\n Hooks may not be referenced as normal values, they must be called.\n```\n\nIt seems the fact that the hook variable name starts with `use_` is causing the compiler assume its usage.\n```tsx\nconst useValue = () => useMemo(() => 42, []);\nconst hooks = { useValue };\n\nconst Fails = () => {\n  const { useValue } = hooks;\n  const value = useValue();\n  //            ^^^^^^^^ Hooks may not be referenced as normal values, they must be called.\n  return <span>{value}</span>\n}\n\nconst Works = () => {\n  const { useValue: v } = hooks;\n  const value = v(); // works as expected\n  return <span>{value}</span>\n}\n```\n\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.1\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[Compiler Bug]:",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [x] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\n. Thank you for helping us make this chatbot experience better!\n\n### Repro steps\n\n4575490200340206\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n10000$\n\n### What version of React Compiler are you using?\n\n4575490200340206"
  },
  {
    "name": "Bug: React DevTools shows different versions (Stable vs. Canary) for Page/App Router in Next.js, potentially indicating an unpatched security risk (19.2.0-canary vs 19.2.1)",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: react 19.1.2 / next 15.3.6\n\n## Steps To Reproduce\n\n1.  Set up a Next.js project with both App Router and Page Router enabled.\n2. Set react and react-dom versions to 19.1.2 in package.json and run pnpm install\n3. Create a page based on the Page Router\n4. Create a page based on the App Router\n5. Open React DevTools.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\nReact DevTools displays 19.2.0-canary on App Router pages.\n\n## The expected behavior\nReact DevTools should display 19.1.2 on App Router pages, consistent with the Page Router and the stable React version specified in the project's package.json.\n\nFurthermore, since version 19.2.0 (or its corresponding Canary build) is known to potentially have security vulnerabilities that were patched in 19.2.1, it is crucial that the App Router's internal RSC runtime dependencies are updated to reflect the 19.2.1 patch level.\n\n<img width=\"3432\" height=\"1409\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/792dbffa-6efe-403d-a930-2c4dc5379b5f\" />\n<img width=\"3427\" height=\"1407\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/287d031d-c5e9-4fe7-b193-8b126e0f9844\" />"
  },
  {
    "name": "[Compiler Bug]: Cannot reassign component props",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [x] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwGEIBbbOhO3AApg+EhAgAafACMyMfAF8AlPmAAdNviYl8ggIRiI+AGTH8e2TBXrN+UePwBefGpCHXAbg12FG7-hgEXFg2QX87AB5CJgA3AD5wu1UAQRgYMgBPADoSGG5hfEp+AHNcAAtkfABmRSlBAH0pJhVHOPwo2PwAawQMx2AmBTjgQwUIgHpo+KVfW3bJ2ITNJS86BRAJEDhWEiZilBAmHhwCXAzMBFV8AAVKKGKmOgB5TGZWMEVRPK58AHJZaQISgAWkwdwedGBgTIjGB2x4TCKMAWYFwv1WGmE-nG43hmERZDedAAshBiJVXGRKJRXBoFPgwISmGBdggPrd7o8XkSwCsNuAyhAAO4ASQECBgdCpYBQ5EoYAQCiAA\n\n### Repro steps\n\nIf you want to reassign props (not modify), and use them in a callback function, React Compiler will skip compiling the component. For example:\n\n```jsx\nexport default function Component({ foo, bar }) {\n\tif (!foo && !bar) {\n\t\tfoo = \"foo\";\n\t}\n\n\treturn (\n\t\t<div>\n\t\t\t{Array.from({ length: 3 }, (_, i) => <div key={i}>{foo}</div>)}\n\t\t</div>\n\t);\n}\n```\n\nI got this error:\n> Todo: Support destructuring of context variables\n\nYou can also replace the if statement `if (!foo && !bar) { foo = \"foo\"; }` with the content itself `foo = \"foo\";`, that will also trigger the issue, however it is meaningless.\n\nThe modified prop must in a callback, directly use it will work fine. For example, `<div>{foo}</div>` works fine, and `<div>{(() => foo)()}</div>` raise an issue.\n\nI think the code above should be automatically converted into the code below which can work properly. Although I can also use the code below directly, it will only cause more trouble.\n\n```jsx\nexport default function Component({ foo: _foo, bar }) {\n\tlet foo = _foo;\n\tif (!foo && !bar) {\n\t\tfoo = \"foo\";\n\t}\n\n\treturn (\n\t\t<div>\n\t\t\t{Array.from({ length: 3 }, (_, i) => <div key={i}>{foo}</div>)}\n\t\t</div>\n\t);\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.1\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: Facebook account nakalimutan ko pano maopen ang aking account",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "ASDF",
    "description": "asfdgdhd"
  },
  {
    "name": "Render",
    "description": "ssagfsashg"
  },
  {
    "name": "Bug : Component not rendering",
    "description": "Issue while rendering components."
  },
  {
    "name": "Bug: Forms are reset when action fails",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.2.1\n\n## Steps To Reproduce\n\n1. Enter some text into the input and submit. The action will fail.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\nhttps://codesandbox.io/p/devbox/funny-jang-3zj83m?file=%2Fsrc%2Findex.js%3A7%2C4&workspaceId=ws_4qKVDKGuRrrvd8oLfoRWSz\n\n## The current behavior\n\nThe form is reset.\n\n## The expected behavior\n\nThe form is not reset when the action fails. [The docs](https://react.dev/reference/react-dom/components/form#handle-form-submission-on-the-client:~:text=After%20the%20action%20function%20succeeds%2C%20all%20uncontrolled%20field%20elements%20in%20the%20form%20are%20reset.) suggest that reset should only happen when the action is successful:\n\n> After the action function succeeds, all uncontrolled field elements in the form are reset.\n\n\nRelated: https://github.com/facebook/react/issues/29034"
  },
  {
    "name": "Bug: set-state-in-use-effect false negative when there are multiple instances",
    "description": "This code correctly produces a lint error:\n\n```tsx\nfunction MyComponent() {\n    const [loading, setLoading] = useState(false);\n    useEffect(() => {\n        setLoading(true);  // \u2705 ESLint correctly flags this (1 error)\n    }, []);\n    // commented out code...\n    return null;\n}\n```\n\nBut this code does not:\n\n```tsx\nfunction MyComponent() {\n    const [loading, setLoading] = useState(false);\n    useEffect(() => {\n        setLoading(true);  // \u274c BUG: Error no longer reported (should still be flagged!)\n    }, []);\n    const [firstName, setFirstName] = useState<string>('');\n    const [lastName, setLastName] = useState<string>('');\n    const [fullName, setFullName] = useState<string>('');\n    useEffect(() => {\n        setFullName(`${firstName} ${lastName}`);  // \u274c BUG: Also not reported\n    }, [firstName, lastName]);\n    return null;\n}\n```\n\nReact version: `^19.2.1`\n\n## Steps To Reproduce\n\nCreated a minimal reproduction repo: https://github.com/roryabraham/set-state-in-effect-bug\n\n## The current behavior\nIn the second code snippet provided, no lint errors are thrown.\n\n## The expected behavior\nLint errors should be thrown for both `setState` calls in `useEffect` callbacks."
  },
  {
    "name": ".",
    "description": "."
  },
  {
    "name": "Bug: eslint-plugin-react-hooks not throwing \"Hooks may not be referenced as normal values\" error",
    "description": "React version: 19.2.\n\n## Steps To Reproduce\n\n1. The expected behavior screenshot shows when my repo had eslint-plugin-react-compiler@beta and eslint-plugin-react-hooks@4.6.2 installed. [Per the React Compiler v1.0 announcement](https://react.dev/blog/2025/10/07/react-compiler-1#migrating-from-eslint-plugin-react-compiler-to-eslint-plugin-react-hooks), I removed eslint-plugin-react-compiler, bumped eslint-plugin-react-hooks to the latest version (7.0.1), and made the eslint config changes accordingly. All other rules, including the new ones, seem to work as expected, except this one seems to have disappeared (I only noticed because our codebase has many instances of this that we've been planning to fix at some point before enabling the compiler). Is this a bug or is this no longer meant to throw a lint error, since it does [explicitly break the rules of React](https://react.dev/reference/rules/react-calls-components-and-hooks#never-pass-around-hooks-as-regular-values) ? \n\n## The current behavior\n<img width=\"898\" height=\"317\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/90110fe0-a571-4aee-b863-4e7968dc9b1b\" />\n\n## The expected behavior\n<img width=\"898\" height=\"317\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1ca533cf-44ce-42d5-a8b1-25760123173b\" />"
  },
  {
    "name": "Bug: react-hooks/set-state-in-effect: false-positive with ternary",
    "description": "React version: react@19.2.0, eslint-plugin-react-hooks@7.0.1\n\n## Steps To Reproduce\n\n```tsx\nfunction getWidth(el: HTMLElement) {\n  return el.clientWidth;\n}\n\nfunction Component({value}: {value: string}) {\n  const [width, setWidth] = useState(0);\n  const ref = useRef<HTMLDivElement>(null);\n\n  useEffect(() => {\n    setWidth(ref.current ? getWidth(ref.current) : 0);\n//  ^^^^^^^^^^^^ Avoid calling setState() directly within an effect\n  }, [value]);\n\n  return <div ref={ref}>{value}{width}</div>;\n}\n```\n\nChanging the line to `setWidth(getWidth(ref.current));` makes the error go away, so it seems the presence of the ternary operator is confusing the rule.\n\n## The current behavior\n\nError is raised.\n\n\n## The expected behavior\n\nNo error to be raised."
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Bug: `progressiveChunkSize` forcing streaming SSR breaks html file integrity and `onAllReady` behavior",
    "description": "React version: 19.2.0\n\n\n## Steps To Reproduce\n\n```tsx\n// App.tsx\n<div className=\"rp-doc rspress-doc\">\n  <Suspense>\n    <MyComp />\n  </Suspense>\n</div>\n```\n\n```ts\nfunction renderToHtml(app: ReactNode): Promise<string> {\n  return new Promise((resolve, reject) => {\n    const passThrough = new PassThrough();\n    const { pipe } = renderToPipeableStream(app, {\n      onError(error) {\n        reject(error);\n      },\n      onAllReady() {\n        pipe(passThrough);\n        text(passThrough).then(resolve, reject);\n      },\n    });\n  });\n}\n```\n\nrelated APIs:\n\n`renderToPipeableStream`  `prerenderToNodeStream` etc\n\n## react 19.1.1\n\n```html\n<main class=\"rp-doc-layout__doc-container\">\n  <div class=\"rp-doc rspress-doc\">\n     <h1>title</h1>\n   </div>\n</main>\n```\n\n`.rspress-doc > h1` works fine\n\n\n## react 19.2.0\n\nIf the article content is very long, it exceeds the chunkSize set by React.\n\n```html\n<main class=\"rp-doc-layout__doc-container\">\n  <div class=\"rp-doc rspress-doc\"><!--$?--><template id=\"B:0\"></template><!--/$--></div>\n</main>\n\n<script>requestAnimationFrame(function () { $RT = performance.now() });</script>\n<div hidden id=\"S:0\">\n  <h1>title</h1>\n</div>\n```\n\n`.rspress-doc > h1` \ud83e\udd15\n\n\n\n## Related links\n\n- https://github.com/web-infra-dev/rspress/pull/2831\n\n- https://github.com/facebook/react/pull/33027\n\n- https://github.com/facebook/react/pull/33027#issuecomment-3403958008\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n`onAllReady` returns a HTML with streaming rendering\n\n## The expected behavior\n\n`onAllReady` behavior as the same as React 19.1.1"
  },
  {
    "name": "[Compiler Bug]: incorrectly uses variable name as JSX tag (`<base />`) instead of referenced value",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhASwLYAcIwC4AEASggIZyEBmMEGBA5DGRfQNwA6AdnBJ2IQCNSYBAQC8DACZoAbmy5cefQgBUE-AMK0cnBJzzJizPADoAYhvEEAFAEpxAPgLAuBAkv4Et2K0JEdONyY8WECAHm8sAHoHLgBfAK4EAA8cfAJJBEpSKAAbKihuPDReAgAJWgQ7Z1cCYNCCMLVNbV49Qhj4rhAAGhAlSjQAcxR0bFxCPABPLFFgAgAFXKghtE4AeSxi3jACOIJqWgYhAQRcgFosZdXOc6ZyPHOebDRchBgo6X55Ti5rF0CBCiUWeWFepG2nAAshBMoZ2CBSLlcgj4gQwBC0GBBupFtc1ptIWBbKxeuAABYQADuAEl9O9OEiwChsrkRHEgA\n\n### Repro steps\n\nGiven this input:\n\n```jsx\nimport React from 'react';\nconst base = 'div';\n\nconst TestComponent: React.FC = () => {\n  const Comp = base;\n  return <Comp/>\n};\n\nexport default function Home() {\n  return <TestComponent />\n}\n```\n\nThe compiler outputs this for the `TestComponent`\n\n```jsx\nconst TestComponent: React.FC = () => {\n  const $ = _c(1);\n  let t0;\n  if ($[0] === Symbol.for(\"react.memo_cache_sentinel\")) {\n    t0 = <base />;\n    $[0] = t0;\n  } else {\n    t0 = $[0];\n  }\n  return t0;\n};\n```\n\nFirst reported here: https://github.com/vercel/next.js/issues/86728\n\nThis outputs a `<base />` tag instead of a `div`.\n\nI searched on issues before posting, but I couldn't find other reports.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nNext.js' version on 16.0.6\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "`react-hooks/rules-of-hooks` false positives inside class instances (not class components)",
    "description": "We use classes to organize our code, and the eslint rule `react-hooks/rules-of-hooks` always warns about hooks not being usable inside class components, regardless if the class is a component.\n\n### Reproduction\n\n```js\nclass Store {\n  use() {\n    return React.useState(4)\n  }\n}\n```"
  },
  {
    "name": "Bug: fabric completeRoot called for suspended comp. w/ same tree output",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.3 (latest main)\n\n## Steps To Reproduce\n\n1. Checkout the test case in this PR: https://github.com/facebook/react/pull/35262\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example: https://github.com/facebook/react/pull/35262\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\nI noticed that when we suspend a component in fabric react and in that component a state update occurs, that we still will call completeRoot.\n\nThis has performance implications, as completeRoot will call into c++ and invoke ShadowTree::commit.\n\nI think that when a suspended component has a state update and we still show the very same fallback component, then there should be no completeRoot call, as technically, nothing has changed about the UI output.\n\nI believe that this is a bug with in react-reconciler / react fabric\n\n\n## The expected behavior\n\nWe shouldn't call `completeRoot` if there is no change in the rendering tree output."
  },
  {
    "name": "Bug: React 19.2 doesn't seem to have useEffectEvent :(",
    "description": "React 19.2 doesn't seem to have useEffectEvent :( \n\n<img width=\"830\" height=\"715\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/03a370be-beba-4e1b-a747-67bd73d45ff4\" />\n\nReact version:\n\n19.2.0\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[DevTools Bug]: Internal state change in cousin component causing a \"re-render\" in another cousin component wrapped inside parent divs.",
    "description": "### Website or app\n\nhttps://github.com/mukesharyal/react-devtools-bug-report\n\n### Repro steps\n\n1. Clone the GitHub repo and run `npm install`\n2. Run the local dev server with `npm run dev` and also run React Devtools\n3. You will see two components `ComponentA` and `ComponentB`\n4. Cause a re-render of `ComponentA` by clicking the button a bunch of times\n5. `ComponentB` will also be shown in the component render highlights and `Profiler` sessions\n\nThe structure of `App` component looks like this:\n\n```\nimport ComponentA from './components/ComponentA';\nimport ComponentB from './components/ComponentB';\n\nfunction App() {\n\n\treturn(\n\n\t\t<div>\n\t\t\t<div>\n\t\t\t\t<ComponentA />\n\t\t\t</div>\n\n\t\t\t\n\n\t\t\t<div>\n\t\t\t\t<ComponentB />\n\t\t\t</div>\n\t\t</div>\n\t);\n\n}\n\nexport default App;\n```\n\nThis is what `ComponentA` looks like:\n\n```\nimport { useState } from \"react\";\n\nexport default function ComponentA()\n{\n    console.log(\"Component A rendered!\");\n\n    const [count, setCount] = useState(0);\n\n    return(\n        \n        <>\n            <h1>\n                Component A\n            </h1>\n               \n            <h2>\n                Count is {count}\n            </h2>\n\n            <button onClick={() => { setCount(count => count + 1) }}>\n                Increase Count\n            </button>\n        </>\n    )\n}\n```\n\nAnd here is `ComponentB`:\n\n```\nexport default function ComponentB()\n{\n    console.log(\"Component B rendered!\");\n\n    return(\n        <>\n            <h1>\n                Component B\n            </h1>\n        </>\n    )\n}\n```\n\nNow, when I re-render `ComponentA` by clicking the button a bunch of times, I see re-render highlights in `ComponentA`. But, the surprising part is that I also see highlights in `ComponentB`.\n\nHere are the highlights:\n\n<img width=\"597\" height=\"297\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/806fdfe3-7132-481c-a664-31302019ef2f\" />\n\nAnd here is a `Profiler` session I recorded:\n\n<img width=\"1024\" height=\"406\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ed397760-ad82-407f-aa25-acf6d9699489\" />\n\nIt clearly shwos that `ComponentB` also \"re-rendered\" a bunch of times, and it actually took **more time** to \"re-render\" `ComponentB` than `ComponentA`.\n\nAnd here is what the console logs look like:\n\n<img width=\"1109\" height=\"843\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d71a14be-6288-4004-9d88-3033d6f021c1\" />\n\nAnd indeed, like we expect, there is only the console log when `ComponentB` rendered the first time and not again. But, the `Profiler` session and the component render highlights say otherwise.\n\nNow, let's change the structure of `App` component just a little bit:\n\n```\nimport ComponentA from './components/ComponentA';\nimport ComponentB from './components/ComponentB';\n\nfunction App() {\n\n\treturn(\n\n\t\t<div>\n\t\t\t<div>\n\t\t\t\t<ComponentA />\n\t\t\t</div>\n\n\t\t\t\n\t\t\t<ComponentB />\n\t\t\t<div>\n\t\t\t\t\n\t\t\t</div>\n\t\t</div>\n\t);\n\n}\n\nexport default App;\n```\n\nAnd now let's see the component highlights and `Profiler` session.\n\nHere is the highlights now:\n\n<img width=\"600\" height=\"563\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/33a33dfe-8e73-4c0b-8521-b4ef1e6b2a96\" />\n\nAnd here is what the `Profiler` says:\n\n<img width=\"1018\" height=\"300\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/51236cf3-f29e-4fd9-bfe7-4b3cb497e288\" />\n\nSo, now the results are what we expect.\n\nThis project was made with `React 19`, but we also see the same things with `React 18` as well. And, the presence or absence of the `React Compiler` makes no difference.\n\nTo verify that the effects are not seen with composition, I made a simple `Wrapper` component and had the same `div`s wrap our components as before. Here is the `Wrapper`:\n\n```\nexport default function Wrapper({ children })\n{\n    return(\n        <div>\n            { children }\n        </div>\n    )\n}\n```\n\nAnd now, we use the `Wrapper` instead of `div`s in our `App` component like this:\n\n```\nimport Wrapper from './components/Wrapper';\n\nimport ComponentA from './components/ComponentA';\nimport ComponentB from './components/ComponentB';\n\nfunction App() {\n\n\treturn(\n\n\t\t<Wrapper>\n\n\t\t\t<Wrapper>\n\t\t\t\t<ComponentA />\n\t\t\t</Wrapper>\n\n\t\t\t\n\n\t\t\t<Wrapper>\n\t\t\t\t<ComponentB />\n\t\t\t</Wrapper>\n\n\t\t</Wrapper>\n\t);\n\n}\n\nexport default App;\n```\n\nThankfully, we do not see the weird behavior in this case. Sigh!\n\nSo after all this, we clearly see that the weird stuffs are only seen in a specific case where we are using `div`s before the composition for layout purposes.\n\nBut, I do think that this is not at all \"expected React behavior\". Either this is a bug with the React library itself, or the React Devtools are lying to us.\n\nSo, that was the weird thing I saw. I have also written a blog post where I discuss this same thing in more detail. It can be found [here](https://www.mukesharyal.com.np/blog/on-react-reconciliation/).\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```"
  },
  {
    "name": "[DevTools Bug]: Internal state change in a cousin component causing a Profiler re-render in another cousin component wrapped inside parent divs.",
    "description": "### Website or app\n\nhttps://github.com/mukesharyal/react-devtools-bug-report\n\n### Repro steps\n\nSteps for reproduction:\n1. Clone the GitHub repo.\n2. Run the dev server and open React Devtools.\n3. You will see two components `ComponentA` and `ComponentB`.\n4. Cause a re-render of `ComponentA` by clicking the button.\n5. `ComponentB` will also be highlighted as being re-rendered and can also be seen in a Profiler session.\n\n`ComponentA` and `ComponentB` are wrapped inside of two `div`s in the `App` component like this:\n\n\n function App() {\n\n\treturn(\n\n\t\t<div>\n\t\t\t<div>\n\t\t\t\t<ComponentA />\n\t\t\t</div>\n\n\t\t\t\n\n\t\t\t<div>\n\t\t\t\t<ComponentB />\n\t\t\t</div>\n\t\t</div>\n\t);\n\n}\n\nexport default App;\n`\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```"
  },
  {
    "name": "Bug: Should have a queue. This is likely a bug in React",
    "description": "<!--\nI upgraded me react native app from 0.73.8 to 0.77.3 and as soon as I run app I get this error \"Should have a queue. This is likely a bug in React. Please file an issue\"\n-->\n\nReact version: 18.3.1\n\n## Steps To Reproduce\n\n1. Run react native app with RN 0.77.3 and react 18.3.1\n2. First thing that shows is the error\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\nApp shows errors once it's built\n\n\n## The expected behavior\n\nApp should run without any errors"
  },
  {
    "name": "Bug: `useActionState`'s second argument is not optional in the React types",
    "description": "React version: 19.2\n\n## Steps To Reproduce\n\n1. `useActionState(action)` currently throws a type error.\n\n## The current behavior\n\nI'm building a library that makes use of React Actions for mutations. There is no need to pass an initial state variable for `useActionState`. An \"implicit undefined\" initial state is perfectly fine and saves people from typing 6 characters each time.\n\nWhen I omit the second argument, React appears to be working just fine. However, `@types/react` expects the second argument to be provided. Can we make it optional or is there a reason to keep it required?\n\n## The expected behavior\n\n`useActionState(action)` should not throw a type error.\n\n\nI submitted a PR for `@types/react` here: https://github.com/DefinitelyTyped/DefinitelyTyped/pull/74156"
  },
  {
    "name": "Bug: Compiler, group memoisation",
    "description": "The compiler seems to mix dependency in some condition and nest block incorrectly.\n\nReact version: 19.2\nIssue demo : [React Compiler Playground](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBANpT6uADT4wCXAGFodXAF18AXnxQ1AZVxlcCfgAZBAbnH5J0uUysBbMCrW4Akp5g+kYmCOaW1rIAjCoATCoAzCoALCoArLqC4k4udDL4JEzUCDAIhAEIXob47pVgAHSFxTD8AG6GAHz4rQCksYYGRnbZElJ5BEwMpR4IOtVCnSI5zr5aim2LrQDUUVkSAL4jzmP5ZITlnvPCBl1iEivqFV4bN-iy9R+tKq31lLMA5rgABb4Lb4KKZHKHOhHfClXCwNgAHkITFaHWW+CRmA6Wg8mCssBECh0+yRAHocZikQAjKC4XCsfCsDSUJhwADWBmAkzg01muH2HT8UwAlzMdCUKXSGawMfcsVR5c4VSImlZShc6vUPGQBLUPIskWz8ByEJxuQahTzPGTyWyOoJoarnLT6Yy2Cy2ZzuWctR4hdxzvgntL3XLqeSlTkKaj0eJoSAlCAxoV-igQEx8TgCLhOJgECJ8AAFShQf6TADyBJYeXw+wKMAghoA5DSyDSEJQALSYMsVujd0pkRjdqT4oolcmomQtxwwuj8O7Ocnk8eYIqWWvsCDEZD4UQgMiUSiHhOqLdgQoIMAl-tVmvSBxJ8BAiAAdxFGrox7AKHIlBqPsQA)\n\n## Steps To Reproduce\n\n1. I create a component with 2 states that won't interact, and array and a counter ([React Compiler Playground Link](https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBANpT6uADT4wCXAGFodXAF18AXnxQ1AZVxlcCfgAZBAbnH5J0uUysBbMCrW4Akp5g+kYmCOaW1rIAjCoATCoAzCoALCoArLqC4k4udDL4JEzUCDAIhAEIXob47pVgAHSFxTD8AG6GAHz4rQCksYYGRnbZElJ5BEwMpR4IOtVCnSI5zr5aim2LrQDUUVkSAL4jzmP5ZITlnvPCBl1iEivqFV4bN-iy9R+tKq31lLMA5rgABb4Lb4KKZHKHOhHfClXCwNgAHkITFaHWW+CRmA6Wg8mCssBECh0+yRAHocZikQAjKC4XCsfCsDSUJhwADWBmAkzg01muH2HT8UwAlzMdCUKXSGawMfcsVR5c4VSImlZShc6vUPGQBLUPIskWz8ByEJxuQahTzPGTyWyOoJoarnLT6Yy2Cy2ZzuWctR4hdxzvgntL3XLqeSlTkKaj0eJoSAlCAxoV-igQEx8TgCLhOJgECJ8AAFShQf6TADyBJYeXw+wKMAghoA5DSyDSEJQALSYMsVujd0pkRjdqT4oolcmomQtxwwuj8O7Ocnk8eYIqWWvsCDEZD4UQgMiUSiHhOqLdgQoIMAl-tVmvSBxJ8BAiAAdxFGrox7AKHIlBqPsQA))\n\n```tsx\nexport default function MyApp() {\n  const [count, setCount] = useState(0);\n  const [items, setItems] = useState([1, 2, 3, 4, 5])\n\n  const filteredItems = items.filter(v => v%2 === 0)\n\n  const increment = () => {\n    setCount(v => v+1)\n  }\n\n  const addItem = () => {\n    setItems(v => [...v, v.length + 1])\n  }\n\n\n  return <div>\n    <p>Compteur {count}</p>\n    <button onClick={increment}>Incr\u00e9menter</button>\n    <ul>\n      {filteredItems.map(item => <li key={item}>{item}</li>)}\n      <button onClick={addItem}>Add Item</button>\n    </ul>\n  </div>\n}\n```\n\n2. I would expect `filteredItem` to be memoised with \"items\" as a dependency. But it produce this code\n\n```jsx\n// Compiler output\n  if ($[1] !== count || $[2] !== items) {\n    const filteredItems = items.filter(_temp);\n    let t5;\n    if ($[7] === Symbol.for(\"react.memo_cache_sentinel\")) {\n      t5 = () => {\n        setCount(_temp2);\n      };\n      $[7] = t5;\n    } else {\n      t5 = $[7];\n    }\n\nlet t2\n// Expected output\n  if ($[1] !== items) {\n    t2 = items.filter(_temp);\n    $[1] = items\n    $[2] = filteredItems\n    } else {\n      t2= $[2];\n    }\n```\n\n## The current behavior\n\nIt groups everything after the filteredItems inside a condition\n\n\n## The expected behavior\n\n- `filteredItems` should be memoised\n- The 3 children of the virtual dom should be in their own condition with the right dependency"
  },
  {
    "name": "[Compiler Bug]: Memoization depends on how the order of the lines of code",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgDMoA7OXASwhPyjAQDEIIAKCgE2XzFxgpIDmASnzAAOjXxxqPfACMAhjHwBeWvQBCSlkIDcE-FJkEAbgoA2UBKqKk4LRTD0SDRkrOD52C3AvwBfGzoELQAvFghMShk2diFnSWl3Aj81GAQwKHNcFm9fABp8M0sEBNdXdNxYGgV9En8QfJAkwgoBFBAKAFtsPHxcAE9Ma08ABUsBfgB5KKp3AKIYCC78AHJFOQRzAFpMCf5t9IVybekeinMEGAB6dgoeVbqJFnFJa+uzzAufOYBZCHYCC4YhAFnMIIkgTAPzArQy+HGUEmJBm0Xceka4AAFhAAO4ASRIuCuJAsYBQvCs-iAA\n\n### Repro steps\n\nThe output of the linked code skips memoization of `func(bar)` and `result(data, value)`, but if line 3 (`const value = func(bar);`) is moved down below the `useBaz` call, the memoization happens as expected.\n\nI would expect this code to be memoized regardless of the order of the lines.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: Build of `react-dom` hangs on Deno runtime",
    "description": "React version: 19.2.0\n\n## Steps To Reproduce\n\n1. clone https://github.com/Yovach/deno-tanstack-start-bug-reproduction\n2. deno install\n3. deno task build\n\n\n## The current behavior\n\nIt's hanging forever.\n\n## The expected behavior\n\nNot hanging on `deno task build`\n\n\n## Context\n\nI've been investigating the vite build hanging issue in Deno and found the root cause. https://github.com/denoland/deno/issues/31248\n\nSeems that `react-dom` package.json maps Deno to use the browser bundle (`server.browser.js`) instead of the node bundle. The browser bundle creates a MessageChannel for task scheduling, and this MessageChannel keeps Deno's process alive forever, causing the hang.\nRuntime mappings:\n  \u2022 Node.js \u2192 uses `setImmediate` from server.node.js (doesn't block exit) :white_check_mark:\n  \u2022 Bun \u2192 uses `queueMicrotask` from server.bun.js (doesn't block exit) :white_check_mark:\n  \u2022 Deno \u2192 uses `MessageChannel` from server.browser.js (blocks exit forever) :x:\n\n## Solutions\n\nChanging the \"deno\" mapping from `\"./server.browser.js\"` to `\"./server.node.js\"` at `node_modules/.deno/react-dom@19.2.0/node_modules/react-dom/package.json` fixed the hang.\n\nI also tested changing the bun bundler to use server.browser.js, to have sure that this is the problem, and it hangs for bun too.\n\nI talked with the Deno CLI team, and we decided to create this issue to check if make sense have a new bundle for Deno. For some reason, seems that solution was rejected in the past (https://github.com/facebook/react/pull/30655#issuecomment-2283099574) \n\nIf it does not make sense again, we will need to add a special condition for `react-dom` on Deno side."
  },
  {
    "name": "Bug: Erro: Falha ao executar 'insertBefore' em 'Node': O n\u00f3 antes do qual o novo n\u00f3 deve ser inserido n\u00e3o \u00e9 filho deste n\u00f3.",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Title: Fix minor typo/formatting inconsistency in React documentation/component code comment",
    "description": "Description:\nThere are a few places in the docs or code comments where:\n\ngrammar is slightly inconsistent\n\ncode snippets are missing semicolons\n\nindentation varies\n\nmarkdown headings spacing is inconsistent\n\nReporting one specific typo or formatting inconsistency is a valid issue. Maintainers appreciate these clean-ups.\nExample content:\n\nIn the \u201cConcurrent Rendering\u201d docs, the phrase \u201c\u2026when React need to yield\u2026\u201d should be \u201c\u2026needs to yield\u2026\u201d.\n\nWhy it's valid:\nDocs and code comments must be clean and error-free; maintainers welcome such contributions."
  },
  {
    "name": "dd example usage of useDeferredValue in React Docs",
    "description": "Description:\nThe current React documentation briefly explains useDeferredValue but lacks a clear, practical example showing how it improves UI responsiveness during heavy re-renders.\nA small example (e.g., search filter with a large list) would help beginners understand the hook better.\n\nWhy it's valid:\nReact maintainers often accept documentation-related issues because clear examples help developers learn API features."
  },
  {
    "name": "Bug: Suspense fallback re-renders excessively during slow network responses",
    "description": "### Description\nWhen using `React.Suspense` with a fallback component, the fallback is being\nre-rendered multiple times during slow network requests. This causes unnecessary\nUI flickering and increased component load, especially on low-performance devices.\n\n### Steps to Reproduce\n1. Wrap a lazy-loaded component inside <Suspense fallback={<Spinner />}>.\n2. Simulate a slow network response (2\u20135 seconds delay).\n3. Observe multiple re-renders of the fallback UI in the console.\n\n### Expected Behavior\nThe fallback component should ideally render once and remain stable until the\nlazy component resolves.\n\n### Actual Behavior\nThe fallback re-renders several times durin"
  },
  {
    "name": "Bug:",
    "description": "### Description\nI observed that the cleanup function inside `useEffect` does not always trigger when\nthere are rapid, consecutive state updates. This leads to stale subscriptions and\nunexpected behavior in components that rely on cleanup.\n\n### Steps to Reproduce\n1. Create a component with a useEffect that logs cleanup.\n2. Trigger state updates inside a loop or with very small intervals (5\u201310ms).\n3. Observe that cleanup logs are skipped or delayed.\n\n### Expected Behavior\nCleanup should run reliably before every re-invocation of the effect, regardless of\nhow fast state is updated.\n\n### Actual Behavior\nCleanup is inconsistently triggered during extremely fast re-renders.\n\n### Version\n- React: Latest main branch\n- Environment: Browser (Chrome/Edge)\n- OS: Windows 10 / macOS (tested both)\n\n### Additional Info\nIssue seems more noticeable in strict mode, possibly due to double rendering behavior."
  },
  {
    "name": "Bug: useState hook not updating state correctly in async functions",
    "description": "## Bug Description\nWhen using useState hook inside an async function, the state doesn't update correctly after awaiting a promise.\n\n## Steps To Reproduce\n1. Create a component with useState\n2. Create an async function that awaits a promise\n3. Try to update state after the await\n4. State shows stale value\n\n## Expected Behavior\nState should update correctly after async operations\n\n## Actual Behavior\nState retains old value\n\nReact version: 19.0.0"
  },
  {
    "name": "Bug: [React 19 | Streaming SSR] Script tags injected via ChunkExtractor not appearing in final HTML when using renderToPipeableStream",
    "description": "Description\n\nI am using React 19, Express, and Streaming SSR (renderToPipeableStream) along with @loadable/server ChunkExtractor to extract and inject script/style tags for client-side hydration.\n\nThe loadable-stats.json is generated correctly, and ChunkExtractor.getScriptTags() returns the expected <script> tags.\n\nHowever, when I inject these <script> tags inside the onAllReady() callback of renderToPipeableStream, they never appear in the final browser HTML output. This results in no client bundle being loaded and therefore hydration never occurs.\n\nThis issue happens even though:\nScripts are extracted correctly\nonAllReady() is called\nres.write(scripts) is executed\nSSR output is streamed successfully\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\nhttps://github.com/sandeep3690Butte/node-r19-ssr\n\n1.git clone [<repo>](https://github.com/sandeep3690Butte/node-r19-ssr)\n2.yarn install\n3.yarn build\n4.yarn start\n5.Open the SSR page \u2192 scripts do not appear in view-source or DevTools Network panel.\n\nRepository Structure\nclient/   \u2192 all client React code  \nserver/   \u2192 Express SSR server  \nwebpack.* \u2192 server/client configs for dev + prod  \n\n## The current behavior\nThe onAllReady() callback is triggered correctly\nres.write() executes with no error\nBut no script tags appear in the final HTML\nClient JS never loads, causing hydration to never run\nNo errors in server logs or browser console\n\n## The expected behavior\nThe <script> tags returned by ChunkExtractor.getScriptTags() should be written to the response in onAllReady() so they load the client bundles required for hydration."
  },
  {
    "name": "[Compiler Bug]: The library for web and native user interface",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nDescription: React is a JavaScript library for building user interfaces, designed to help developers create interactive, component-based user interfaces for web (and native) applications. \n\n### Repro steps\n\nDescription: React is a JavaScript library for building user interfaces, designed to help developers create interactive, component-based user interfaces for web (and native) applications. \n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nDescription: React is a JavaScript library for building user interfaces, designed to help developers create interactive, component-based user interfaces for web (and native) applications. \n\n### What version of React Compiler are you using?\n\nDescription: React is a JavaScript library for building user interfaces, designed to help developers create interactive, component-based user interfaces for web (and native) applications."
  },
  {
    "name": "Bug: React DevTools: Component props panel freezes when inspecting rapidly updating components",
    "description": "### Summary\nWhen inspecting components in React DevTools, the props/state panel sometimes freezes when the component tree is rapidly updating (e.g., frequent re-renders triggered by data polling or animations). DevTools UI stops responding until switching to another tab or selecting a different component.\n\n### Steps to Reproduce\n1. Create a component that re-renders frequently (e.g., using setInterval or a high-frequency animation state update).\n2. Open React DevTools \u2192 Components tab.\n3. Select the rapidly updating component.\n4. Observe props/state panel becoming unresponsive or delayed.\n\n### Expected Behavior\nDevTools should continue updating props/state smoothly without UI freeze.\n\n### Actual Behavior\nProps/state panel becomes unresponsive; UI stops updating and interaction freezes temporarily.\n\n### Environment\n- React DevTools version: (latest stable, please fill exact version)\n- Browser: Chrome/Firefox (tested on Chrome)\n- OS: Windows/Linux/macOS (repro across machines)\n\n### Additional Details\n- Switching to Profiler tab and back sometimes restores responsiveness.\n- Suspect bottleneck during frequent renders or serialization.\n- Can provide minimal reproduction repo if required."
  },
  {
    "name": "[DevTools Bug]: Standalone DevTools disconnects when selecting a component with large state",
    "description": "### Website or app\n\nhttps://github.com/drewhoener/react-standalone-devtools-disconnect-repro\n\n### Repro steps\n\nWhen using standalone devtools and attempting to inspect a component with large props or memoized data, the devtools disconnects.\n\nI assume this is because there's just too much to send over the socket?\n\nThis works fine with the devtools extension in chrome and firefox, but fails with standalone.\n\nFollow the instructions in the readme of the linked repro repository, just running and attempting to inspect the element with standalone devtools causes the disconnect\n\n### How often does this bug happen?\n\nEvery time\n\n### DevTools package (automated)\n\n_No response_\n\n### DevTools version (automated)\n\n_No response_\n\n### Error message (automated)\n\n_No response_\n\n### Error call stack (automated)\n\n```text\n\n```\n\n### Error component stack (automated)\n\n```text\n\n```\n\n### GitHub query string (automated)\n\n```text\n\n```"
  },
  {
    "name": "Bug: Suspense + React.lazy rendering order regression between React 18.3.1 \u2192 19.1.0 \u2192 19.2.0 (unexpected repeated renders in 19.2)",
    "description": "I\u2019m seeing a behavioral change in Suspense + React.lazy rendering order when upgrading from React 18.3.1 to React 19.x.\nSpecifically:\n\nReact 18.3.1: Components render once, in order:\nCompA \u2192 CompB \u2192 CompC \u2192 CompD\n\nReact 19.1.0: Still renders cleanly in order (same as 18.3.1).\n\nReact 19.2.0: Rendering order changes significantly, and components render multiple times in progressive batches, producing logs like:\n```\nCompA\nCompB\nCompA\nCompB\nCompC\nCompA\nCompB\nCompC\nCompD\n```\nThis occurs even in production build, and without StrictMode.\nSplitting each component into its own Suspense boundary restores the expected behavior.\n\nThis appears to be caused by a change in Suspense reveal / pre-warm scheduling between 19.1 and 19.2 \u2014 but I'm not sure if this is an intended change, an edge case, or a regression.\n\nI'm opening this issue to confirm:\n- Is this new repeated rendering behavior expected in 19.2?\n- Is there documentation on the Suspense behavior change between 19.1 and 19.2?\n- Should a single Suspense boundary cause repeated re-renders for sibling lazy components in this minimal case?\n\n## Minimal Reproduction\nReproduction environment\nReact versions tested:\n18.3.1\n19.1.0\n19.2.0\nBuild: Production build \nMode: No StrictMode\nBundler: any (Vite/Webpack tested) \u2014 same results\nBrowser: Chrome stable\n\n## Code Example (minimal)\n\nApp.tsx\n```\nimport { lazy, Suspense } from \"react\";\n\nconst CompA = lazy(() => import(\"./CompA\"));\nconst CompB = lazy(() => import(\"./CompB\"));\nconst CompC = lazy(() => import(\"./CompC\"));\nconst CompD = lazy(() => import(\"./CompD\"));\n\nexport default function App() {\n  return (\n    <div>\n      <Suspense fallback={null}>\n        <CompA />\n        <CompB />\n        <CompC />\n        <CompD />\n      </Suspense>\n    </div>\n  );\n}\n\n```\n\nCompA.tsx (same for B/C/D)\n\n```\nconst CompA = () => {\n  console.log(\"CompA\");\n  return <div>CompA</div>;\n};\n\nexport default CompA;\n\n```\n\n## Output logs\nReact 18.3.1\n```\nCompA\nCompB\nCompC\nCompD\n```\n\nReact 19.1.0\n```\nCompA\nCompB\nCompC\nCompD\n```\n\nReact 19.2.0\n```\nCompA\nCompB\nCompA\nCompB\nCompC\nCompA\nCompB\nCompC\nCompD\n```\n## Additional Observation\nIf I change the code to one Suspense per component:\n```\n<div>\n  <Suspense fallback={null}><CompA /></Suspense>\n  <Suspense fallback={null}><CompB /></Suspense>\n  <Suspense fallback={null}><CompC /></Suspense>\n  <Suspense fallback={null}><CompD /></Suspense>\n</div>\n```\nThen React 19.2 logs return to normal:\n```\nCompA\nCompB\nCompC\nCompD\n```\nThis strongly suggests the repeated-render behavior is tied to shared sibling Suspense trees.\n\n# Questions for the React team\n\n- Is the repeated, interleaved rendering behavior in React 19.2 an intentional Suspense change?\n- Is this part of the new \"pre-warm / batched reveals\" behavior introduced in React 19?\n- Should this happen even in a minimal tree where each component only does a static lazy import?\n- Is this considered a regression, or should apps rely on separate Suspense boundaries to avoid this?"
  },
  {
    "name": "Bug: A tree hydrated but some attributes of the server rendered HTML didn't match the client properties.",
    "description": "Hi,\n\nOriginal issue: https://github.com/mantinedev/mantine/issues/8319 (the author of this package and Next.js developers indicated that it is related to React)\n\nI have this error which happens very randomly (on refresh or navigation, 1 time out of 5):\n\n<img width=\"983\" height=\"747\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9fe6c9cf-29cc-4029-93f1-e44d1f80bc8d\" />\n\n> A tree hydrated but some attributes of the server rendered HTML didn't match the client properties. This won't be patched up. This can happen if a SSR-ed Client Component used:\n> \n> - A server/client branch `if (typeof window !== 'undefined')`.\n> - Variable input such as `Date.now()` or `Math.random()` which changes each time it's called.\n> - Date formatting in a user's locale which doesn't match the server.\n> - External changing data without sending a snapshot of it along with the HTML.\n> - Invalid HTML tag nesting.\n> \n> It can also happen if the client has a browser extension installed which messes with the HTML before React loaded.\n> \n> See more info here: https://nextjs.org/docs/messages/react-hydration-error\n> \n>                             `<@mantine/core/Paper component=\"header\" bg=\"dark\" radius={0} p={{base:12,sm:24,md:32}} ...>\n>                               <@mantine/core/Box ref={null} mod={[...]} className=\"!flex flex...\" style={{...}} ...>\n>                                 <InlineStyles selector=\".__m__-_R_...\" styles={{padding:\"c...\"}} media={[...]}>\n>                                   <style\n>                                     data-mantine-styles=\"inline\"\n>                                     nonce={undefined}\n>                                     dangerouslySetInnerHTML={{\n> +                                     __html: \".__m__-_R_albn5ritvlb_{padding:calc(0.75rem * var(--mantine-scale));}@m...\"\n> -                                     __html: \".__m__-_R_1apbn5ritvlb_{padding:calc(0.75rem * var(--mantine-scale));}@...\"\n>                                     }}\n>                                   >\n>                                 <header\n>                                   ref={null}\n>                                   style={{--paper-radius:\"0rem\",background:\"var(--mant...\"}}\n> +                                 className=\"!flex flex-wrap justify-between gap-4 m_1b7284a3 mantine-Paper-root __m__...\"\n> -                                 className=\"!flex flex-wrap justify-between gap-4 m_1b7284a3 mantine-Paper-root __m__...\"\n>                                   data-variant={undefined}\n>                                   data-size={undefined}\n>                                   size={undefined}\n>                                 >`\n\nYES, I already read this article: https://help.mantine.dev/q/color-scheme-hydration-warning\nYES, I already applied `<html lang=\"en\" {...mantineHtmlProps}>`\n\nReact version: 19.2.0\n\n## Steps To Reproduce in your environment\n\n1. Use Mantine's library (maybe other UI libraries has the same problem, no idea)\n2. Use \"Box\" and \"Paper\" components (you can try other ones too)\n3. Navigate between pages and you will get randomly this error\n\nLink to reproduce the problem: https://www.ndnci.com/tools/\n\n## Steps To Reproduce in the demo\n\n1. Go to \"Tools\" listing page\n2. Click on any tool, then in the breadcrumbs, click back on \"Tools\" and repeat the same thing on several tools, I don't know how many click we need to get this but when it happens it is very anoying because the UI is completely broken, we have to reload the page\n\n## The current behavior\n\nHydratation issue brokes the UI during navigation randomly\n\n<img width=\"1858\" height=\"728\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9c678333-db3b-450d-bffd-daa39aca070c\" />\n\n## The expected behavior\n\nHydratation issue must be fixed and never break the UI\n\n<img width=\"1549\" height=\"680\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/22d864ed-7d2e-47aa-a24e-86a5421206fa\" />"
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[Compiler Bug]: no-deriving-state-in-effects does not error if you add the state setter to the dependencies array",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wApgRCHCgBbBHVxh8AX2T5ghEeMnSFZOp1kBKRQB02+OKzAEA2stESpYADJMzAGnxgEuACIqb0h2YC6+AC8hvhh+FBuAMq4ZLgI+AA8+ABKCGTKdJQ8MDBknEn4XtZqHu5kTJQyAD74dFSU+AB8+LX1lI0t-FaqtjoA3Iah+AD0I-gA5kwAbggyZPhzlExSi7k4w2lkjAB0kQgAoiQkCIz8-HpBLcDDYW6e3mr2jrjdj32DRrIulu-S-gMhkYxsI5nUIAQprN8AslisCAh1jBNuldvsjiczhdgtdbq53MVer4Xm8Sh9ht98L8ydIXPdCT5ngFAXRZIYQE4QCY6CQmBMUCAmGJsHh8LhOJgEkIAAqUKBTOgAeUwzFMcnwJBgEDE+AA5AAjMj6hCUAC0mDlCtNMFRuFNJmFlURI0IL11n0MgmGYwdmEqcRYdHYEGICn0IDIHXDw0k0yYWroPgUNyMYXD00jTEIcQQADkIFF3DEcwBJOgY07ScMKXAwKAICmGWSuANgXlg2XylbK1V0MADDngAAWEAA7mX4jA6JGwChyFUELIgA\n\n### Repro steps\n\n1. See playground - one useEffect gives a eslint error the other does not\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.1.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "[Compiler Bug]: Post Increment Assignment Compiling to Pre Increment Assignment",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/kbitgood/rc-bug-repro-increment-assign\n\n### Repro steps\n\n#### Description\nWhen using the post increment operator in an assignment (`y = x++`) the value is incremented before being assigned when using the React Compiler.\n \n#### Steps to Reproduce\n1. Check out the reproduction repository\n2. Run `npm run dev`, or `npm run build && npm run start` as the problem exists in both dev mode and production\n3. See the index field of each item in the next list is off by one. Every item in the list should have a sequential \"Count\" value starting at 1, not\n\n#### Details in the Reproduction Code\nThe code in `src/app/page.tsx` uses Array.reduce to build an array of nodes, and keeps track of a counter in the aggregated value. \n\nWhen we use this code:\n```ts\nconst count = agg.itemCounter++;\n```\nThe compiled output translates to:\n```ts\nagg.itemCounter = agg.itemCounter + 1;\nconst count = agg.itemCounter;\n```\nWhich is not equivalent because it increments before assigning, rather than assigning and then incrementing.\n\nThe issue goes away by taking the counter out of the aggregate object and into a normal variable outside of the reduce function. \n\n#### Conclusion\nI realize that this way of keeping the count in the aggregated value is not the best way, but this is not strictly against the \"Rules of React\". If it is, it should probably be added to the ESLint rules. \n\nThis issue was found on a large codebase that was working just fine before enabling the compiler. And we had no indication that this code would not work after enabling the compiler.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "[Compiler Bug]: React compiler discards computed property keys when used in method shorthand",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABOgWwAcpsEATABRgiIGkEBPXAXlwB0QAzCCAIwCGMTgG52GLlAxxsAS0y4AYrwAUASlzBxuHbhgJssDLgA85WQDcAfNt12TAISG5+AKxbBguANqESZKhp6JgBddU1cAF8o6IB6G2NdE1jza3FIkAAaNEwuWQBzFBBZYggYPGxGIgQIygAbKHzZDAB5IjlMMCjcLhoCXAByQT4EOoBaIgamjDH9ARkxv1k6hBgU2RwBsQxxFS1E2NiluoEOjABZCHIEZA4QATq6znTcMFONvIQu+sbmtrOwGoRFlwAALCAAdwAkhgyDAMA8wCguIiEJEgA\n\n### Repro steps\n\nSee the playground link for the full example.\n\npass an object with a computed key to a react component\neg:\n```tsx\n<Bar obj={{ [computedPropKey]() { } }} />\n```\nobserve the compiler generates incorrect code\n```tsx\n<Bar obj={{ computedPropKey() {} }} />\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\nlatest on the compiler playground\n\n### What version of React Compiler are you using?\n\nlatest on the compiler playground"
  },
  {
    "name": "Bug: useEffect cleanup function throws TypeError when returning null in React 19",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Bug: [React 19] [Regression] Unexpected TypeError when `useEffect` returns `null` \u2014 \"destroy is not a function\" in React 19.x (cleanup now stricter)",
    "description": "Bug: [React 19] [Regression] Unexpected TypeError when useEffect returns null \u2014 \"destroy is not a function\" in React 19.x (cleanup now stricter) \n\n### Summary\nAfter upgrading to React 19.x, code that previously returned `null` from a `useEffect` callback now throws `TypeError: destroy is not a function` during cleanup. Previously returning `null` or other non-function values often worked. This appears to be stricter runtime enforcement of the rule that an effect may only return a cleanup function or nothing.\n\n### Reproduction (minimal)\n```jsx\nimport React, { useEffect } from \"react\";\n\nexport default function App() {\n  useEffect(() => {\n    // early exit path that used to return null\n    if (Date.now() < 0) {\n      return null;\n    }\n    // normal effect (no cleanup)\n    console.log(\"effect ran\");\n  }, []);\n\n  return <div>Test</div>;\n}"
  },
  {
    "name": "Bug: [EPRH] Adding multiple rules to eslint-disable-next-line along with noInlineConfig: true somehow doesn't report any errors",
    "description": "React version: 18\n\nI had originally reported this in the ESLint repo, but they suggested it to move it here eslint/eslint#20344\n## Steps To Reproduce\n\n1. Clone the repo https://github.com/ankitprahladsoni/eslint-noInlineConfig-bug\n2. run `npm run lint`\n\n## The current behavior\nRunning npm run eslint results in 2 warnings\n```\n   8:5  warning  '// eslint-disable-next-line react-hooks/exhaustive-deps, react-hooks/set-state-in-effect' has no effect because you have 'noInlineConfig' setting in your config\n  10:6  warning  React Hook useEffect has a missing dependency: 'count'. Either include it or remove the dependency array. You can also do a functional update 'setCount(c => ...)' if you only need 'count' in the 'setCount' call  react-hooks/exhaustive-deps\n```\n\n\n## The expected behavior\nESLint should have thrown at least one error, since noInlineConfig is set to true. We can get it if we remove the inline config manually\n`8:5 error    Error: Calling setState synchronously within an effect can trigger cascading renders`"
  },
  {
    "name": "Bug: `useEffectEvent` retain the first render value when is used inside a component wrapped in `memo()`",
    "description": "`useEffectEvent` retains the first render value when it's used inside a component wrapped in `memo()`\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\nMinimal repro:\n\n```js\nimport { useState, useEffectEvent, useEffect, memo } from \"react\";\n\nfunction App() {\n  const [counter, setCounter] = useState(0);\n\n  const hello = useEffectEvent(() => {\n    console.log(counter);\n  });\n\n  useEffect(() => {\n    const id = setInterval(() => {\n      hello();\n    }, 1000);\n    return () => clearInterval(id);\n  }, []);\n\n  return (\n    <div>\n      <button onClick={() => setCounter(counter + 1)}>{counter}</button>\n    </div>\n  );\n}\n\nexport default memo(App);\n```\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\nhttps://codesandbox.io/p/sandbox/gj356q\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\nIf you click the button and increment the counter you always see `0` in the log. By removing the `memo` you will see the current counter value.\n\n\n## The expected behavior\nSee the latest counter value even if the component is wrapped in `memo`, since the documentation https://react.dev/reference/react/useEffectEvent does not describe any different behavior if component is wrapped in `memo`."
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "Bug:",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "TypeScript Bug: Type '\"bounding-box\"' is not assignable to type 'PointerEvents | undefined'. (ts 2322)",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\n1. Add this svg into a TypeScript React component: ```html\n<svg\n      style={{  pointerEvents: \"bounding-box\" }}\n    />```\n2. Error: `Type '\"bounding-box\"' is not assignable to type 'PointerEvents | undefined'. (ts 2322)`\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example: https://codesandbox.io/p/devbox/flamboyant-tu-q2v7g5\n(It's a JS code sandbox but you can hover over the property to see that `bounding-box` is not in the property list that the editor renders.)\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\nGives a TS error.\n\n\n## The expected behavior\nShould accept all valid values as listed (for example) here: https://developer.mozilla.org/en-US/docs/Web/SVG/Reference/Attribute/pointer-events#usage_notes"
  },
  {
    "name": "[Compiler Bug]: `using` syntax is not preserved",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://github.com/cormacrelf/react-compiler-using-bug\n\n### Repro steps\n\n`pnpm i && pnpm dev`. Open App.tsx to see there are three [diposables with `using` syntax](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/using). Open the console, note that only one of the Disposables had its `Symbol.dispose` method called.\n\nTo show that it's react-compiler's fault, you can:\n- observe that only `using` syntax inside the component is transformed, the one at the top level is preserved.\n- uncomment `effect.useIt()` which makes react-compiler skip the component and all the disposals work.\n- skip the component\n- disable the transform in vite.config.ts\n\nNote that react-compiler transforms the `using render = new Disposable(\"render\");` to just `new Disposable(\"render\")`, which is not a valid transform. It would be valid for a `const` or `let` binding.\n\nThere is another bug in here that I can't reproduce, because react-compiler keeps bailing out when I add the `useIt` call. But very similar code to the following *does* get compiled, and in that case the using is just transformed to a const, which again breaks the disposal.\n\n\n```typescript\nfunction App() {\n  const context = useContext();\n  const ref = React.useRef(null);\n\n  useEffect(() => {\n    if (ref.current) {\n      using effect = new Disposable(context);\n      effect.useIt();\n    }\n  });\n\n  return <div ref={ref}></div>;\n}\n```\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Bug: eslint-plugin-react-hooks fails handling null chaining operator (react-hooks/immutability)",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nI get a false positive on the react-hooks/immutability rule.\n\nReact version: 19.2.0\neslint-plugin-react-hooks version: 7.0.1\n\n## Steps To Reproduce\n\n1. Check example below\n2. Simple removing the `?` in line 7 fixes the issue\n\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example: https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdOqNxxWYAjFL4AvPihgEAJVJDxk6XVn5cCA8tUIAwmUqUARmTgBrfkKUA+ETtyT5JAHRxYeTpcX1lOSgRfQiYwTEoyTiV8URA6VgQUgG5PAF8AGnwAbQBdQWyJXDMAURISBEZnYUV3MQrdGQJcAAsEAGVccIQkn39AhGCAflCBiPLJSSYSfH4AQm6+mYRheUqYOjmvXHX+waiYuISklJtKCEcs3IKSss8d2DYAHmiAN3wfRWAPhyrgAEghrBB8AB1HCUQgfAD0P1c5Ry4hAeRAehITAA5igQEwALbYPCGTiYIbAfAABUoUFxTDoAHlMMwZPgcvgSDAIET8AByOw2cEAWjiDKZovk9lwoukJKYERgSJiuAFczo-Fa+HwCIRCswSrI7Lo7AgxGQyRAVkoKXEXLAJpiOOMtPpjJZbJY+jKGPAXQgAHcAJLBBB7KxgFDkShqHJAA\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n```\nFound 1 error:\n\nError: This value cannot be modified\n\nModifying a value previously passed as an argument to a hook is not allowed. Consider moving the modification before calling the hook.\n\n   7 | \t\tconst theStyle = ref.current?.style;\n   8 | \t\tif (!theStyle) return;\n>  9 | \t\ttheStyle.display = \"block\";\n     | \t\t^^^^^^^^ `ref` cannot be modified\n  10 | \t}, []);\n  11 | \treturn <div ref={ref}>Hello World</div>;\n  12 | }\n```\n## The expected behavior\nNo error"
  },
  {
    "name": "Bug: Learn bugs",
    "description": "<!--\nThere is an issue while rendering the output result in the learning pages below the code example where it appeared the UI, now it doesn't  appear anymore\n-->\n\nReact version: 19.2\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n[ Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.](https://react.dev/learn/updating-arrays-in-state)\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[Compiler Bug]: React Compiler infers incorrect dependencies based on variable declaration order",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAWwEMAHXAXlwwQHdcBZEgCgG0AdDXXNkQ1kAGlzACASwzJcARgC+AXXaduAIz6Dh+MRIBMc9rICU7dggAexCDDzoseAHJR8CGCLgBJDMSjYAwhHzmqDDwKRmEAN0IAGygEQUwANSiY7wALQgwAcwQAE1xpfXIAPiEFXGscSgclJwAVAE9iBHJcCOiEAH4AOizse3xqmHrGxn0AbiMOTjLMCoSkhFT0rOzvKMilQjgAa2bGKmpEtokMKqcCsmLgUqmpubbFzJzGQ5jOgDMYPz6BvZoXhH0Y2ueXGGGB5Tw6jEeWaRGI3QQ2D2p0GDQBAEJQcCYIjYBxGMCbgAePpOFzuTzYQk3KatGJkYB0joI3ookbSak00QYBkaDAcyZcm53ZJpR7ZBkihZi5arSLrTZbAVC3AAekKwKB-NBAjQmDeIgyKBAIn8Fjw2DRQlwAAVohkxAB5YjYEQzGEfPy4ADkG2qkQAtMR7WIAzjNtgA+h-CJIk5VdkRDhvViMKFSqrVdHiLHCK7MHQINkEBI+Gs+OxpLgwHmkwaEGBbSGMM781gxrqwCkINR3NgnBgomAUG8hwhpEA\n\n### Repro steps\n\nThe React Compiler infers incorrect dependencies for the variable min when its declaration appears after the callback definition. Given the following code:\n\n``` typescript\nconst map = new Map([\n  [\"a\", { min: 1 }],\n  [\"b\", { min: 2 }],\n]);\n\nexport const NumericInputComponent = ({ value, onValueChanged }) => {\n  const numberType = value?.getNumberType();\n\n  const onValueChangedCallback = (newValue: number) => {\n    onValueChanged(Value.fromNumber(newValue));\n  };\n\n  const { min } = map.get(numberType)!;\n\n  return (\n    <NumericInput\n      value={value?.getNumber()}\n      min={min}\n      onValueChanged={onValueChangedCallback}\n    />\n  );\n};\n```\n\nIn the compiled output, the compiler reports that min depends on both value and onValueChanged.\nThis is incorrect, because min is derived exclusively from numberType \u2192 value, and does not use onValueChanged in any way.\n\nIf the declarations are reversed:\n\n``` typescript\nconst { min } = map.get(numberType)!;\n\nconst onValueChangedCallback = (newValue: number) => {\n  onValueChanged(Value.fromNumber(newValue));\n};\n```\n\n\u2026then the compiler correctly infers that min depends only on value.\n\nExpected behavior:\nmin should depend solely on value, regardless of declaration order.\n\nActual behavior:\nmin incorrectly depends on onValueChanged when declared after the callback.\n\nImpact:\nThis causes unnecessary reactivity and avoidable re-renders.\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2\n\n### What version of React Compiler are you using?\n\n1"
  },
  {
    "name": "Bug: eslint-react-hooks false positives on immutability rule",
    "description": "As in [#34776](https://github.com/facebook/react/issues/34776), I have eslint which raises me an error about the immutability rule when I am trying to redirect with `window.location.href`\n\n```\nError: This value cannot be modified\n\nModifying a variable defined outside a component or hook is not allowed. Consider using an effect.\n\n   5 |     await new Promise((resolve) => setTimeout(() => {resolve()}, 1000))\n   6 |\n>  7 |     window.location.href = \"/\";\n     |     ^^^^^^^^^^^^^^^ value cannot be modified\n   8 |   }\n   9 |   \n  10 |   return <form onSubmit={handleSubmit(onSubmit)}>\n```\n\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\nIn my case, I am using react-hook-form which provide a `handleSubmit` callback  to do the form validation, and it seems that using `window.location.href` in this callback trigger the error. \n\nYou can find a simple reproduction [here](https://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABACwEMMATAGwQGUoAjAWwEs8BeXACgDMMBKXZgPlxc23ADoZxCAB4AHCDDwkEHQlDJ4OUDHGwNMuALIBPAIIyZI3MHG5c6LHkzV6TPrkJgj29rwFWbtu4A7oSuGAhBuAAKMBCMYAhsbDAIkGQAbgi+ggnYACoMdAjQ2EnZVilpmSIAvgA0uACMAAyt3GISGIG4QQykEEEAdGQQcIS6mIP4KRxuoiAA9PMA3AE1AQEp2LBdADwc8nS4TrSM2MzARKQUzmdsJy7Y3DX8Aba7fTJQeNhGMgjMebYaTYea4BavLq4XYLA4wOiCXCrDDrDAgOpoTAcBgAcxQIEKcgUuF+-ys0TIUBxfQA8jIJlhcDUhLEjgByGiEGgIMgAWhklOpGF5KUIOl56DoMgYFBgCxIDBwbOR4jY1ihCwWkulZHGegwBggSmQuHmhDIZHm4mZYD1YGxqQpVNp9P1YG4y3R4HwAwAkhhgTAMOawCgVGQEjUgA) (without react-hook-form)"
  },
  {
    "name": "[Compiler]: Detect mutation of state values using type information",
    "description": "### What kind of issue is this?\n\n- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAejQAkDLkmCCMMAhgJ6YAOUYAFpgLZQAuRjAlhAHYA6HAZlBzhtOmAMIQ65Tgg6M8ACgCUmYD0yY4nMI0wBtVowR0wAGkxgEjAJKHjAXUwBeTACUERIQDoqCAMrNDeV0ARjMAJjMAZjtFAG4eNQ0tHSIAE1SbIydMJScAPhVE9QMjME9KGnkAFjjMDEwAWSYWVg4AcwBCIvNLTON5EuNa+t8iOgRMGAReBCnBBDMOCEmEAFo51NnEgF94jm2EjnrcAHkAIwArBCEKGAhyWcYyIjAwVjaOcdkefkFhDjEEikHBkjAAQrlVACkhxtHofDAzBZGABVCwwBzONweRjeCz+FgIeTADhjBDITAAcjwABtWIhKds4od1JpYTooORUoSAHJk7K5RwFKHqdQIzyk8bZSlgiBnSmxOpYJoBVqdbrItGzeQI4ZYUZSqYzOaIHZ7A7cI5YXC+CAwHQAdwM1GgOjA5CmaTVPwEQnYAPEkmkslEkMSbLh+lsph61mjmNc7i8PgJgV0kTMoUwYRie3DyXMdrjpQFyiFhWh6mRfTAA2jnkg9qUesL9vozUMYC60N2PG2IBMIDZvHeKBArCDbaeDxUmAACjSoG1WidyP8wJhtpheHc6FSzkQzggaatyIvlxx1knGKtNJJWDTZmhUqxtArDsTEhg7+QHy1OA0ECbBSXAgEQNI0qBfbmC0YAjggG4LkuK5rv6YBxAO4Aug6ViyLMpI0mAKC8OBFjbEAA\n\n### Repro steps\n\nRef. https://react.dev/reference/eslint-plugin-react-hooks/lints/immutability \n\nIn the above article, some invalid patterns are introduced, however, the playground do not throw any errors for the violations.\nIs this the limitations for the react compiler or the bug?\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2.0\n\n### What version of React Compiler are you using?\n\n1.0.0"
  },
  {
    "name": "Test",
    "description": "Test issue"
  },
  {
    "name": "Test",
    "description": "Test issue"
  },
  {
    "name": "Bug: React 19.2 flushSync flickers briefly",
    "description": "We\u2019re seeing a **production-only flicker regression for flushSync calls in React 19.2** that does **not** occur in React 19.1.\n\n### Demo Links\n\n* **React 19.1 (no flicker):**\n  [https://deploy-preview-8--react-responsive-overflow-list.netlify.app/](https://deploy-preview-8--react-responsive-overflow-list.netlify.app)\n\n  [source PR](https://github.com/Eliav2/react-responsive-overflow-list/pull/8)\n\n  ![Image](https://github.com/user-attachments/assets/df1271c9-c6fe-4437-b288-5f15b027dd1b)\n\n* **React 19.2 (visible flicker):**\n  [https://deploy-preview-10--react-responsive-overflow-list.netlify.app/](https://deploy-preview-10--react-responsive-overflow-list.netlify.app)\n\n  [source PR](https://github.com/Eliav2/react-responsive-overflow-list/pull/10)\n  \n  ![Image](https://github.com/user-attachments/assets/bf69ce3e-70d9-4f11-a20c-a6e064d010f6)\n  \n\n  Note: performance throttling was enabled here to see flickering more clearly while resize\n\n### What Happens\n\nThe component relies on `flushSync` inside `useResizeObserver`:\n[https://github.com/Eliav2/react-responsive-overflow-list/blob/main/src/hooks/useResizeObserver.ts](https://github.com/Eliav2/react-responsive-overflow-list/blob/main/src/hooks/useResizeObserver.ts)\n\nOn resize, we intentionally trigger synchronous layout updates using flushSync to prevent any intermediate visual state from appearing.\n\n### Behavior Difference\n\nReact 19.1:\nflushSync blocks and applies the update immediately \u2014 the intermediate state never appears.\n\nReact 19.2:\nReact momentarily exposes the intermediate layout state even inside a flushSync block in production builds, resulting in a very short but noticeable flicker.\n\n### Additional Notes\n\n* Flicker **only** appears in **production builds** of React 19.2.\n* Both 19.1 and 19.2 **do not flicker** in local development mode.\n* No code changes between demos\u2014only the React version changed."
  },
  {
    "name": "Bug: PATCH Request Body Ignored/Stripped When Using fetch API on React 19.2.0",
    "description": "When attempting to send a **JSON** request body using the standard fetch API with the **HTTP** method set to **PATCH,** the body is consistently ignored or stripped from the request payload. This regression makes it impossible to perform partial updates requiring data transfer via the request body on React projects running **version 19.2.0.**\n\nThe issue is specifically observed when correctly setting the **Content-Type: application/json** header and stringifying the payload using **JSON.stringify()**.\n\n### To Reproduce \n\n1. Set up a new React project using version 19.2.0\n2. Create a component that performs a **PATCH** request using the **fetch** API, similar to the code snippet provided below. \n3. Set a network monitor (e.g., Chrome Devtools, Wireshark) on the target endpoint. \n4. Execute the **handlePatch** function. \n\n### Minimal Code Snippet: \n\n```bash\n// This is the structure used to reproduce the bug\nconst dataToUpdate = { status: 'pending', notes: 'Needs review' };\n\nfetch('[https://api.example.com/items/123](https://api.example.com/items/123)', {\n    method: 'PATCH',\n    headers: {\n        'Content-Type': 'application/json',\n    },\n    body: JSON.stringify(dataToUpdate),\n})\n.then(response => response.json())\n.then(data => console.log('Response:', data));\n// When monitoring the request, the body is empty or not transmitted.\n\n```\n\n### Expected Behavior \n\nThe **PATCH** request should include the serialized JSON object **({\"status\":\"pending\", \"notes\":\"Needs review\"})** in its body, as specified by the **body** property of the **fetch** configuration object.\n\n### Observed Behavior\n\nThe request is sent with an empty body, or the server-side logging indicates a null/empty body was received, despite the client-side **fetch** configuration containing a valid **JSON.stringify(body).**\n\n### Environment \n\n- React Version: 19.2.0\n- Browser: All (Confirmed on macOS, Windows 11) \n- Operating System: All (Confirmed on macOS, Windows 11) \n- Environment Type: Standard web browser environment."
  },
  {
    "name": "Bug: React is good...!",
    "description": "REACT is gooooodddd!!!!!"
  },
  {
    "name": "Bug: recovery my settlements",
    "description": "<!--\n  Please provide a clear and concise description of what the bug is. Include\n  screenshots if needed. Please test using the latest version of the relevant\n  React packages to make sure your issue has not already been fixed.\n-->\n\nReact version:\n\n## Steps To Reproduce\n\n1.\n2.\n\n<!--\n  Your bug will get fixed much faster if we can run your code and it doesn't\n  have dependencies other than React. Issues without reproduction steps or\n  code examples may be immediately closed as not actionable.\n-->\n\nLink to code example:\n\n<!--\n  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a\n  repository on GitHub, or provide a minimal code example that reproduces the\n  problem. You may provide a screenshot of the application if you think it is\n  relevant to your bug report. Here are some tips for providing a minimal\n  example: https://stackoverflow.com/help/mcve.\n-->\n\n## The current behavior\n\n\n## The expected behavior"
  },
  {
    "name": "[Compiler Bug]: stable setter for useOptimistic",
    "description": "### What kind of issue is this?\n\n- [x] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)\n- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)\n- [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin)\n- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)\n\n### Link to repro\n\nhttps://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAwhALYAOhCmOAFMGAgDYK4CCuBmAvgJRFgAHUzCccQmBxEA2njAAFWgBM8mAOYAaIsxwBJJavUaAukQC8RKMwDylfOQX449NAEMWzfgG5R-iSkZd25CS2tmEk8WACMQgGt6ekELAD4hAPE9Q2VMNU16HBgoBF9MooQcWGJmNk5QzGS-MRxeHVla9hwufEJTMrEWmErqonpygB5U8vEJgGVWLv1MSihghotgEN6+IgB6aZbxcWAFXPyNIgAyK6IJsEp3TFSAcgA6D5eJvYen1N5JgcAvxRLwQFoQJJMGg8BoUCA8FQIDAZDgAJ6UBBCIiKFhQDTqew7MBEXhENAwChEF5xGKsAC0lDxBMw9OG23pkioeDYMD2amkL2aokYoiI+z2XMoPPcOwAshAVAhkERhCBomqxaqQI9MHg4AAVAAWwzARogLBUapVaswNDVWi1apw7hgGkq1u1AEYAJwO0G6WUKGEIEm4-GEhw8MC+cHgc0Ad2WOAQMEwnjAKA8XgQvCAA\n\n### Repro steps\n\n<img width=\"1280\" height=\"635\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/47665500-8654-4d6b-9d20-444460671c2f\" />\n\n### How often does this bug happen?\n\nEvery time\n\n### What version of React are you using?\n\n19.2\n\n### What version of React Compiler are you using?\n\nplayground"
  },
  {
    "name": "Bug: Fallback image unnecessarily downloaded in `<picture>` element on Safari",
    "description": "When a <picture> element with a fallback image is rendered by React, Safari downloads both the fallback image and the correct image from the <source> element. This issue occurs only in Safari and results in the fallback image being downloaded unnecessarily.\n\nReproduced on macOS Safari 18.6\n\nReact version: 19.2.0\n\n## Steps To Reproduce\n\n1. Add the following tags inside a React component JSX : \n```\n      <picture>\n        <source\n          media=\"(min-width:650px)\"\n          srcSet=\"https://www.w3schools.com/TAGS/img_pink_flowers.jpg\"\n        />\n        <source\n          media=\"(min-width:465px)\"\n          srcSet=\"https://www.w3schools.com/TAGS/img_white_flower.jpg\"\n        />\n        <img\n          src=\"https://www.w3schools.com/TAGS/img_orange_flowers.jpg\"\n          alt=\"Flowers\"\n        />\n      </picture>\n```\n2. Open the app on Safari\n3. Observe that 2 images are downloaded : the one corresponding to the screen size **and** the fallback image\n4. Move the <picture> element to the root index.html so it is rendered outside of React\n5. Observe that only the image corresponding to the screen size is downloaded \n\n\nLink to code example:\n\nhttps://codesandbox.io/p/sandbox/dreamy-swartz-rfm5wv\n\n## The current behavior\n<img width=\"1288\" height=\"477\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/757a1a0e-1c79-4264-a242-e47d22cea59c\" />\n\nThe fallback image is being downloaded unnecessarily.\n\n## The expected behavior\n\nOnly one picture should be downloaded, just like with other browsers."
  },
  {
    "name": "Bug: ReactFlightAsyncDebugInfo test 'can track async information when awaited' fails due to snapshot mismatch",
    "description": "React version:Experimental / latest commit\n\n\n## Steps To Reproduce\n\n1. Clone the React repository.\n2. Run the following test:\n   yarn test packages/react-server/src/__tests__/ReactFlightAsyncDebugInfo-test.js --testNamePattern=\"can track async information when awaited\" --runInBand --verbose\n3. Observe that the test fails due to a snapshot mismatch.\n\nhttps://github.com/user-attachments/assets/426920b1-7338-4d5d-833e-22e33a7d5c4c\n\n-->\n\nLink to code example: packages/react-server/src/__tests__/ReactFlightAsyncDebugInfo-test.js\n\n\n# Context: Snapshot Mismatch in ReactFlightAsyncDebugInfo-test\n\n###  Failing Test Case\n\ntest(\"can track async information when awaited\", async () => {\n  const result = await someAsyncFunction();\n  expect(getDebugInfo(result)).toMatchInlineSnapshot(`\n    Object {\n      \"value\": undefined,\n    }\n  `);\n});\n\n### implementation\n\nexport async function someAsyncFunction() {\n  return {\n    value: [undefined],\n  };\n}\n-->\n\n### Actual Output\n```js\nObject {\n  \"value\": [undefined],\n}\n\n-->\n\n## The current behavior\nThe inline snapshot does not match the current output. The test expects `\"value\": undefined` but receives `\"value\": [ , ]`.\n\n## The expected behavior\nThe test should pass and the snapshot should match the current output."
  },
  {
    "name": "Bug: Dev Mode breaks applications: calling getters in props objects causes side-effects",
    "description": "In dev mode, there is a feature that inspects the properties of all props passed to a component, recursively. This happens using a simple `for (key in obj)` loop and thus traverses all enumerable properties, recursively, including getters. This feature cannot be disabled without disabling dev mode and it will only trigger in slightly advanced setups and thus is hard to debug.\nIterating all enumerable keys for getters in an object is a problem, because these getters are also evaluated using simple unconditional `obj[key]` syntax, causing possible side-effects to happen for non-trivial getters. \nAlthough frowned-upon in many cases, it is valid for getters to have side-effects that could change the state of the objects or throw exceptions. Specifically in objects that are unrelated to the React components.\nHowever, in dev mode, this causes the application to break or to behave differently in non-dev mode with hard to diagnose bugs.\n\nReact version: 19.2.0\n\n(this is a regression)\n\n## Steps To Reproduce\n\n1. Run the following demo in dev mode \n\n```\nimport {useEffect, useState} from 'react'\nimport {createRoot} from \"react-dom/client\";\n\nfunction getData() {\n    const result = {}\n    result._someInnocentUnusedProp = {}\n    Object.defineProperty(result._someInnocentUnusedProp, \"nestedUnusedCalculatedProp\", {\n        get: () => {\n            console.log('get nestedUnusedCalculatedProp called - why?')\n            throw new Error(\"I should not call getters unconditionally\")\n        }, enumerable: true\n    })\n    return result._someInnocentUnusedProp\n}\n\nfunction DataRenderer() { return null }\n\nfunction Child(props) {\n    return <div>child: {props.count}<DataRenderer data={getData()}></DataRenderer></div>;\n}\n\nfunction App() {\n    const [count, setCount] = useState(0);\n\n    // just to cause a re-render\n    useEffect(() => {\n        if (count > 5) return\n        setCount((c) => c + 1);\n    }, [count]);\n\n    return <Child count={count}/>;\n}\n\ncreateRoot(document.getElementById(\"root\")).render(<App/>);\n```\n\n2. You will get a stacktrace like the following:\n\n```\n Uncaught Error: I should not call getters unconditionally\n    at Object.get [as nestedUnusedCalculatedProp] (main.jsx:10:19)\n    at addObjectDiffToProperties (react-dom-client.development.js:3968:21)\n    at addObjectDiffToProperties (react-dom-client.development.js:4016:23)\n    at logComponentRender (react-dom-client.development.js:4130:22)\n    at commitPassiveMountOnFiber (react-dom-client.development.js:15469:13)\n    at recursivelyTraversePassiveMountEffects (react-dom-client.development.js:15439:11)\n    at commitPassiveMountOnFiber (react-dom-client.development.js:15718:11)\n    at recursivelyTraversePassiveMountEffects (react-dom-client.development.js:15439:11)\n    at commitPassiveMountOnFiber (react-dom-client.development.js:15476:11)\n    at recursivelyTraversePassiveMountEffects (react-dom-client.development.js:15439:11)\n```\n\nLink to code example:\n\n[Code Sandbox](https://codesandbox.io/p/sandbox/optimistic-ioana-ct7j24)\n\n(Sometimes you need to reload the page a couple of times for the issue to appear. The timing is not deterministic, which makes the issue especially hard to diagnose.)\n\n## The current behavior\n\nReact Dev mode evaluates getters recursively from time to time, possibly causing side effects.\n\n\n## The expected behavior\n\nRect dev mode should not evaluate all getters in props objects unconditionally.\n\n\n## My thoughts\n\nThe problematic code is here, which evaluates all enumerable properties, even from prototypes: \n\nhttps://github.com/facebook/react/blob/3a495ae72264c46b4a4355904c6b4958b0a2f9b2/packages/shared/ReactPerformanceTrackProperties.js#L295\n\nThe workaround here (other than not using dev mode) is to make the properties non-enumerable, however that is not always feasible. \n\nMy suggested fix would be to not evaluate getters for the logging purposes. They are likely to return new objects, perform a lot of work and thus slow down the execution, may change the state of the application, or may even throw. Only ever compare fields to avoid changing the state or executing code at all costs. \n\nUsing `Object.keys()` would improve the situation for properties defined on classes, but ideally \"getter\"s should not be called at all, no matter whether they are defined as own properties or prototype properties.\n\nThe current state requires complicated workarounds and causes hard to debug issues in complex applications. As it only affects dev-mode, I would love to see this improved in a bugfix so that people can upgrade to 19.2"
  }
]